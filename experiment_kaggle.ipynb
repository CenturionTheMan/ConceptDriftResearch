{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnW2ICuewM0-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "REMOVE_RES_CONTENT = True\n",
    "IMG_SIZE=224\n",
    "#IMG_SIZE=64\n",
    "IMG_RESIZE=True\n",
    "SEED = 42\n",
    "RES_PATH = '/kaggle/working/res/'\n",
    "DATASET_PATH = '/kaggle/input/deepfakedataset/data/'\n",
    "SHOW_EXTRA_INFO=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPKtaDEKrVlM",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "f35d774d-1a32-46a0-fbb9-1b5dd41d79ad",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q tabulate\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tabulate as tb\n",
    "from typing import Dict\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import re\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Input, BatchNormalization, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.applications import Xception, EfficientNetB4, InceptionV3, EfficientNetV2M,EfficientNetV2S , MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, BatchNormalization, LeakyReLU,\n",
    "    MaxPooling2D, Flatten, Dense, Dropout, concatenate\n",
    ")\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score, log_loss, brier_score_loss\n",
    "import time\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name:\n",
    "    print(f\"GPU available: {device_name}\")\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset splitting - handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQHjq4DXrY3h",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "ee466649-50e5-43a4-c318-b6aedc03dc0d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = DATASET_PATH + 'metadata.csv'\n",
    "df_tmp = pd.read_csv(file_path, sep=',')\n",
    "df_tmp['path'] = DATASET_PATH + df_tmp['path']\n",
    "\n",
    "df_tmp = df_tmp[df_tmp['deepfake'] != 0]\n",
    "\n",
    "df_tmp['ethnicity'] = df_tmp.apply(\n",
    "    lambda row: 'white' if row['white'] == 1 else ('black' if row['black'] == 1 else (\n",
    "        'asian' if row['asian'] == 1 else None)), axis=1)\n",
    "\n",
    "df = df_tmp[['deepfake', 'male', 'ethnicity', 'eyeglasses', 'heavy_makeup', 'big_lips', 'path']]\n",
    "\n",
    "df = df.rename(columns={'deepfake': 'type', 'male': 'sex', 'heavy_makeup': 'makeup', 'big_lips': 'lips',})\n",
    "\n",
    "df['typeName'] = df['type'].replace({1: 'fake', -1: 'real'})\n",
    "df['type'] = df['typeName'].map({'fake': 1, 'real': 0}).values\n",
    "df['sex'] = df['sex'].replace({-1: 'female', 0: None, 1: 'male'})\n",
    "df['makeup'] = df['makeup'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
    "df['lips'] = df['lips'].replace({-1: 'small', 0: None, 1: 'big'})\n",
    "df['eyeglasses'] = df['eyeglasses'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
    "\n",
    "\n",
    "print(tb.tabulate(df.head(), headers='keys', tablefmt='psql'))\n",
    "print(f\"Dataset size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqJXku-tsxK7",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "03c62d12-4bff-4e41-cd9b-7d694cc52732",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_balanced_subset(\n",
    "    df, class_col, feature_col, feature_value,\n",
    "    samples_per_class, randomize=True, reset_index=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a balanced subset of the data for a given feature value, with equal number of samples per class.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        class_col: column name of class labels\n",
    "        feature_col: column name of feature\n",
    "        feature_value: specific feature value to filter\n",
    "        samples_per_class: number of samples per class\n",
    "        randomize: whether to shuffle within class before selecting\n",
    "        reset_index: whether to reset index of returned DataFrame\n",
    "        seed: random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Balanced DataFrame subset\n",
    "    \"\"\"\n",
    "    tmp = df[df[feature_col] == feature_value]\n",
    "\n",
    "    counts = tmp[class_col].value_counts()\n",
    "    for cl, count in counts.items():\n",
    "        if count < samples_per_class:\n",
    "            raise ValueError(f\"Not enough samples for class '{cl}' in feature '{feature_value}'. \"\n",
    "                             f\"Required: {samples_per_class}, Available: {count}\")\n",
    "\n",
    "    tmp = pd.concat([\n",
    "        (g.sample(frac=1, random_state=SEED).head(samples_per_class) if randomize else g.head(samples_per_class))\n",
    "        for _, g in tmp.groupby(class_col)\n",
    "    ])\n",
    "\n",
    "    if reset_index:\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "tmp_test = get_balanced_subset(\n",
    "    df=df, class_col='type', feature_col='sex', feature_value='male',\n",
    "    samples_per_class=2, randomize=True, reset_index=True)\n",
    "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJZmhWnis0yh",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "555bdf23-ce59-42e2-da9c-dffb8e307822",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_exp_data(df, class_col, feature_col, ratio : Dict, size, randomize=True, exclude_column=None, exclude_df=None, max_diff=0.05):\n",
    "    '''\n",
    "    Get a balanced subset of the data based on specified ratios for features.\n",
    "    Args:\n",
    "        df: DataFrame containing the data\n",
    "        class_col: column name for class labels\n",
    "        feature_col: column name for features\n",
    "        ratio: dictionary with feature values as keys and their ratios as values\n",
    "        size: total number of samples to return\n",
    "        randomize: whether to shuffle the DataFrame before processing\n",
    "        exclude_column: column name to exclude from the DataFrame\n",
    "        exclude_df: DataFrame containing values to exclude based on exclude_column\n",
    "    '''\n",
    "    if randomize:\n",
    "        df_rnd = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    else:\n",
    "        df_rnd = df.copy()\n",
    "\n",
    "    if exclude_column is not None and exclude_df is not None:\n",
    "        if exclude_column not in df_rnd.columns:\n",
    "            raise ValueError(f\"Column '{exclude_column}' not found in DataFrame.\")\n",
    "        if exclude_column not in exclude_df.columns:\n",
    "            raise ValueError(f\"Column '{exclude_column}' not found in exclude DataFrame.\")\n",
    "        df_rnd = df_rnd[~df_rnd[exclude_column].isin(exclude_df[exclude_column])]\n",
    "\n",
    "    uniq_classes = df_rnd[class_col].unique()\n",
    "    uniq_features = df_rnd[feature_col].unique()\n",
    "\n",
    "    def get_exp_data_inner(tmp_df, size):\n",
    "        df_tmp = None\n",
    "        for uf in uniq_features:\n",
    "            if ratio.get(uf) is None:\n",
    "                if SHOW_EXTRA_INFO:\n",
    "                    print(f\"Feature '{uf}' not found in ratios. Skipping.\")\n",
    "                continue\n",
    "            c_amt = int(size * ratio[uf] / len(uniq_classes))\n",
    "            # if c_amt <= 0:\n",
    "            #     raise ValueError(f\"Calculated samples per class ({c_amt}) is less than or equal to zero for feature '{uf}' with ratio {ratio}.\")\n",
    "            tmp = get_balanced_subset(df=tmp_df, class_col=class_col, feature_col=feature_col, feature_value=uf,\n",
    "                                        samples_per_class=c_amt, randomize=False)\n",
    "            if df_tmp is None:\n",
    "                df_tmp = tmp\n",
    "            else:\n",
    "                df_tmp = pd.concat([df_tmp, tmp])\n",
    "        return df_tmp\n",
    "\n",
    "    df_res = get_exp_data_inner(df_rnd, size)\n",
    "\n",
    "    if len(df_res) < size and SHOW_EXTRA_INFO:\n",
    "        print(f\"Samples for ({len(df_res)}) are less than requested ({size}).\")\n",
    "\n",
    "    ratios_fet = df_res[feature_col].value_counts(normalize=True).to_dict()\n",
    "    ratios_cls = df_res[class_col].value_counts(normalize=False).to_dict()\n",
    "\n",
    "    if SHOW_EXTRA_INFO:\n",
    "        print(f\"[] Ratios for {feature_col}: {ratios_fet}\")\n",
    "        print(f\"[] Ratios for {class_col}: {ratios_cls}\")\n",
    "        print()\n",
    "        \n",
    "\n",
    "    for k in ratio:\n",
    "        if ratios_fet.get(k) is None:\n",
    "            if ratio[k] > 0.0:\n",
    "                raise ValueError(f\"Feature '{k}' not found in DataFrame after sampling (try increase 'size' parameter).\")\n",
    "        elif abs(ratios_fet[k] - ratio[k]) > max_diff:\n",
    "            raise ValueError(f\"Feature '{k}' ratio {ratios_fet[k]} differs from requested {ratio[k]} by more than {max_diff}.\")\n",
    "\n",
    "    \n",
    "\n",
    "    df_res = df_res.reset_index(drop=True)\n",
    "\n",
    "    return df_res\n",
    "\n",
    "tmp_test = get_exp_data(\n",
    "    df=df, class_col='type', feature_col='ethnicity', ratio={'white':0.2, 'black':0.6, 'asian': 0.2}, size=10)\n",
    "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNCRWX5js2d7",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapplications\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mefficientnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_input \u001b[38;5;28;01mas\u001b[39;00m preprocess_efficientnet\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapplications\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mxception\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_input \u001b[38;5;28;01mas\u001b[39;00m preprocess_xception\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapplications\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minception_v3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_input \u001b[38;5;28;01mas\u001b[39;00m preprocess_inception_v3\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\ConceptDriftResearch\\.venv\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[32m     47\u001b[39m _tf2.enable()\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\ConceptDriftResearch\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\ConceptDriftResearch\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mag_ctx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimpl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\ConceptDriftResearch\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthreading\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtf_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[32m     25\u001b[39m stacks = threading.local()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\ConceptDriftResearch\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext_managers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor_list\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\ConceptDriftResearch\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontextlib\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcontrol_dependency_on_returns\u001b[39m(return_value):\n\u001b[32m     24\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Create a TF control dependency on the return values of a function.\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[33;03m  If the function had no return value, a no-op context is returned.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m \u001b[33;03m    A context manager.\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\ConceptDriftResearch\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:42\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_data_flow_ops\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_ops\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msaved_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nested_structure_coder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Personal\\ConceptDriftResearch\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:88\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_ops_stack\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_array_ops\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_bitwise_ops\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_data_flow_ops\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_logging_ops\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1322\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1262\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1532\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1506\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1605\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficientnet\n",
    "from tensorflow.keras.applications.xception import preprocess_input as preprocess_xception\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as preprocess_inception_v3\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as preprocess_efficientnetv2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_mobilenet_v2\n",
    "\n",
    "augmentation_std = tf.keras.Sequential([\n",
    "    #layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.05),\n",
    "    #layers.RandomContrast(0.05),\n",
    "    #layers.RandomBrightness(0.05),\n",
    "], name=\"augmentation_std\")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def load_image(file_path, label, preprocess, augment):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "\n",
    "    if IMG_RESIZE:\n",
    "        image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    else:\n",
    "        image = tf.cast(image, tf.float32)\n",
    "\n",
    "    if preprocess:\n",
    "        image = preprocess(image)\n",
    "\n",
    "    if augment:\n",
    "        image = augment(image)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def get_data_for_model(df, class_col, files_col, batch_size, preprocess, augment):\n",
    "    image_paths = df[files_col].values\n",
    "    labels = df[class_col]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda path, label: load_image(path, label, preprocess, augment), num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYJbzSoMv9Vd",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "07789495-ad79-42f3-eacb-3cc9507b3425",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create folder if it doesn't exist\n",
    "if not os.path.exists(RES_PATH):\n",
    "    os.makedirs(RES_PATH)\n",
    "    print(f\"Created folder: {RES_PATH}\")\n",
    "elif REMOVE_RES_CONTENT:\n",
    "    # Remove all files inside the folder\n",
    "    for filename in os.listdir(RES_PATH):\n",
    "        file_path = os.path.join(RES_PATH, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)          # remove file or link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)      # remove folder and contents\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "    print(f\"Cleared contents of folder: {RES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQ_JQedB1623",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_done_reps(model_name, feature_name, amt_per_rep):\n",
    "  results_path = f'{RES_PATH}res_{feature_name}_{model_name.replace(\" \", \"_\")}.csv'\n",
    "  if not os.path.exists(results_path):\n",
    "    return [], None\n",
    "\n",
    "  tmp_df = pd.read_csv(results_path)\n",
    "  dones = []\n",
    "  for r in tmp_df[\"rep\"].unique():\n",
    "    amt = len(tmp_df[tmp_df[\"rep\"]==r])\n",
    "    if amt == amt_per_rep:\n",
    "      dones.append(r)\n",
    "\n",
    "  return dones, tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_fairness_metrics(y_true, y_pred, group_labels):\n",
    "    \"\"\"Compute per-group rates and their gaps.\"\"\"\n",
    "    groups = pd.unique(group_labels)\n",
    "    tpr = {}\n",
    "    fpr = {}\n",
    "    pos_rate = {}\n",
    "\n",
    "    for g in groups:\n",
    "        mask = (group_labels == g)\n",
    "        yt = y_true[mask]\n",
    "        yp = y_pred[mask]\n",
    "        tn, fp, fn, tp = confusion_matrix(yt, yp, labels=[0,1]).ravel()\n",
    "        tpr[g] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        fpr[g] = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "        pos_rate[g] = (yp == 1).mean()\n",
    "\n",
    "    tpr_gap = max(tpr.values()) - min(tpr.values())\n",
    "    fpr_gap = max(fpr.values()) - min(fpr.values())\n",
    "    pos_rate_gap = max(pos_rate.values()) - min(pos_rate.values())\n",
    "\n",
    "    return {\n",
    "        \"tpr_gap\": tpr_gap,\n",
    "        \"fpr_gap\": fpr_gap,\n",
    "        \"pos_rate_gap\": pos_rate_gap,\n",
    "        \"per_group\": {\n",
    "            \"tpr\": tpr,\n",
    "            \"fpr\": fpr,\n",
    "            \"pos_rate\": pos_rate\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ik6Z-AUis6G7",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def perform_tests(df, train_metas, test_metas, validation_size, reps, feature_split_col, get_model, preprocess, class_col='type', exclude_column='path', \n",
    "                  files_col='path', data_augmentation=False, epochs_num=15, batch_size=64):\n",
    "    res = []\n",
    "\n",
    "    model_name = get_model.__name__.replace(\"create_\", \"\")\n",
    "\n",
    "    done_reps, prev_results = get_done_reps(model_name, feature_split_col, len(test_metas) * len(train_metas))\n",
    "\n",
    "    for r in range(reps):\n",
    "        if r in done_reps:\n",
    "          res.extend(prev_results[prev_results['rep']==r].values.tolist())\n",
    "          print(f\"Rep {r} already done for {model_name}. Skipping...\")\n",
    "          continue\n",
    "\n",
    "        np.random.seed(SEED + r)\n",
    "        tf.random.set_seed(SEED + r)\n",
    "\n",
    "        for train_meta in train_metas:\n",
    "            train_val = get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=train_meta['ratio'], size=train_meta['size'] + validation_size)\n",
    "            train_val = train_val.sample(frac=1, random_state=SEED+r).reset_index(drop=True)\n",
    "\n",
    "            stratify_key = train_val[class_col].astype(str) + \"_\" + train_val[feature_split_col].astype(str)\n",
    "\n",
    "            train, val = train_test_split(\n",
    "                train_val,\n",
    "                test_size=validation_size / (train_meta['size'] + validation_size),\n",
    "                stratify=stratify_key,\n",
    "                random_state=SEED + r\n",
    "            )\n",
    "\n",
    "            tests = [\n",
    "                get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=tm['ratio'], size=tm['size'], exclude_column=exclude_column, exclude_df=train) for tm in test_metas\n",
    "            ]\n",
    "\n",
    "            train_dataset = get_data_for_model(train, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=data_augmentation)\n",
    "            val_dataset = get_data_for_model(val, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=False)\n",
    "            test_datasets = [\n",
    "                get_data_for_model(test, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=False) for test in tests\n",
    "            ]\n",
    "\n",
    "            train_ratio = '/'.join([f\"{k}:{v}\" for k, v in train_meta['ratio'].items()])\n",
    "            train_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in train[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
    "            train_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', train_ratio)\n",
    "\n",
    "            model = get_model(input_shape=(IMG_SIZE,IMG_SIZE,3), \n",
    "                              train_dataset=train_dataset, val_dataset=val_dataset, \n",
    "                              epochs_num=epochs_num)\n",
    "\n",
    "            for test_dataset, test_meta, test_df in zip(test_datasets, test_metas, tests):\n",
    "                predictions = model.predict(test_dataset)\n",
    "                y_true = test_df[class_col]\n",
    "                y_pred = (predictions > 0.5).astype(int).flatten()\n",
    "                \n",
    "                tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "                accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "                fairness = compute_fairness_metrics(\n",
    "                    y_true=np.array(y_true),\n",
    "                    y_pred=np.array(y_pred),\n",
    "                    group_labels=test_df[feature_split_col].values\n",
    "                )\n",
    "                fairness_json = json.dumps(fairness[\"per_group\"])\n",
    "                \n",
    "                test_ratio = '/'.join([f\"{k}:{v}\" for k, v in test_meta['ratio'].items()])\n",
    "                test_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in test_df[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
    "                test_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', test_ratio)\n",
    "\n",
    "                roc_df = pd.DataFrame({\n",
    "                    'y_true': y_true,\n",
    "                    'y_pred_proba': predictions.flatten()\n",
    "                })\n",
    "                path_roc = f'{RES_PATH}roc_{feature_split_col}_{model_name.replace(\" \", \"_\")}_rep{r}_{time.time_ns()}.csv'\n",
    "                roc_df.to_csv(path_roc, index=False)\n",
    "                                \n",
    "                res.append([\n",
    "                    r,\n",
    "                    model_name,\n",
    "                    feature_split_col,\n",
    "                    train_meta['size'],\n",
    "                    train_ratio,\n",
    "                    test_meta['size'],\n",
    "                    test_ratio,\n",
    "                    train_ratio_rel,\n",
    "                    test_ratio_rel,\n",
    "                    train_ratio_sim,\n",
    "                    test_ratio_sim,\n",
    "                    accuracy,\n",
    "                    tn, fp, fn, tp,\n",
    "                    fairness[\"tpr_gap\"],\n",
    "                    fairness[\"fpr_gap\"],\n",
    "                    fairness[\"pos_rate_gap\"],\n",
    "                    fairness_json,\n",
    "                    path_roc\n",
    "                ])\n",
    "\n",
    "                print(f\"Rep: {r:2} | Model: {model_name} | Feature Split: {feature_split_col} | Ratio: {test_ratio} | Acc: {accuracy:.2f}\")\n",
    "\n",
    "                res_df = pd.DataFrame(res, columns=[\n",
    "                    'rep', 'model_name', 'feature_split_col',\n",
    "                    'train_size', 'train_ratio_detail', 'test_size', 'test_ratio_detail',\n",
    "                    'train_ratio_rel', 'test_ratio_rel', \"train_ratio\", \"test_ratio\",\n",
    "                    'accuracy', 'TN', 'FP', 'FN', 'TP',\n",
    "                    'fairness_tpr_gap', 'fairness_fpr_gap', 'fairness_pos_rate_gap',\n",
    "                    'fairness_per_group_json',\n",
    "                    'path_roc'\n",
    "                ])\n",
    "\n",
    "                res_df.to_csv(f'{RES_PATH}res_{feature_split_col}_{model_name.replace(\" \", \"_\")}.csv', index=False)\n",
    "    print(f\"Done for {model_name}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cfHfqsPvi3O",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_xception(input_shape, train_dataset, val_dataset, epochs_num):\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    #x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"FITTING FULL XCEPTION\")\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_efficientnetb4(input_shape, train_dataset, val_dataset, epochs_num):\n",
    "    base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    #x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
    "\n",
    "    print(\"TRAINING FULL EfficientNetB4\")\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_efficientnetv2(input_shape, train_dataset, val_dataset, epochs_num):\n",
    "    base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    #base_model = EfficientNetV2M(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    #x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
    "\n",
    "    print(\"TRAINING FULL EfficientNetV2S\")\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_inceptionv3(input_shape, train_dataset, val_dataset, epochs_num):\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"TRAINING FULL InceptionV3\")\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_mobilenetv2(input_shape, train_dataset, val_dataset, epochs_num):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"TRAINING FULL MobileNetV2\")\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
    "    model.fit(train_dataset,\n",
    "              validation_data=val_dataset,\n",
    "              epochs=epochs_num,\n",
    "              callbacks=[early_stopping])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "```\n",
    "perform_tests(df=df,\n",
    "             train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_xception,\n",
    "              preprocess=preprocess_xception,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "perform_tests(df=df,\n",
    "              train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_inceptionv3,\n",
    "              preprocess=preprocess_inception_v3,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "perform_tests(df=df,\n",
    "             train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_efficientnetv2,\n",
    "              preprocess=preprocess_efficientnetv2,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "perform_tests(df=df,\n",
    "              train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_efficientnetb4,\n",
    "              preprocess=preprocess_efficientnet,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "\n",
    "perform_tests(df=df,\n",
    "             train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_simple_cnn,\n",
    "              preprocess=preprocess_simple_cnn,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-22T09:54:26.680Z"
    },
    "id": "yM2K_awwwv2m",
    "outputId": "65cbb83b-b777-4386-d68f-0cd46a28dc09",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "perform_tests(df=df,\n",
    "             train_metas=[\n",
    "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.4, 'female':0.6}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.6, 'female':0.4}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.8, 'female':0.2}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_xception,\n",
    "              preprocess=preprocess_xception,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "perform_tests(df=df,\n",
    "              train_metas=[\n",
    "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.4, 'female':0.6}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.6, 'female':0.4}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.8, 'female':0.2}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_inceptionv3,\n",
    "              preprocess=preprocess_inception_v3,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "perform_tests(df=df,\n",
    "             train_metas=[\n",
    "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.4, 'female':0.6}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.6, 'female':0.4}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.8, 'female':0.2}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_efficientnetv2,\n",
    "              preprocess=preprocess_efficientnetv2,\n",
    "              data_augmentation=augmentation_std\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-22T09:54:26.680Z"
    },
    "id": "ivm4W_DLs7jK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_summarise_res(model_name:str):\n",
    "  path = RES_PATH + f'res_sex_{model_name}.csv'\n",
    "\n",
    "  if not os.path.exists(path):\n",
    "    print(f\"File {path} does not exists!\")\n",
    "    return\n",
    "\n",
    "  res = pd.read_csv(path)\n",
    "  gr = res.groupby(['train_ratio', 'test_ratio']).agg(\n",
    "      # Model=('model_name', 'first'),\n",
    "      TrainRatio=('train_ratio', 'first'),\n",
    "      TestRatio=('test_ratio', 'first'),\n",
    "      Accuracy= ('accuracy', 'mean'),\n",
    "      AccuracySTD= ('accuracy', 'std'),\n",
    "  ).reset_index(drop=True)\n",
    "\n",
    "  gr = gr.round(3).sort_values(by=['TrainRatio', 'TestRatio'], ascending=False)\n",
    "\n",
    "  print(\"MODEL: \" + model_name)\n",
    "  print(tb.tabulate(gr, headers='keys', tablefmt='psql'))\n",
    "  print()\n",
    "  print()\n",
    "\n",
    "print_summarise_res('efficientnetv2')\n",
    "print_summarise_res('xception')\n",
    "print_summarise_res('inceptionv3')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNmMsy80ByKTqTKo1AGnh2X",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7892222,
     "sourceId": 12521871,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
