{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNmMsy80ByKTqTKo1AGnh2X","include_colab_link":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12521871,"sourceType":"datasetVersion","datasetId":7892222}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"REMOVE_RES_CONTENT = True\nIMG_SIZE=224\nIMG_RESIZE=True\nSEED = 42\nRES_PATH = '/kaggle/working/res/'\nDATASET_PATH = '/kaggle/input/deepfakedataset/data/'\nWORKID_DIR_PATH='/kaggle/working/'\nSHOW_EXTRA_INFO=False","metadata":{"id":"jnW2ICuewM0-","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:13:45.220139Z","iopub.execute_input":"2025-07-21T16:13:45.220844Z","iopub.status.idle":"2025-07-21T16:13:45.224524Z","shell.execute_reply.started":"2025-07-21T16:13:45.220819Z","shell.execute_reply":"2025-07-21T16:13:45.223912Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"!pip install -q fairlearn tabulate\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport tabulate as tb\nfrom typing import Dict\nimport tensorflow as tf\nimport re\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Input, BatchNormalization, concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\nfrom tensorflow.keras.applications import Xception, EfficientNetB4, InceptionV3, EfficientNetV2M,EfficientNetV2S \nfrom tensorflow.keras.callbacks import EarlyStopping\nimport os\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, BatchNormalization, LeakyReLU,\n    MaxPooling2D, Flatten, Dense, Dropout, concatenate\n)\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score, log_loss, brier_score_loss\n\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n\ndevice_name = tf.test.gpu_device_name()\nif device_name:\n    print(f\"GPU available: {device_name}\")\nelse:\n    print(\"No GPU available!\")","metadata":{"id":"cPKtaDEKrVlM","outputId":"f35d774d-1a32-46a0-fbb9-1b5dd41d79ad","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-21T16:13:45.242584Z","iopub.execute_input":"2025-07-21T16:13:45.243206Z","iopub.status.idle":"2025-07-21T16:13:48.888754Z","shell.execute_reply.started":"2025-07-21T16:13:45.243184Z","shell.execute_reply":"2025-07-21T16:13:48.887853Z"}},"outputs":[{"name":"stdout","text":"GPU available: /device:GPU:0\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1753114428.877427      36 gpu_device.cc:2022] Created device /device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1753114428.879000      36 gpu_device.cc:2022] Created device /device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"# Dataset splitting - handling","metadata":{}},{"cell_type":"code","source":"file_path = DATASET_PATH + 'metadata.csv'\ndf_tmp = pd.read_csv(file_path, sep=',')\ndf_tmp['path'] = DATASET_PATH + df_tmp['path']\n\ndf_tmp = df_tmp[df_tmp['deepfake'] != 0]\n\ndf_tmp['ethnicity'] = df_tmp.apply(\n    lambda row: 'white' if row['white'] == 1 else ('black' if row['black'] == 1 else (\n        'asian' if row['asian'] == 1 else None)), axis=1)\n\ndf = df_tmp[['deepfake', 'male', 'ethnicity', 'eyeglasses', 'heavy_makeup', 'big_lips', 'path']]\n\ndf = df.rename(columns={'deepfake': 'type', 'male': 'sex', 'heavy_makeup': 'makeup', 'big_lips': 'lips',})\n\ndf['type'] = df['type'].replace({1: 'fake', -1: 'real'})\ndf['sex'] = df['sex'].replace({-1: 'female', 0: None, 1: 'male'})\ndf['makeup'] = df['makeup'].replace({-1: 'no', 0: None, 1: 'yes'})\ndf['lips'] = df['lips'].replace({-1: 'small', 0: None, 1: 'big'})\ndf['eyeglasses'] = df['eyeglasses'].replace({-1: 'no', 0: None, 1: 'yes'})\n\n\nprint(tb.tabulate(df.head(), headers='keys', tablefmt='psql'))\nprint(f\"Dataset size: {len(df)}\")","metadata":{"id":"BQHjq4DXrY3h","outputId":"ee466649-50e5-43a4-c318-b6aedc03dc0d","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-21T16:13:48.899913Z","iopub.execute_input":"2025-07-21T16:13:48.900170Z","iopub.status.idle":"2025-07-21T16:13:49.593356Z","shell.execute_reply.started":"2025-07-21T16:13:48.900148Z","shell.execute_reply":"2025-07-21T16:13:49.592577Z"}},"outputs":[{"name":"stdout","text":"+----+--------+--------+-------------+--------------+----------+--------+--------------------------------------------------------------+\n|    | type   | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                                                         |\n|----+--------+--------+-------------+--------------+----------+--------+--------------------------------------------------------------|\n|  0 | real   | male   |             | yes          | no       | big    | /kaggle/input/deepfakedataset/data/original/805/frame271.jpg |\n|  1 | real   | female | white       | no           |          | big    | /kaggle/input/deepfakedataset/data/original/083/frame191.jpg |\n|  2 | real   | male   | white       | no           | no       | small  | /kaggle/input/deepfakedataset/data/original/878/frame111.jpg |\n|  3 | real   | female | white       | no           |          |        | /kaggle/input/deepfakedataset/data/original/158/frame201.jpg |\n|  4 | real   | female | white       | no           |          |        | /kaggle/input/deepfakedataset/data/original/606/frame71.jpg  |\n+----+--------+--------+-------------+--------------+----------+--------+--------------------------------------------------------------+\nDataset size: 59552\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"def get_balanced_subset(\n    df, class_col, feature_col, feature_value,\n    samples_per_class, randomize=True, reset_index=False\n):\n    \"\"\"\n    Select a balanced subset of the data for a given feature value, with equal number of samples per class.\n\n    Args:\n        df: DataFrame\n        class_col: column name of class labels\n        feature_col: column name of feature\n        feature_value: specific feature value to filter\n        samples_per_class: number of samples per class\n        randomize: whether to shuffle within class before selecting\n        reset_index: whether to reset index of returned DataFrame\n        seed: random seed for reproducibility\n\n    Returns:\n        Balanced DataFrame subset\n    \"\"\"\n    tmp = df[df[feature_col] == feature_value]\n\n    counts = tmp[class_col].value_counts()\n    for cl, count in counts.items():\n        if count < samples_per_class:\n            raise ValueError(f\"Not enough samples for class '{cl}' in feature '{feature_value}'. \"\n                             f\"Required: {samples_per_class}, Available: {count}\")\n\n    tmp = pd.concat([\n        (g.sample(frac=1, random_state=SEED).head(samples_per_class) if randomize else g.head(samples_per_class))\n        for _, g in tmp.groupby(class_col)\n    ])\n\n    if reset_index:\n        tmp = tmp.reset_index(drop=True)\n\n    return tmp\n\ntmp_test = get_balanced_subset(\n    df=df, class_col='type', feature_col='sex', feature_value='male',\n    samples_per_class=2, randomize=True, reset_index=True)\nprint(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))","metadata":{"id":"jqJXku-tsxK7","outputId":"03c62d12-4bff-4e41-cd9b-7d694cc52732","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-21T16:13:49.594125Z","iopub.execute_input":"2025-07-21T16:13:49.594402Z","iopub.status.idle":"2025-07-21T16:13:49.625262Z","shell.execute_reply.started":"2025-07-21T16:13:49.594373Z","shell.execute_reply":"2025-07-21T16:13:49.624585Z"}},"outputs":[{"name":"stdout","text":"+----+--------+-------+-------------+--------------+----------+--------+------------------------------------------------------------------+\n|    | type   | sex   | ethnicity   | eyeglasses   | makeup   | lips   | path                                                             |\n|----+--------+-------+-------------+--------------+----------+--------+------------------------------------------------------------------|\n|  0 | fake   | male  | white       | no           | no       | small  | /kaggle/input/deepfakedataset/data/deepfake/374_407/frame41.jpg  |\n|  1 | fake   | male  | white       | no           | no       | small  | /kaggle/input/deepfakedataset/data/deepfake/015_919/frame281.jpg |\n|  2 | real   | male  |             | no           | no       | big    | /kaggle/input/deepfakedataset/data/original/995/frame11.jpg      |\n|  3 | real   | male  | white       | no           | no       |        | /kaggle/input/deepfakedataset/data/original/579/frame201.jpg     |\n+----+--------+-------+-------------+--------------+----------+--------+------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"def get_exp_data(df, class_col, feature_col, ratio : Dict, size, randomize=True, exclude_column=None, exclude_df=None, max_diff=0.05):\n    '''\n    Get a balanced subset of the data based on specified ratios for features.\n    Args:\n        df: DataFrame containing the data\n        class_col: column name for class labels\n        feature_col: column name for features\n        ratio: dictionary with feature values as keys and their ratios as values\n        size: total number of samples to return\n        randomize: whether to shuffle the DataFrame before processing\n        exclude_column: column name to exclude from the DataFrame\n        exclude_df: DataFrame containing values to exclude based on exclude_column\n    '''\n    if randomize:\n        df_rnd = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n    else:\n        df_rnd = df.copy()\n\n    if exclude_column is not None and exclude_df is not None:\n        if exclude_column not in df_rnd.columns:\n            raise ValueError(f\"Column '{exclude_column}' not found in DataFrame.\")\n        if exclude_column not in exclude_df.columns:\n            raise ValueError(f\"Column '{exclude_column}' not found in exclude DataFrame.\")\n        df_rnd = df_rnd[~df_rnd[exclude_column].isin(exclude_df[exclude_column])]\n\n    uniq_classes = df_rnd[class_col].unique()\n    uniq_features = df_rnd[feature_col].unique()\n\n    def get_exp_data_inner(tmp_df, size):\n        df_tmp = None\n        for uf in uniq_features:\n            if ratio.get(uf) is None:\n                if SHOW_EXTRA_INFO:\n                    print(f\"Feature '{uf}' not found in ratios. Skipping.\")\n                continue\n            c_amt = int(size * ratio[uf] / len(uniq_classes))\n            # if c_amt <= 0:\n            #     raise ValueError(f\"Calculated samples per class ({c_amt}) is less than or equal to zero for feature '{uf}' with ratio {ratio}.\")\n            tmp = get_balanced_subset(df=tmp_df, class_col=class_col, feature_col=feature_col, feature_value=uf,\n                                        samples_per_class=c_amt, randomize=False)\n            if df_tmp is None:\n                df_tmp = tmp\n            else:\n                df_tmp = pd.concat([df_tmp, tmp])\n        return df_tmp\n\n    df_res = get_exp_data_inner(df_rnd, size)\n\n    if len(df_res) < size and SHOW_EXTRA_INFO:\n        print(f\"Samples for ({len(df_res)}) are less than requested ({size}).\")\n\n    ratios_fet = df_res[feature_col].value_counts(normalize=True).to_dict()\n    ratios_cls = df_res[class_col].value_counts(normalize=False).to_dict()\n\n    if SHOW_EXTRA_INFO:\n        print(f\"[] Ratios for {feature_col}: {ratios_fet}\")\n        print(f\"[] Ratios for {class_col}: {ratios_cls}\")\n        print()\n        \n\n    for k in ratio:\n        if ratios_fet.get(k) is None:\n            if ratio[k] > 0.0:\n                raise ValueError(f\"Feature '{k}' not found in DataFrame after sampling (try increase 'size' parameter).\")\n        elif abs(ratios_fet[k] - ratio[k]) > max_diff:\n            raise ValueError(f\"Feature '{k}' ratio {ratios_fet[k]} differs from requested {ratio[k]} by more than {max_diff}.\")\n\n    \n\n    df_res = df_res.reset_index(drop=True)\n\n    return df_res\n\ntmp_test = get_exp_data(\n    df=df, class_col='type', feature_col='ethnicity', ratio={'white':0.2, 'black':0.6, 'asian': 0.2}, size=10)\nprint(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))","metadata":{"id":"JJZmhWnis0yh","outputId":"555bdf23-ce59-42e2-da9c-dffb8e307822","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-21T16:13:49.626769Z","iopub.execute_input":"2025-07-21T16:13:49.626974Z","iopub.status.idle":"2025-07-21T16:13:49.692949Z","shell.execute_reply.started":"2025-07-21T16:13:49.626959Z","shell.execute_reply":"2025-07-21T16:13:49.692028Z"}},"outputs":[{"name":"stdout","text":"+----+--------+--------+-------------+--------------+----------+--------+------------------------------------------------------------------+\n|    | type   | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                                                             |\n|----+--------+--------+-------------+--------------+----------+--------+------------------------------------------------------------------|\n|  0 | fake   | male   | white       | no           | no       | small  | /kaggle/input/deepfakedataset/data/deepfake/594_530/frame121.jpg |\n|  1 | real   | female | white       | no           |          |        | /kaggle/input/deepfakedataset/data/original/240/frame41.jpg      |\n|  2 | fake   | female | asian       |              |          | big    | /kaggle/input/deepfakedataset/data/deepfake/249_280/frame261.jpg |\n|  3 | real   | female | asian       | no           |          | big    | /kaggle/input/deepfakedataset/data/original/758/frame161.jpg     |\n|  4 | fake   | female | black       | no           |          | big    | /kaggle/input/deepfakedataset/data/deepfake/986_994/frame271.jpg |\n|  5 | fake   | male   | black       | no           | no       | big    | /kaggle/input/deepfakedataset/data/deepfake/144_122/frame101.jpg |\n|  6 | fake   | male   | black       | yes          | no       | big    | /kaggle/input/deepfakedataset/data/deepfake/081_087/frame41.jpg  |\n|  7 | real   | male   | black       | no           | no       | big    | /kaggle/input/deepfakedataset/data/original/715/frame231.jpg     |\n|  8 | real   | female | black       | no           |          | big    | /kaggle/input/deepfakedataset/data/original/762/frame61.jpg      |\n|  9 | real   | female | black       | no           |          | big    | /kaggle/input/deepfakedataset/data/original/328/frame241.jpg     |\n+----+--------+--------+-------------+--------------+----------+--------+------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"# Methods for testing","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficientnet\nfrom tensorflow.keras.applications.xception import preprocess_input as preprocess_xception\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input as preprocess_inception_v3\nfrom tensorflow.keras.applications.efficientnet_v2 import preprocess_input as preprocess_efficientnetv2\n\naugmentation_std = tf.keras.Sequential([\n    #layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.05),\n    layers.RandomZoom(0.05),\n    #layers.RandomContrast(0.05),\n    #layers.RandomBrightness(0.05),\n], name=\"augmentation_std\")\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ndef load_image(file_path, label, preprocess, augment):\n    image = tf.io.read_file(file_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n\n    if IMG_RESIZE:\n        image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    else:\n        image = tf.cast(image, tf.float32)\n        \n    if preprocess:\n        image = preprocess(image)\n\n    if augment:\n        image = augment(image)\n\n    return image, label\n\ndef get_data_for_model(df, class_col, files_col, batch_size, preprocess, augment):\n    image_paths = df[files_col].values\n    labels = df[class_col].astype('category').cat.codes.values\n\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n    dataset = dataset.map(lambda path, label: load_image(path, label, preprocess, augment), num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n\n    return dataset","metadata":{"id":"LNCRWX5js2d7","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:13:49.693741Z","iopub.execute_input":"2025-07-21T16:13:49.693972Z","iopub.status.idle":"2025-07-21T16:13:49.717446Z","shell.execute_reply.started":"2025-07-21T16:13:49.693954Z","shell.execute_reply":"2025-07-21T16:13:49.716733Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# Create folder if it doesn't exist\nif not os.path.exists(RES_PATH):\n    os.makedirs(RES_PATH)\n    print(f\"Created folder: {RES_PATH}\")\nelif REMOVE_RES_CONTENT:\n    # Remove all files inside the folder\n    for filename in os.listdir(RES_PATH):\n        file_path = os.path.join(RES_PATH, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)          # remove file or link\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)      # remove folder and contents\n        except Exception as e:\n            print(f'Failed to delete {file_path}. Reason: {e}')\n    print(f\"Cleared contents of folder: {RES_PATH}\")","metadata":{"id":"iYJbzSoMv9Vd","outputId":"07789495-ad79-42f3-eacb-3cc9507b3425","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-21T16:13:49.718357Z","iopub.execute_input":"2025-07-21T16:13:49.718648Z","iopub.status.idle":"2025-07-21T16:13:49.728209Z","shell.execute_reply.started":"2025-07-21T16:13:49.718627Z","shell.execute_reply":"2025-07-21T16:13:49.727547Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"def get_done_reps(model_name, feature_name, amt_per_rep):\n  results_path = f'{RES_PATH}res_{feature_name}_{model_name.replace(\" \", \"_\")}.csv'\n  if not os.path.exists(results_path):\n    return [], None\n\n  tmp_df = pd.read_csv(results_path)\n  dones = []\n  for r in tmp_df[\"rep\"].unique():\n    amt = len(tmp_df[tmp_df[\"rep\"]==r])\n    if amt == amt_per_rep:\n      dones.append(r)\n\n  return dones, tmp_df","metadata":{"id":"CQ_JQedB1623","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-21T16:13:49.729102Z","iopub.execute_input":"2025-07-21T16:13:49.729389Z","iopub.status.idle":"2025-07-21T16:13:49.749902Z","shell.execute_reply.started":"2025-07-21T16:13:49.729368Z","shell.execute_reply":"2025-07-21T16:13:49.749033Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"def perform_tests(df, train_metas, test_metas, validation_size, reps, feature_split_col, get_model, preprocess, class_col='type', exclude_column='path', \n                  files_col='path', data_augmentation=False, epochs_num=15, batch_size=64):\n    res = []\n\n    model_name = get_model.__name__.replace(\"create_\", \"\")\n\n    done_reps, prev_results = get_done_reps(model_name, feature_split_col, len(test_metas) * len(train_metas))\n\n    for r in range(reps):\n        if r in done_reps:\n          res.extend(prev_results[prev_results['rep']==r].values.tolist())\n          print(f\"Rep {r} already done for {model_name}. Skipping...\")\n          continue\n\n        np.random.seed(SEED + r)\n        tf.random.set_seed(SEED + r)\n\n        for train_meta in train_metas:\n            train_val = get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=train_meta['ratio'], size=train_meta['size'] + validation_size)\n            train_val = train_val.sample(frac=1, random_state=SEED+r).reset_index(drop=True)\n\n            stratify_key = train_val[class_col].astype(str) + \"_\" + train_val[feature_split_col].astype(str)\n\n            train, val = train_test_split(\n                train_val,\n                test_size=validation_size / (train_meta['size'] + validation_size),\n                stratify=stratify_key,\n                random_state=SEED + r\n            )\n\n            train = train.reset_index(drop=True)\n            val = val.reset_index(drop=True)\n\n            tests = [\n                get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=tm['ratio'], size=tm['size'], exclude_column=exclude_column, exclude_df=train) for tm in test_metas\n            ]\n\n            train_dataset = get_data_for_model(train, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=data_augmentation)\n            val_dataset = get_data_for_model(val, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=False)\n            test_datasets = [\n                get_data_for_model(test, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=False) for test in tests\n            ]\n\n            train_ratio = '/'.join([f\"{k}:{v}\" for k, v in train_meta['ratio'].items()])\n            train_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in train[feature_split_col].value_counts(normalize=True).to_dict().items()])\n            train_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', train_ratio)\n\n            model = get_model(input_shape=(IMG_SIZE,IMG_SIZE,3), \n                              train_dataset=train_dataset, val_dataset=val_dataset, \n                              epochs_num=epochs_num)\n\n            for test_dataset, test_meta, test_df in zip(test_datasets, test_metas, tests):\n                predictions = model.predict(test_dataset)\n                y_true = test_df[class_col].astype('category').cat.codes.values\n                y_pred = (predictions > 0.5).astype(int).flatten()\n                acc = accuracy_score(y_true, y_pred)\n                f1 = f1_score(y_true, y_pred, average='weighted')\n                eo_diff = equalized_odds_difference(y_true, y_pred, sensitive_features=test_df[feature_split_col])\n                precision = precision_score(y_true, y_pred, average='weighted')\n                recall = recall_score(y_true, y_pred, average='weighted')\n                try:\n                    roc_auc = roc_auc_score(y_true, predictions)\n                except:\n                    roc_auc = np.nan\n                \n                try:\n                    logloss = log_loss(y_true, predictions)\n                except:\n                    logloss = np.nan\n                \n                try:\n                    brier = brier_score_loss(y_true, predictions)\n                except:\n                    brier = np.nan\n                \n                cm = confusion_matrix(y_true, y_pred)\n                TN, FP, FN, TP = cm.ravel() if cm.size == 4 else (np.nan, np.nan, np.nan, np.nan)\n                specificity = TN / (TN + FP) if (TN + FP) != 0 else np.nan\n\n                \n                test_ratio = '/'.join([f\"{k}:{v}\" for k, v in test_meta['ratio'].items()])\n                test_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in test_df[feature_split_col].value_counts(normalize=True).to_dict().items()])\n                test_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', test_ratio)\n\n                res.append([\n                    r,\n                    model_name,\n                    feature_split_col,\n                    train_meta['size'],\n                    train_ratio,\n                    test_meta['size'],\n                    test_ratio,\n                    acc,\n                    f1,\n                    eo_diff,\n                    train_ratio_rel,\n                    test_ratio_rel,\n                    train_ratio_sim,\n                    test_ratio_sim,\n                    precision,\n                    recall,\n                    roc_auc,\n                    logloss,\n                    brier,\n                    TP, TN, FP, FN, specificity\n                ])\n\n                print(f\"Rep: {r:2} | Model: {model_name} | Feature Split: {feature_split_col} | Ratio: {test_ratio} | Acc: {acc:.2f}\")\n\n                res_df = pd.DataFrame(res, columns=[\n                    'rep', 'model_name', 'feature_split_col',\n                    'train_size', 'train_ratio_detail', 'test_size', 'test_ratio_detail',\n                    'accuracy', 'f1_score', 'eo_diff', 'train_ratio_rel', 'test_ratio_rel', \"train_ratio\", \"test_ratio\",\n                    \"precision\", \"recall\", \"roc_auc\", \"logloss\", \"brier\", \"TP\", \"TN\", \"FP\", \"FN\", \"specificity\" \n                    \n                ])\n\n                res_df.to_csv(f'{RES_PATH}res_{feature_split_col}_{model_name.replace(\" \", \"_\")}.csv', index=False)\n    print(f\"Done for {model_name}.\")\n","metadata":{"id":"ik6Z-AUis6G7","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:13:49.750804Z","iopub.execute_input":"2025-07-21T16:13:49.751077Z","iopub.status.idle":"2025-07-21T16:13:49.769382Z","shell.execute_reply.started":"2025-07-21T16:13:49.751056Z","shell.execute_reply":"2025-07-21T16:13:49.768633Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":"# Models implementation","metadata":{}},{"cell_type":"code","source":"def create_xception(input_shape, train_dataset, val_dataset, epochs_num):\n    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = True\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    #x = Dense(128, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    predictions = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    model.compile(optimizer=Adam(learning_rate=1e-4),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n    print(\"FITTING FULL XCEPTION\")\n    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n\n    return model","metadata":{"id":"_cfHfqsPvi3O","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:13:49.770300Z","iopub.execute_input":"2025-07-21T16:13:49.770630Z","iopub.status.idle":"2025-07-21T16:13:49.789442Z","shell.execute_reply.started":"2025-07-21T16:13:49.770600Z","shell.execute_reply":"2025-07-21T16:13:49.788693Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"def create_efficientnetb4(input_shape, train_dataset, val_dataset, epochs_num):\n    base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = True \n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    #x = Dense(128, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    predictions = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n    model.compile(optimizer=Adam(learning_rate=1e-4),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n\n    print(\"TRAINING FULL EfficientNetB4\")\n    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:13:49.791851Z","iopub.execute_input":"2025-07-21T16:13:49.792117Z","iopub.status.idle":"2025-07-21T16:13:49.809823Z","shell.execute_reply.started":"2025-07-21T16:13:49.792100Z","shell.execute_reply":"2025-07-21T16:13:49.809080Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"def create_efficientnetv2(input_shape, train_dataset, val_dataset, epochs_num):\n    base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=input_shape)\n    #base_model = EfficientNetV2M(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = True \n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    #x = Dense(128, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    predictions = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n    model.compile(optimizer=Adam(learning_rate=1e-4),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n\n    print(\"TRAINING FULL EfficientNetV2S\")\n    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:13:49.810677Z","iopub.execute_input":"2025-07-21T16:13:49.811003Z","iopub.status.idle":"2025-07-21T16:13:49.825836Z","shell.execute_reply.started":"2025-07-21T16:13:49.810980Z","shell.execute_reply":"2025-07-21T16:13:49.825039Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"def create_inceptionv3(input_shape, train_dataset, val_dataset, epochs_num):\n    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = True\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(0.3)(x)\n    predictions = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n    model.compile(optimizer=Adam(learning_rate=1e-4),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n    print(\"TRAINING FULL InceptionV3\")\n    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:13:49.826551Z","iopub.execute_input":"2025-07-21T16:13:49.826813Z","iopub.status.idle":"2025-07-21T16:13:49.844027Z","shell.execute_reply.started":"2025-07-21T16:13:49.826795Z","shell.execute_reply":"2025-07-21T16:13:49.843468Z"}},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":"# Tests starter","metadata":{}},{"cell_type":"markdown","source":"perform_tests(df=df,\n             train_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n                  ],\n              test_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n              ],\n              validation_size=500,\n              reps=10,\n              feature_split_col='sex',\n              get_model=create_xception,\n              preprocess=preprocess_xception,\n              data_augmentation=augmentation_std\n              )\n\nperform_tests(df=df,\n               train_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n                  ],\n              test_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n              ],\n              validation_size=500,\n              reps=10,\n              feature_split_col='sex',\n              get_model=create_inceptionv3,\n              preprocess=preprocess_inception_v3,\n              data_augmentation=augmentation_std\n              )\n\nperform_tests(df=df,\n              train_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n                  ],\n              test_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n              ],\n              validation_size=500,\n              reps=10,\n              feature_split_col='sex',\n              get_model=create_efficientnetv2,\n              preprocess=preprocess_efficientnetv2,\n              data_augmentation=augmentation_std\n              )\n\n\n\n\n\n\nperform_tests(df=df,\n              train_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n                  ],\n              test_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n              ],\n              validation_size=500,\n              reps=10,\n              feature_split_col='sex',\n              get_model=create_efficientnetb4,\n              preprocess=preprocess_efficientnet,\n              data_augmentation=augmentation_std\n              )\n","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"perform_tests(df=df,\n             train_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n                  ],\n              test_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n              ],\n              validation_size=500,\n              reps=10,\n              feature_split_col='sex',\n              get_model=create_xception,\n              preprocess=preprocess_xception,\n              data_augmentation=augmentation_std\n              )\n\nperform_tests(df=df,\n              train_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n                  ],\n              test_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n              ],\n              validation_size=500,\n              reps=10,\n              feature_split_col='sex',\n              get_model=create_inceptionv3,\n              preprocess=preprocess_inception_v3,\n              data_augmentation=augmentation_std\n              )\n\nperform_tests(df=df,\n             train_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n                  ],\n              test_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n              ],\n              validation_size=500,\n              reps=10,\n              feature_split_col='sex',\n              get_model=create_efficientnetv2,\n              preprocess=preprocess_efficientnetv2,\n              data_augmentation=augmentation_std\n              )","metadata":{"execution":{"iopub.status.busy":"2025-07-21T16:13:49.844917Z","iopub.execute_input":"2025-07-21T16:13:49.845183Z","iopub.status.idle":"2025-07-21T16:51:59.041962Z","shell.execute_reply.started":"2025-07-21T16:13:49.845162Z","shell.execute_reply":"2025-07-21T16:51:59.041053Z"},"id":"yM2K_awwwv2m","outputId":"65cbb83b-b777-4386-d68f-0cd46a28dc09","trusted":true},"outputs":[{"name":"stdout","text":"Rep 0 already done for xception. Skipping...\nRep 1 already done for xception. Skipping...\nDone for xception.\nRep 0 already done for inceptionv3. Skipping...\nRep 1 already done for inceptionv3. Skipping...\nDone for inceptionv3.\nTRAINING FULL EfficientNetV2S\nEpoch 1/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 2s/step - accuracy: 0.5317 - loss: 0.6997 - val_accuracy: 0.6620 - val_loss: 0.6008\nEpoch 2/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 940ms/step - accuracy: 0.7442 - loss: 0.5141 - val_accuracy: 0.8440 - val_loss: 0.3781\nEpoch 3/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 933ms/step - accuracy: 0.8610 - loss: 0.3027 - val_accuracy: 0.8940 - val_loss: 0.2486\nEpoch 4/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 926ms/step - accuracy: 0.9219 - loss: 0.1873 - val_accuracy: 0.9060 - val_loss: 0.2479\nEpoch 5/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 928ms/step - accuracy: 0.9303 - loss: 0.1656 - val_accuracy: 0.9320 - val_loss: 0.1745\nEpoch 6/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 931ms/step - accuracy: 0.9600 - loss: 0.1005 - val_accuracy: 0.9400 - val_loss: 0.1468\nEpoch 7/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 930ms/step - accuracy: 0.9634 - loss: 0.0929 - val_accuracy: 0.9320 - val_loss: 0.2031\nEpoch 8/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 931ms/step - accuracy: 0.9726 - loss: 0.0704 - val_accuracy: 0.9480 - val_loss: 0.1723\nEpoch 9/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 933ms/step - accuracy: 0.9800 - loss: 0.0555 - val_accuracy: 0.9560 - val_loss: 0.1591\nEpoch 10/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 927ms/step - accuracy: 0.9845 - loss: 0.0413 - val_accuracy: 0.9380 - val_loss: 0.1506\nEpoch 11/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 930ms/step - accuracy: 0.9873 - loss: 0.0337 - val_accuracy: 0.9340 - val_loss: 0.2122\nEpoch 11: early stopping\nRestoring model weights from the end of the best epoch: 9.\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step \nRep:  0 | Model: efficientnetv2 | Feature Split: sex | Ratio: male:0.5/female:0.5 | Acc: 0.96\nTRAINING FULL EfficientNetV2S\nEpoch 1/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 2s/step - accuracy: 0.5299 - loss: 0.6999 - val_accuracy: 0.6360 - val_loss: 0.6166\nEpoch 2/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 938ms/step - accuracy: 0.7195 - loss: 0.5477 - val_accuracy: 0.8040 - val_loss: 0.3981\nEpoch 3/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 929ms/step - accuracy: 0.8539 - loss: 0.3375 - val_accuracy: 0.8760 - val_loss: 0.2928\nEpoch 4/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 933ms/step - accuracy: 0.9065 - loss: 0.2132 - val_accuracy: 0.9140 - val_loss: 0.1981\nEpoch 5/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 932ms/step - accuracy: 0.9379 - loss: 0.1512 - val_accuracy: 0.9340 - val_loss: 0.1806\nEpoch 6/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 931ms/step - accuracy: 0.9564 - loss: 0.1201 - val_accuracy: 0.9380 - val_loss: 0.1764\nEpoch 7/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 929ms/step - accuracy: 0.9623 - loss: 0.0958 - val_accuracy: 0.9120 - val_loss: 0.2238\nEpoch 8/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 932ms/step - accuracy: 0.9713 - loss: 0.0788 - val_accuracy: 0.9400 - val_loss: 0.1493\nEpoch 9/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 930ms/step - accuracy: 0.9803 - loss: 0.0551 - val_accuracy: 0.9520 - val_loss: 0.1550\nEpoch 10/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 931ms/step - accuracy: 0.9802 - loss: 0.0486 - val_accuracy: 0.9540 - val_loss: 0.1648\nEpoch 11/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 929ms/step - accuracy: 0.9817 - loss: 0.0466 - val_accuracy: 0.9480 - val_loss: 0.1922\nEpoch 12/15\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 944ms/step - accuracy: 0.9841 - loss: 0.0435 - val_accuracy: 0.9320 - val_loss: 0.1735\nEpoch 12: early stopping\nRestoring model weights from the end of the best epoch: 10.\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3s/step \nRep:  1 | Model: efficientnetv2 | Feature Split: sex | Ratio: male:0.5/female:0.5 | Acc: 0.95\nDone for efficientnetv2.\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"def print_summarise_res(model_name:str):\n  path = RES_PATH + f'res_sex_{model_name}.csv'\n\n  if not os.path.exists(path):\n    print(f\"File {path} does not exists!\")\n    return\n\n  res = pd.read_csv(path)\n  gr = res.groupby(['train_ratio', 'test_ratio']).agg(\n      # Model=('model_name', 'first'),\n      TrainRatio=('train_ratio', 'first'),\n      TestRatio=('test_ratio', 'first'),\n      Accuracy= ('accuracy', 'mean'),\n      AccuracySTD= ('accuracy', 'std'),\n      F1=('f1_score', 'mean'),\n      F1STD=('f1_score', 'std'),\n      EODiff=('eo_diff', 'mean'),\n      EODiffSTD=('eo_diff', 'std'),\n  ).reset_index(drop=True)\n\n  gr = gr.round(3).sort_values(by=['TrainRatio', 'TestRatio'], ascending=False)\n\n  print(\"MODEL: \" + model_name)\n  print(tb.tabulate(gr, headers='keys', tablefmt='psql'))\n  print()\n  print()\n\nprint_summarise_res('efficientnetv2')\nprint_summarise_res('xception')\nprint_summarise_res('inceptionv3')","metadata":{"id":"ivm4W_DLs7jK","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:51:59.044450Z","iopub.execute_input":"2025-07-21T16:51:59.044673Z","iopub.status.idle":"2025-07-21T16:51:59.088405Z","shell.execute_reply.started":"2025-07-21T16:51:59.044656Z","shell.execute_reply":"2025-07-21T16:51:59.087711Z"}},"outputs":[{"name":"stdout","text":"File /kaggle/working/res/res_sex_efficientnetb4.csv does not exists!\nMODEL: efficientnetv2\n+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n|    | TrainRatio   | TestRatio   |   Accuracy |   AccuracySTD |    F1 |   F1STD |   EODiff |   EODiffSTD |\n|----+--------------+-------------+------------+---------------+-------+---------+----------+-------------|\n|  0 | 5/5          | 5/5         |      0.955 |         0.001 | 0.955 |   0.001 |     0.02 |       0.017 |\n+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n\n\nMODEL: xception\n+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n|    | TrainRatio   | TestRatio   |   Accuracy |   AccuracySTD |    F1 |   F1STD |   EODiff |   EODiffSTD |\n|----+--------------+-------------+------------+---------------+-------+---------+----------+-------------|\n|  0 | 5/5          | 5/5         |      0.915 |         0.016 | 0.915 |   0.016 |     0.04 |       0.011 |\n+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n\n\nMODEL: inceptionv3\n+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n|    | TrainRatio   | TestRatio   |   Accuracy |   AccuracySTD |    F1 |   F1STD |   EODiff |   EODiffSTD |\n|----+--------------+-------------+------------+---------------+-------+---------+----------+-------------|\n|  0 | 5/5          | 5/5         |      0.931 |         0.018 | 0.931 |   0.018 |    0.044 |       0.028 |\n+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n\n\n","output_type":"stream"}],"execution_count":74}]}