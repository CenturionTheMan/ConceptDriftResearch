{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNmMsy80ByKTqTKo1AGnh2X","include_colab_link":true},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12504781,"sourceType":"datasetVersion","datasetId":7892222}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"REMOVE_RES_CONTENT = False\n# IMG_SIZE=299\nIMG_SIZE=224\nIMG_RESIZE = False\nSEED = 42\nRES_PATH = '/kaggle/working/res/'\nDATASET_PATH = '/kaggle/input/deepfakedataset/data/'\nWORKID_DIR_PATH='/kaggle/working/'","metadata":{"execution":{"iopub.status.busy":"2025-07-18T07:15:17.187216Z","iopub.execute_input":"2025-07-18T07:15:17.187876Z","iopub.status.idle":"2025-07-18T07:15:17.198255Z","shell.execute_reply.started":"2025-07-18T07:15:17.187846Z","shell.execute_reply":"2025-07-18T07:15:17.197748Z"},"id":"jnW2ICuewM0-","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q fairlearn tabulate\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport tabulate as tb\nfrom typing import Dict\nimport tensorflow as tf\nimport re\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tensorflow.keras.applications import MobileNetV2\nfrom fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\nfrom tensorflow.keras.applications import Xception, EfficientNetB0, ResNet50\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport os\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n\ndevice_name = tf.test.gpu_device_name()\nif device_name:\n    print(f\"GPU available: {device_name}\")\nelse:\n    print(\"No GPU available!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-07-18T07:15:17.199168Z","iopub.execute_input":"2025-07-18T07:15:17.199397Z","iopub.status.idle":"2025-07-18T07:15:38.210686Z","shell.execute_reply.started":"2025-07-18T07:15:17.199380Z","shell.execute_reply":"2025-07-18T07:15:38.209675Z"},"id":"cPKtaDEKrVlM","outputId":"f35d774d-1a32-46a0-fbb9-1b5dd41d79ad","trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"2025-07-18 07:15:23.993364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752822924.232817      74 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752822924.297442      74 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"GPU available: /device:GPU:0\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1752822938.205222      74 gpu_device.cc:2022] Created device /device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1752822938.206027      74 gpu_device.cc:2022] Created device /device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"file_path = DATASET_PATH + 'metadata.csv'\ndf_tmp = pd.read_csv(file_path, sep=',')\ndf_tmp['path'] = DATASET_PATH + df_tmp['path']\n\ndf_tmp = df_tmp[df_tmp['deepfake'] != 0]\n\ndf_tmp['ethnicity'] = df_tmp.apply(\n    lambda row: 'white' if row['white'] == 1 else ('black' if row['black'] == 1 else (\n        'asian' if row['asian'] == 1 else None)), axis=1)\n\ndf = df_tmp[['deepfake', 'male', 'ethnicity', 'eyeglasses', 'heavy_makeup', 'big_lips', 'path']]\n\ndf = df.rename(columns={'deepfake': 'type', 'male': 'sex', 'heavy_makeup': 'makeup', 'big_lips': 'lips',})\n\ndf['type'] = df['type'].replace({1: 'fake', -1: 'real'})\ndf['sex'] = df['sex'].replace({-1: 'female', 0: None, 1: 'male'})\ndf['makeup'] = df['makeup'].replace({-1: 'no', 0: None, 1: 'yes'})\ndf['lips'] = df['lips'].replace({-1: 'small', 0: None, 1: 'big'})\ndf['eyeglasses'] = df['eyeglasses'].replace({-1: 'no', 0: None, 1: 'yes'})\n\n\nprint(tb.tabulate(df.head(), headers='keys', tablefmt='psql'))\nprint(f\"Dataset size: {len(df)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-07-18T07:15:38.212017Z","iopub.execute_input":"2025-07-18T07:15:38.212631Z","iopub.status.idle":"2025-07-18T07:15:39.019620Z","shell.execute_reply.started":"2025-07-18T07:15:38.212593Z","shell.execute_reply":"2025-07-18T07:15:39.018659Z"},"id":"BQHjq4DXrY3h","outputId":"ee466649-50e5-43a4-c318-b6aedc03dc0d","trusted":true},"outputs":[{"name":"stdout","text":"+----+--------+--------+-------------+--------------+----------+--------+--------------------------------------------------------------+\n|    | type   | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                                                         |\n|----+--------+--------+-------------+--------------+----------+--------+--------------------------------------------------------------|\n|  0 | real   | male   |             | yes          | no       | big    | /kaggle/input/deepfakedataset/data/original/805/frame271.jpg |\n|  1 | real   | female | white       | no           |          | big    | /kaggle/input/deepfakedataset/data/original/083/frame191.jpg |\n|  2 | real   | male   | white       | no           | no       | small  | /kaggle/input/deepfakedataset/data/original/878/frame111.jpg |\n|  3 | real   | female | white       | no           |          |        | /kaggle/input/deepfakedataset/data/original/158/frame201.jpg |\n|  4 | real   | female | white       | no           |          |        | /kaggle/input/deepfakedataset/data/original/606/frame71.jpg  |\n+----+--------+--------+-------------+--------------+----------+--------+--------------------------------------------------------------+\nDataset size: 59552\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def get_balanced_subset(\n    df, class_col, feature_col, feature_value,\n    samples_per_class, randomize=True, reset_index=False\n):\n    \"\"\"\n    Select a balanced subset of the data for a given feature value, with equal number of samples per class.\n\n    Args:\n        df: DataFrame\n        class_col: column name of class labels\n        feature_col: column name of feature\n        feature_value: specific feature value to filter\n        samples_per_class: number of samples per class\n        randomize: whether to shuffle within class before selecting\n        reset_index: whether to reset index of returned DataFrame\n        seed: random seed for reproducibility\n\n    Returns:\n        Balanced DataFrame subset\n    \"\"\"\n    tmp = df[df[feature_col] == feature_value]\n\n    counts = tmp[class_col].value_counts()\n    for cl, count in counts.items():\n        if count < samples_per_class:\n            raise ValueError(f\"Not enough samples for class '{cl}' in feature '{feature_value}'. \"\n                             f\"Required: {samples_per_class}, Available: {count}\")\n\n    tmp = pd.concat([\n        (g.sample(frac=1, random_state=SEED).head(samples_per_class) if randomize else g.head(samples_per_class))\n        for _, g in tmp.groupby(class_col)\n    ])\n\n    if reset_index:\n        tmp = tmp.reset_index(drop=True)\n\n    return tmp\n\ntmp_test = get_balanced_subset(\n    df=df, class_col='type', feature_col='sex', feature_value='male',\n    samples_per_class=2, randomize=True, reset_index=True)\nprint(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-07-18T07:15:39.021981Z","iopub.execute_input":"2025-07-18T07:15:39.022251Z","iopub.status.idle":"2025-07-18T07:15:39.054369Z","shell.execute_reply.started":"2025-07-18T07:15:39.022221Z","shell.execute_reply":"2025-07-18T07:15:39.053696Z"},"id":"jqJXku-tsxK7","outputId":"03c62d12-4bff-4e41-cd9b-7d694cc52732","trusted":true},"outputs":[{"name":"stdout","text":"+----+--------+-------+-------------+--------------+----------+--------+------------------------------------------------------------------+\n|    | type   | sex   | ethnicity   | eyeglasses   | makeup   | lips   | path                                                             |\n|----+--------+-------+-------------+--------------+----------+--------+------------------------------------------------------------------|\n|  0 | fake   | male  | white       | no           | no       | small  | /kaggle/input/deepfakedataset/data/deepfake/374_407/frame41.jpg  |\n|  1 | fake   | male  | white       | no           | no       | small  | /kaggle/input/deepfakedataset/data/deepfake/015_919/frame281.jpg |\n|  2 | real   | male  |             | no           | no       | big    | /kaggle/input/deepfakedataset/data/original/995/frame11.jpg      |\n|  3 | real   | male  | white       | no           | no       |        | /kaggle/input/deepfakedataset/data/original/579/frame201.jpg     |\n+----+--------+-------+-------------+--------------+----------+--------+------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def get_exp_data(df, class_col, feature_col, ratio : Dict, size, randomize=True, exclude_column=None, exclude_df=None, max_diff=0.05):\n    '''\n    Get a balanced subset of the data based on specified ratios for features.\n    Args:\n        df: DataFrame containing the data\n        class_col: column name for class labels\n        feature_col: column name for features\n        ratio: dictionary with feature values as keys and their ratios as values\n        size: total number of samples to return\n        randomize: whether to shuffle the DataFrame before processing\n        exclude_column: column name to exclude from the DataFrame\n        exclude_df: DataFrame containing values to exclude based on exclude_column\n    '''\n    if randomize:\n        df_rnd = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n    else:\n        df_rnd = df.copy()\n\n    if exclude_column is not None and exclude_df is not None:\n        if exclude_column not in df_rnd.columns:\n            raise ValueError(f\"Column '{exclude_column}' not found in DataFrame.\")\n        if exclude_column not in exclude_df.columns:\n            raise ValueError(f\"Column '{exclude_column}' not found in exclude DataFrame.\")\n        df_rnd = df_rnd[~df_rnd[exclude_column].isin(exclude_df[exclude_column])]\n\n    uniq_classes = df_rnd[class_col].unique()\n    uniq_features = df_rnd[feature_col].unique()\n\n    def get_exp_data_inner(tmp_df, size):\n        df_tmp = None\n        for uf in uniq_features:\n            if ratio.get(uf) is None:\n                print(f\"Feature '{uf}' not found in ratios. Skipping.\")\n                continue\n            c_amt = int(size * ratio[uf] / len(uniq_classes))\n            # if c_amt <= 0:\n            #     raise ValueError(f\"Calculated samples per class ({c_amt}) is less than or equal to zero for feature '{uf}' with ratio {ratio}.\")\n            tmp = get_balanced_subset(df=tmp_df, class_col=class_col, feature_col=feature_col, feature_value=uf,\n                                        samples_per_class=c_amt, randomize=False)\n            if df_tmp is None:\n                df_tmp = tmp\n            else:\n                df_tmp = pd.concat([df_tmp, tmp])\n        return df_tmp\n\n    df_res = get_exp_data_inner(df_rnd, size)\n\n    if len(df_res) < size:\n        print(f\"Samples for ({len(df_res)}) are less than requested ({size}).\")\n\n    ratios_fet = df_res[feature_col].value_counts(normalize=True).to_dict()\n    ratios_cls = df_res[class_col].value_counts(normalize=False).to_dict()\n    print(f\"[] Ratios for {feature_col}: {ratios_fet}\")\n    print(f\"[] Ratios for {class_col}: {ratios_cls}\")\n\n    for k in ratio:\n        if ratios_fet.get(k) is None:\n            if ratio[k] > 0.0:\n                raise ValueError(f\"Feature '{k}' not found in DataFrame after sampling (try increase 'size' parameter).\")\n        elif abs(ratios_fet[k] - ratio[k]) > max_diff:\n            raise ValueError(f\"Feature '{k}' ratio {ratios_fet[k]} differs from requested {ratio[k]} by more than {max_diff}.\")\n\n    print()\n\n    df_res = df_res.reset_index(drop=True)\n\n    return df_res\n\ntmp_test = get_exp_data(\n    df=df, class_col='type', feature_col='ethnicity', ratio={'white':0.2, 'black':0.6, 'asian': 0.2}, size=10, randomize=True)\nprint(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-07-18T07:15:39.055172Z","iopub.execute_input":"2025-07-18T07:15:39.055409Z","iopub.status.idle":"2025-07-18T07:15:39.182359Z","shell.execute_reply.started":"2025-07-18T07:15:39.055391Z","shell.execute_reply":"2025-07-18T07:15:39.181531Z"},"id":"JJZmhWnis0yh","outputId":"555bdf23-ce59-42e2-da9c-dffb8e307822","trusted":true},"outputs":[{"name":"stdout","text":"Feature 'None' not found in ratios. Skipping.\n[] Ratios for ethnicity: {'black': 0.6, 'white': 0.2, 'asian': 0.2}\n[] Ratios for type: {'fake': 5, 'real': 5}\n\n+----+--------+--------+-------------+--------------+----------+--------+------------------------------------------------------------------+\n|    | type   | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                                                             |\n|----+--------+--------+-------------+--------------+----------+--------+------------------------------------------------------------------|\n|  0 | fake   | male   | white       | no           | no       | small  | /kaggle/input/deepfakedataset/data/deepfake/594_530/frame121.jpg |\n|  1 | real   | female | white       | no           |          |        | /kaggle/input/deepfakedataset/data/original/240/frame41.jpg      |\n|  2 | fake   | female | asian       |              |          | big    | /kaggle/input/deepfakedataset/data/deepfake/249_280/frame261.jpg |\n|  3 | real   | female | asian       | no           |          | big    | /kaggle/input/deepfakedataset/data/original/758/frame161.jpg     |\n|  4 | fake   | female | black       | no           |          | big    | /kaggle/input/deepfakedataset/data/deepfake/986_994/frame271.jpg |\n|  5 | fake   | male   | black       | no           | no       | big    | /kaggle/input/deepfakedataset/data/deepfake/144_122/frame101.jpg |\n|  6 | fake   | male   | black       | yes          | no       | big    | /kaggle/input/deepfakedataset/data/deepfake/081_087/frame41.jpg  |\n|  7 | real   | male   | black       | no           | no       | big    | /kaggle/input/deepfakedataset/data/original/715/frame231.jpg     |\n|  8 | real   | female | black       | no           |          | big    | /kaggle/input/deepfakedataset/data/original/762/frame61.jpg      |\n|  9 | real   | female | black       | no           |          | big    | /kaggle/input/deepfakedataset/data/original/328/frame241.jpg     |\n+----+--------+--------+-------------+--------------+----------+--------+------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def load_image(file_path):\n    image = tf.io.read_file(file_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    if IMG_RESIZE:\n      image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\ndef get_data_for_model(df, class_col, files_col, batch_size):\n    image_paths = df[files_col].values\n    labels = df[class_col].values\n    labels = df[class_col].astype('category').cat.codes.values #classes strs to ints\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n    dataset = dataset.map(lambda path, label: (load_image(path), label))\n    dataset = dataset.batch(batch_size)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2025-07-18T07:15:39.183299Z","iopub.execute_input":"2025-07-18T07:15:39.183612Z","iopub.status.idle":"2025-07-18T07:15:39.189336Z","shell.execute_reply.started":"2025-07-18T07:15:39.183586Z","shell.execute_reply":"2025-07-18T07:15:39.188681Z"},"id":"LNCRWX5js2d7","trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_mobile_net2(num_classes, input_shape, model=None):\n  if model is None:\n    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = False\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128, activation='relu')(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n  else:\n    for layer in model.layers:\n        layer.trainable = True\n    model.compile(\n        optimizer=Adam(learning_rate=1e-5),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n  return model, \"mobile net 2\"\n\ndef create_resnet50(num_classes, input_shape, model=None):\n  if model is None:\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = False\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128, activation='relu')(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n  else:\n    for layer in model.layers:\n      layer.trainable = True\n    model.compile(\n        optimizer=Adam(learning_rate=1e-5),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n  return model, \"resnet50\"\n\ndef create_efficientnet_b0(num_classes, input_shape, model=None):\n  if model is None:\n    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = False\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128, activation='relu')(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n  else:\n    for layer in model.layers:\n      layer.trainable = True\n    model.compile(\n        optimizer=Adam(learning_rate=1e-5),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n  return model, \"efficientnet b0\"\n\n\ndef create_xception(num_classes, input_shape, model=None):\n  if model is None:\n    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = False\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128, activation='relu')(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n  else:\n    for layer in model.layers:\n        layer.trainable = True\n    model.compile(\n        optimizer=Adam(learning_rate=1e-5),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n  return model, \"xception\"\n","metadata":{"execution":{"iopub.status.busy":"2025-07-18T07:15:39.190041Z","iopub.execute_input":"2025-07-18T07:15:39.190238Z","iopub.status.idle":"2025-07-18T07:15:39.206115Z","shell.execute_reply.started":"2025-07-18T07:15:39.190223Z","shell.execute_reply":"2025-07-18T07:15:39.205503Z"},"id":"_cfHfqsPvi3O","trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Create folder if it doesn't exist\nif not os.path.exists(RES_PATH):\n    os.makedirs(RES_PATH)\n    print(f\"Created folder: {RES_PATH}\")\nelif REMOVE_RES_CONTENT:\n    # Remove all files inside the folder\n    for filename in os.listdir(RES_PATH):\n        file_path = os.path.join(RES_PATH, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)          # remove file or link\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)      # remove folder and contents\n        except Exception as e:\n            print(f'Failed to delete {file_path}. Reason: {e}')\n    print(f\"Cleared contents of folder: {RES_PATH}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-07-18T07:15:39.206851Z","iopub.execute_input":"2025-07-18T07:15:39.207128Z","iopub.status.idle":"2025-07-18T07:15:39.225714Z","shell.execute_reply.started":"2025-07-18T07:15:39.207100Z","shell.execute_reply":"2025-07-18T07:15:39.225008Z"},"id":"iYJbzSoMv9Vd","outputId":"07789495-ad79-42f3-eacb-3cc9507b3425","trusted":true},"outputs":[{"name":"stdout","text":"Created folder: /kaggle/working/res/\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def get_done_reps(model_name, feature_name, amt_per_rep):\n  results_path = f'{RES_PATH}res_{feature_name}_{model_name.replace(\" \", \"_\")}.csv'\n  if not os.path.exists(results_path):\n    return [], None\n\n  tmp_df = pd.read_csv(results_path)\n  dones = []\n  for r in tmp_df[\"rep\"].unique():\n    amt = len(tmp_df[tmp_df[\"rep\"]==r])\n    if amt == amt_per_rep:\n      dones.append(r)\n\n  return dones, tmp_df","metadata":{"execution":{"iopub.status.busy":"2025-07-18T07:15:39.226292Z","iopub.execute_input":"2025-07-18T07:15:39.226471Z","iopub.status.idle":"2025-07-18T07:15:39.239563Z","shell.execute_reply.started":"2025-07-18T07:15:39.226456Z","shell.execute_reply":"2025-07-18T07:15:39.238948Z"},"id":"CQ_JQedB1623","trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def perform_tests(df, train_metas, test_metas, validation_size, reps, class_col, feature_split_col, exclude_column, files_col, get_model, epochs_num_head, epochs_num_whole, batch_size):\n    res = []\n\n    _, model_name = get_model(num_classes=len(df[class_col].unique()), input_shape=(IMG_SIZE,IMG_SIZE,3))\n\n    done_reps, prev_results = get_done_reps(model_name, feature_split_col, len(test_metas) * len(train_metas))\n\n    for r in range(reps):\n        if r in done_reps:\n          res.extend(prev_results[prev_results['rep']==r].values.tolist())\n          print(f\"Rep {r} already done for {model_name}. Skipping...\")\n          continue\n\n        np.random.seed(SEED + r)\n        tf.random.set_seed(SEED + r)\n\n        for train_meta in train_metas:\n            train_val = get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=train_meta['ratio'], size=train_meta['size'] + validation_size)\n            train_val = train_val.sample(frac=1, random_state=SEED+r).reset_index(drop=True)\n\n            stratify_key = train_val[class_col].astype(str) + \"_\" + train_val[feature_split_col].astype(str)\n\n            train, val = train_test_split(\n                train_val,\n                test_size=validation_size / (train_meta['size'] + validation_size),\n                stratify=stratify_key,\n                random_state=SEED + r\n            )\n\n            train = train.reset_index(drop=True)\n            val = val.reset_index(drop=True)\n\n            tests = [\n                get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=tm['ratio'], size=tm['size'], exclude_column=exclude_column, exclude_df=train) for tm in test_metas\n            ]\n\n            train_dataset = get_data_for_model(train, class_col=class_col, files_col=files_col, batch_size=batch_size)\n            val_dataset = get_data_for_model(val, class_col=class_col, files_col=files_col, batch_size=batch_size)\n            test_datasets = [\n                get_data_for_model(test, class_col=class_col, files_col=files_col, batch_size=batch_size) for test in tests\n            ]\n\n            train_ratio = '/'.join([f\"{k}:{v}\" for k, v in train_meta['ratio'].items()])\n            train_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in train[feature_split_col].value_counts(normalize=True).to_dict().items()])\n            train_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', train_ratio)\n\n            print(\"FITTING HEAD\")\n            model, model_name = get_model(num_classes=len(df[class_col].unique()), input_shape=(IMG_SIZE,IMG_SIZE,3))\n            early_stopping_head = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n            model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num_head, callbacks=[early_stopping_head])\n\n            print(\"FITTING WHOLE\")\n            model, model_name = get_model(num_classes=len(df[class_col].unique()), input_shape=(IMG_SIZE,IMG_SIZE,3), model=model)\n            early_stopping_whole = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n            model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num_whole, callbacks=[early_stopping_whole])\n\n            for test_dataset, test_meta, test_df in zip(test_datasets, test_metas, tests):\n                predictions = model.predict(test_dataset)\n                y_true = test_df[class_col].astype('category').cat.codes.values\n                y_pred = np.argmax(predictions, axis=1)\n                acc = accuracy_score(y_true, y_pred)\n                f1 = f1_score(y_true, y_pred, average='weighted')\n                eo_diff = equalized_odds_difference(y_true, y_pred, sensitive_features=test_df[feature_split_col])\n\n                test_ratio = '/'.join([f\"{k}:{v}\" for k, v in test_meta['ratio'].items()])\n                test_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in test_df[feature_split_col].value_counts(normalize=True).to_dict().items()])\n                test_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', test_ratio)\n\n                res.append([\n                    r,\n                    model_name,\n                    feature_split_col,\n                    train_meta['size'],\n                    train_ratio,\n                    test_meta['size'],\n                    test_ratio,\n                    acc,\n                    f1,\n                    eo_diff,\n                    train_ratio_rel,\n                    test_ratio_rel,\n                    train_ratio_sim,\n                    test_ratio_sim\n                ])\n\n                print(f\"Rep: {r:2} | Model: {model_name} | Feature Split: {feature_split_col} | Ratio: {test_ratio} | Acc: {acc:.2f}\")\n\n                res_df = pd.DataFrame(res, columns=[\n                    'rep', 'model_name', 'feature_split_col',\n                    'train_size', 'train_ratio_detail', 'test_size', 'test_ratio_detail',\n                    'accuracy', 'f1_score', 'eo_diff', 'train_ratio_rel', 'test_ratio_rel', \"train_ratio\", \"test_ratio\"\n                ])\n\n                res_df.to_csv(f'{RES_PATH}res_{feature_split_col}_{model_name.replace(\" \", \"_\")}.csv', index=False)\n    print(f\"Done for {model_name}.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-07-18T07:15:39.241748Z","iopub.execute_input":"2025-07-18T07:15:39.241955Z","iopub.status.idle":"2025-07-18T07:15:39.258803Z","shell.execute_reply.started":"2025-07-18T07:15:39.241936Z","shell.execute_reply":"2025-07-18T07:15:39.258068Z"},"id":"ik6Z-AUis6G7","trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\nperform_tests(df=df,\n              train_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n                  ],\n              test_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n              ],\n              validation_size=500,\n              reps=10,\n              class_col='type',\n              feature_split_col='sex',\n              exclude_column='path',\n              files_col='path',\n              get_model=create_efficientnet_b0,\n              epochs_num_head=8,\n              epochs_num_whole=4,\n              batch_size=64\n              )\n\nperform_tests(df=df,\n              train_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n                  ],\n              test_metas=[\n                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n              ],\n              validation_size=500,\n              reps=10,\n              class_col='type',\n              feature_split_col='sex',\n              exclude_column='path',\n              files_col='path',\n              get_model=create_xception,\n              epochs_num_head=8,\n              epochs_num_whole=4,\n              batch_size=64\n              )","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-07-18T07:16:47.155875Z","iopub.execute_input":"2025-07-18T07:16:47.156155Z","iopub.status.idle":"2025-07-18T07:24:38.008255Z","shell.execute_reply.started":"2025-07-18T07:16:47.156137Z","shell.execute_reply":"2025-07-18T07:24:38.006656Z"},"id":"yM2K_awwwv2m","outputId":"65cbb83b-b777-4386-d68f-0cd46a28dc09","trusted":true},"outputs":[{"name":"stderr","text":"I0000 00:00:1752823007.180869      74 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1752823007.181120      74 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'female': 0.9, 'male': 0.1}\n[] Ratios for type: {'fake': 2750, 'real': 2750}\n\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'female': 0.9, 'male': 0.1}\n[] Ratios for type: {'fake': 250, 'real': 250}\n\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'female': 0.7, 'male': 0.3}\n[] Ratios for type: {'fake': 250, 'real': 250}\n\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'female': 0.5, 'male': 0.5}\n[] Ratios for type: {'fake': 250, 'real': 250}\n\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'male': 0.7, 'female': 0.3}\n[] Ratios for type: {'fake': 250, 'real': 250}\n\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'male': 0.9, 'female': 0.1}\n[] Ratios for type: {'fake': 250, 'real': 250}\n\nFITTING HEAD\nEpoch 1/8\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1752823022.238285     133 service.cc:148] XLA service 0x7b7c0884df30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1752823022.239425     133 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1752823022.239445     133 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1752823023.709645     133 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 1/79\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23:27\u001b[0m 18s/step - accuracy: 0.5312 - loss: 0.7949","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1752823030.056368     133 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 324ms/step - accuracy: 0.5021 - loss: 0.7314 - val_accuracy: 0.5180 - val_loss: 0.6957\nEpoch 2/8\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 157ms/step - accuracy: 0.5004 - loss: 0.7025 - val_accuracy: 0.5000 - val_loss: 0.7054\nEpoch 3/8\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 162ms/step - accuracy: 0.5044 - loss: 0.7040 - val_accuracy: 0.5000 - val_loss: 0.7060\nEpoch 4/8\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 167ms/step - accuracy: 0.4964 - loss: 0.7025 - val_accuracy: 0.5020 - val_loss: 0.7053\nEpoch 5/8\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 176ms/step - accuracy: 0.4971 - loss: 0.7007 - val_accuracy: 0.5020 - val_loss: 0.7043\nEpoch 5: early stopping\nRestoring model weights from the end of the best epoch: 1.\nFITTING WHOLE\nEpoch 1/4\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 966ms/step - accuracy: 0.5145 - loss: 0.8957 - val_accuracy: 0.5060 - val_loss: 0.7068\nEpoch 2/4\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 628ms/step - accuracy: 0.8201 - loss: 0.4890 - val_accuracy: 0.5340 - val_loss: 0.6921\nEpoch 3/4\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 598ms/step - accuracy: 0.9168 - loss: 0.3334 - val_accuracy: 0.5120 - val_loss: 0.7039\nEpoch 4/4\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 614ms/step - accuracy: 0.9616 - loss: 0.2106 - val_accuracy: 0.4940 - val_loss: 0.7418\nRestoring model weights from the end of the best epoch: 2.\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 748ms/step\nRep:  0 | Model: resnet50 | Feature Split: sex | Ratio: male:0.1/female:0.9 | Acc: 0.53\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\nRep:  0 | Model: resnet50 | Feature Split: sex | Ratio: male:0.3/female:0.7 | Acc: 0.53\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\nRep:  0 | Model: resnet50 | Feature Split: sex | Ratio: male:0.5/female:0.5 | Acc: 0.51\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\nRep:  0 | Model: resnet50 | Feature Split: sex | Ratio: male:0.7/female:0.3 | Acc: 0.48\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\nRep:  0 | Model: resnet50 | Feature Split: sex | Ratio: male:0.9/female:0.1 | Acc: 0.48\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'male': 0.5, 'female': 0.5}\n[] Ratios for type: {'fake': 2750, 'real': 2750}\n\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'female': 0.9, 'male': 0.1}\n[] Ratios for type: {'fake': 250, 'real': 250}\n\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'female': 0.7, 'male': 0.3}\n[] Ratios for type: {'fake': 250, 'real': 250}\n\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'male': 0.5, 'female': 0.5}\n[] Ratios for type: {'fake': 250, 'real': 250}\n\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'male': 0.7, 'female': 0.3}\n[] Ratios for type: {'fake': 250, 'real': 250}\n\nFeature 'None' not found in ratios. Skipping.\n[] Ratios for sex: {'male': 0.9, 'female': 0.1}\n[] Ratios for type: {'fake': 250, 'real': 250}\n\nFITTING HEAD\nEpoch 1/8\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 286ms/step - accuracy: 0.4956 - loss: 0.7765 - val_accuracy: 0.4980 - val_loss: 0.6980\nEpoch 2/8\n\u001b[1m78/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.5015 - loss: 0.7090","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_74/4055628184.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m perform_tests(df=df,\n\u001b[0m\u001b[1;32m      2\u001b[0m               train_metas=[\n\u001b[1;32m      3\u001b[0m                   \u001b[0;34m{\u001b[0m\u001b[0;34m'ratio'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'male'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'female'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0;34m{\u001b[0m\u001b[0;34m'ratio'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'male'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'female'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   ],\n","\u001b[0;32m/tmp/ipykernel_74/1198535472.py\u001b[0m in \u001b[0;36mperform_tests\u001b[0;34m(df, train_metas, test_metas, validation_size, reps, class_col, feature_split_col, exclude_column, files_col, get_model, epochs_num_head, epochs_num_whole, batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mearly_stopping_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_num_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_head\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FITTING WHOLE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    393\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                     )\n\u001b[0;32m--> 395\u001b[0;31m                 val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    396\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_evaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[1;32m    219\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"def print_summarise_res(model_name:str):\n  path = RES_PATH + f'res_sex_{model_name}.csv'\n\n  if not os.path.exists(path):\n    print(f\"File {path} does not exists!\")\n    return\n\n  res = pd.read_csv(path)\n  gr = res.groupby(['train_ratio', 'test_ratio']).agg(\n      # Model=('model_name', 'first'),\n      TrainRatio=('train_ratio', 'first'),\n      TestRatio=('test_ratio', 'first'),\n      Accuracy= ('accuracy', 'mean'),\n      AccuracySTD= ('accuracy', 'std'),\n      F1=('f1_score', 'mean'),\n      F1STD=('f1_score', 'std'),\n      EODiff=('eo_diff', 'mean'),\n      EODiffSTD=('eo_diff', 'std'),\n  ).reset_index(drop=True)\n\n  gr = gr.round(3).sort_values(by=['TrainRatio', 'TestRatio'], ascending=False)\n\n  print(\"MODEL: \" + model_name)\n  print(tb.tabulate(gr, headers='keys', tablefmt='psql'))\n  print()\n  print()\n\nprint_summarise_res('resnet50')\nprint_summarise_res('efficientnet_b0')\nprint_summarise_res('xception')","metadata":{"execution":{"iopub.status.busy":"2025-07-18T07:24:38.011958Z","iopub.status.idle":"2025-07-18T07:24:38.012196Z","shell.execute_reply.started":"2025-07-18T07:24:38.012080Z","shell.execute_reply":"2025-07-18T07:24:38.012090Z"},"id":"ivm4W_DLs7jK","trusted":true},"outputs":[],"execution_count":null}]}