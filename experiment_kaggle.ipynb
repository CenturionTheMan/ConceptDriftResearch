{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:45.220844Z",
     "iopub.status.busy": "2025-07-21T16:13:45.220139Z",
     "iopub.status.idle": "2025-07-21T16:13:45.224524Z",
     "shell.execute_reply": "2025-07-21T16:13:45.223912Z",
     "shell.execute_reply.started": "2025-07-21T16:13:45.220819Z"
    },
    "id": "jnW2ICuewM0-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "REMOVE_RES_CONTENT = True\n",
    "IMG_SIZE=224\n",
    "IMG_RESIZE=True\n",
    "SEED = 42\n",
    "RES_PATH = '/kaggle/working/res/'\n",
    "# DATASET_PATH = '/kaggle/input/deepfakedataset/data/'\n",
    "DATASET_PATH = 'data/'\n",
    "WORKID_DIR_PATH='/kaggle/working/'\n",
    "SHOW_EXTRA_INFO=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:45.243206Z",
     "iopub.status.busy": "2025-07-21T16:13:45.242584Z",
     "iopub.status.idle": "2025-07-21T16:13:48.888754Z",
     "shell.execute_reply": "2025-07-21T16:13:48.887853Z",
     "shell.execute_reply.started": "2025-07-21T16:13:45.243184Z"
    },
    "id": "cPKtaDEKrVlM",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "f35d774d-1a32-46a0-fbb9-1b5dd41d79ad",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q fairlearn tabulate\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tabulate as tb\n",
    "from typing import Dict\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import re\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Input, BatchNormalization, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "from tensorflow.keras.applications import Xception, EfficientNetB4, InceptionV3, EfficientNetV2M,EfficientNetV2S \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, BatchNormalization, LeakyReLU,\n",
    "    MaxPooling2D, Flatten, Dense, Dropout, concatenate\n",
    ")\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score, log_loss, brier_score_loss\n",
    "import time \n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name:\n",
    "    print(f\"GPU available: {device_name}\")\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset splitting - handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:48.900170Z",
     "iopub.status.busy": "2025-07-21T16:13:48.899913Z",
     "iopub.status.idle": "2025-07-21T16:13:49.593356Z",
     "shell.execute_reply": "2025-07-21T16:13:49.592577Z",
     "shell.execute_reply.started": "2025-07-21T16:13:48.900148Z"
    },
    "id": "BQHjq4DXrY3h",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "ee466649-50e5-43a4-c318-b6aedc03dc0d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-------------+--------------+----------+--------+--------------------------------+------------+\n",
      "|    |   type | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                           | typeName   |\n",
      "|----+--------+--------+-------------+--------------+----------+--------+--------------------------------+------------|\n",
      "|  0 |      0 | male   |             | yes          | no       | big    | data/original/805/frame271.jpg | real       |\n",
      "|  1 |      0 | female | white       | no           |          | big    | data/original/083/frame191.jpg | real       |\n",
      "|  2 |      0 | male   | white       | no           | no       | small  | data/original/878/frame111.jpg | real       |\n",
      "|  3 |      0 | female | white       | no           |          |        | data/original/158/frame201.jpg | real       |\n",
      "|  4 |      0 | female | white       | no           |          |        | data/original/606/frame71.jpg  | real       |\n",
      "+----+--------+--------+-------------+--------------+----------+--------+--------------------------------+------------+\n",
      "Dataset size: 59552\n"
     ]
    }
   ],
   "source": [
    "file_path = DATASET_PATH + 'metadata.csv'\n",
    "df_tmp = pd.read_csv(file_path, sep=',')\n",
    "df_tmp['path'] = DATASET_PATH + df_tmp['path']\n",
    "\n",
    "df_tmp = df_tmp[df_tmp['deepfake'] != 0]\n",
    "\n",
    "df_tmp['ethnicity'] = df_tmp.apply(\n",
    "    lambda row: 'white' if row['white'] == 1 else ('black' if row['black'] == 1 else (\n",
    "        'asian' if row['asian'] == 1 else None)), axis=1)\n",
    "\n",
    "df = df_tmp[['deepfake', 'male', 'ethnicity', 'eyeglasses', 'heavy_makeup', 'big_lips', 'path']]\n",
    "\n",
    "df = df.rename(columns={'deepfake': 'type', 'male': 'sex', 'heavy_makeup': 'makeup', 'big_lips': 'lips',})\n",
    "\n",
    "df['typeName'] = df['type'].replace({1: 'fake', -1: 'real'})\n",
    "df['type'] = df['typeName'].map({'fake': 1, 'real': 0}).values\n",
    "df['sex'] = df['sex'].replace({-1: 'female', 0: None, 1: 'male'})\n",
    "df['makeup'] = df['makeup'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
    "df['lips'] = df['lips'].replace({-1: 'small', 0: None, 1: 'big'})\n",
    "df['eyeglasses'] = df['eyeglasses'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
    "\n",
    "\n",
    "print(tb.tabulate(df.head(), headers='keys', tablefmt='psql'))\n",
    "print(f\"Dataset size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:49.594402Z",
     "iopub.status.busy": "2025-07-21T16:13:49.594125Z",
     "iopub.status.idle": "2025-07-21T16:13:49.625262Z",
     "shell.execute_reply": "2025-07-21T16:13:49.624585Z",
     "shell.execute_reply.started": "2025-07-21T16:13:49.594373Z"
    },
    "id": "jqJXku-tsxK7",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "03c62d12-4bff-4e41-cd9b-7d694cc52732",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+-------------+--------------+----------+--------+------------------------------------+------------+\n",
      "|    |   type | sex   | ethnicity   | eyeglasses   | makeup   | lips   | path                               | typeName   |\n",
      "|----+--------+-------+-------------+--------------+----------+--------+------------------------------------+------------|\n",
      "|  0 |      0 | male  |             | no           | no       | big    | data/original/995/frame11.jpg      | real       |\n",
      "|  1 |      0 | male  | white       | no           | no       |        | data/original/579/frame201.jpg     | real       |\n",
      "|  2 |      1 | male  | white       | no           | no       | small  | data/deepfake/374_407/frame41.jpg  | fake       |\n",
      "|  3 |      1 | male  | white       | no           | no       | small  | data/deepfake/015_919/frame281.jpg | fake       |\n",
      "+----+--------+-------+-------------+--------------+----------+--------+------------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "def get_balanced_subset(\n",
    "    df, class_col, feature_col, feature_value,\n",
    "    samples_per_class, randomize=True, reset_index=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a balanced subset of the data for a given feature value, with equal number of samples per class.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        class_col: column name of class labels\n",
    "        feature_col: column name of feature\n",
    "        feature_value: specific feature value to filter\n",
    "        samples_per_class: number of samples per class\n",
    "        randomize: whether to shuffle within class before selecting\n",
    "        reset_index: whether to reset index of returned DataFrame\n",
    "        seed: random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Balanced DataFrame subset\n",
    "    \"\"\"\n",
    "    tmp = df[df[feature_col] == feature_value]\n",
    "\n",
    "    counts = tmp[class_col].value_counts()\n",
    "    for cl, count in counts.items():\n",
    "        if count < samples_per_class:\n",
    "            raise ValueError(f\"Not enough samples for class '{cl}' in feature '{feature_value}'. \"\n",
    "                             f\"Required: {samples_per_class}, Available: {count}\")\n",
    "\n",
    "    tmp = pd.concat([\n",
    "        (g.sample(frac=1, random_state=SEED).head(samples_per_class) if randomize else g.head(samples_per_class))\n",
    "        for _, g in tmp.groupby(class_col)\n",
    "    ])\n",
    "\n",
    "    if reset_index:\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "tmp_test = get_balanced_subset(\n",
    "    df=df, class_col='type', feature_col='sex', feature_value='male',\n",
    "    samples_per_class=2, randomize=True, reset_index=True)\n",
    "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:49.626974Z",
     "iopub.status.busy": "2025-07-21T16:13:49.626769Z",
     "iopub.status.idle": "2025-07-21T16:13:49.692949Z",
     "shell.execute_reply": "2025-07-21T16:13:49.692028Z",
     "shell.execute_reply.started": "2025-07-21T16:13:49.626959Z"
    },
    "id": "JJZmhWnis0yh",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "555bdf23-ce59-42e2-da9c-dffb8e307822",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-------------+--------------+----------+--------+------------------------------------+------------+\n",
      "|    |   type | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                               | typeName   |\n",
      "|----+--------+--------+-------------+--------------+----------+--------+------------------------------------+------------|\n",
      "|  0 |      0 | female | white       | no           |          |        | data/original/240/frame41.jpg      | real       |\n",
      "|  1 |      1 | male   | white       | no           | no       | small  | data/deepfake/594_530/frame121.jpg | fake       |\n",
      "|  2 |      0 | female | asian       | no           |          | big    | data/original/758/frame161.jpg     | real       |\n",
      "|  3 |      1 | female | asian       |              |          | big    | data/deepfake/249_280/frame261.jpg | fake       |\n",
      "|  4 |      0 | male   | black       | no           | no       | big    | data/original/715/frame231.jpg     | real       |\n",
      "|  5 |      0 | female | black       | no           |          | big    | data/original/762/frame61.jpg      | real       |\n",
      "|  6 |      0 | female | black       | no           |          | big    | data/original/328/frame241.jpg     | real       |\n",
      "|  7 |      1 | female | black       | no           |          | big    | data/deepfake/986_994/frame271.jpg | fake       |\n",
      "|  8 |      1 | male   | black       | no           | no       | big    | data/deepfake/144_122/frame101.jpg | fake       |\n",
      "|  9 |      1 | male   | black       | yes          | no       | big    | data/deepfake/081_087/frame41.jpg  | fake       |\n",
      "+----+--------+--------+-------------+--------------+----------+--------+------------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "def get_exp_data(df, class_col, feature_col, ratio : Dict, size, randomize=True, exclude_column=None, exclude_df=None, max_diff=0.05):\n",
    "    '''\n",
    "    Get a balanced subset of the data based on specified ratios for features.\n",
    "    Args:\n",
    "        df: DataFrame containing the data\n",
    "        class_col: column name for class labels\n",
    "        feature_col: column name for features\n",
    "        ratio: dictionary with feature values as keys and their ratios as values\n",
    "        size: total number of samples to return\n",
    "        randomize: whether to shuffle the DataFrame before processing\n",
    "        exclude_column: column name to exclude from the DataFrame\n",
    "        exclude_df: DataFrame containing values to exclude based on exclude_column\n",
    "    '''\n",
    "    if randomize:\n",
    "        df_rnd = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    else:\n",
    "        df_rnd = df.copy()\n",
    "\n",
    "    if exclude_column is not None and exclude_df is not None:\n",
    "        if exclude_column not in df_rnd.columns:\n",
    "            raise ValueError(f\"Column '{exclude_column}' not found in DataFrame.\")\n",
    "        if exclude_column not in exclude_df.columns:\n",
    "            raise ValueError(f\"Column '{exclude_column}' not found in exclude DataFrame.\")\n",
    "        df_rnd = df_rnd[~df_rnd[exclude_column].isin(exclude_df[exclude_column])]\n",
    "\n",
    "    uniq_classes = df_rnd[class_col].unique()\n",
    "    uniq_features = df_rnd[feature_col].unique()\n",
    "\n",
    "    def get_exp_data_inner(tmp_df, size):\n",
    "        df_tmp = None\n",
    "        for uf in uniq_features:\n",
    "            if ratio.get(uf) is None:\n",
    "                if SHOW_EXTRA_INFO:\n",
    "                    print(f\"Feature '{uf}' not found in ratios. Skipping.\")\n",
    "                continue\n",
    "            c_amt = int(size * ratio[uf] / len(uniq_classes))\n",
    "            # if c_amt <= 0:\n",
    "            #     raise ValueError(f\"Calculated samples per class ({c_amt}) is less than or equal to zero for feature '{uf}' with ratio {ratio}.\")\n",
    "            tmp = get_balanced_subset(df=tmp_df, class_col=class_col, feature_col=feature_col, feature_value=uf,\n",
    "                                        samples_per_class=c_amt, randomize=False)\n",
    "            if df_tmp is None:\n",
    "                df_tmp = tmp\n",
    "            else:\n",
    "                df_tmp = pd.concat([df_tmp, tmp])\n",
    "        return df_tmp\n",
    "\n",
    "    df_res = get_exp_data_inner(df_rnd, size)\n",
    "\n",
    "    if len(df_res) < size and SHOW_EXTRA_INFO:\n",
    "        print(f\"Samples for ({len(df_res)}) are less than requested ({size}).\")\n",
    "\n",
    "    ratios_fet = df_res[feature_col].value_counts(normalize=True).to_dict()\n",
    "    ratios_cls = df_res[class_col].value_counts(normalize=False).to_dict()\n",
    "\n",
    "    if SHOW_EXTRA_INFO:\n",
    "        print(f\"[] Ratios for {feature_col}: {ratios_fet}\")\n",
    "        print(f\"[] Ratios for {class_col}: {ratios_cls}\")\n",
    "        print()\n",
    "        \n",
    "\n",
    "    for k in ratio:\n",
    "        if ratios_fet.get(k) is None:\n",
    "            if ratio[k] > 0.0:\n",
    "                raise ValueError(f\"Feature '{k}' not found in DataFrame after sampling (try increase 'size' parameter).\")\n",
    "        elif abs(ratios_fet[k] - ratio[k]) > max_diff:\n",
    "            raise ValueError(f\"Feature '{k}' ratio {ratios_fet[k]} differs from requested {ratio[k]} by more than {max_diff}.\")\n",
    "\n",
    "    \n",
    "\n",
    "    df_res = df_res.reset_index(drop=True)\n",
    "\n",
    "    return df_res\n",
    "\n",
    "tmp_test = get_exp_data(\n",
    "    df=df, class_col='type', feature_col='ethnicity', ratio={'white':0.2, 'black':0.6, 'asian': 0.2}, size=10)\n",
    "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:49.693972Z",
     "iopub.status.busy": "2025-07-21T16:13:49.693741Z",
     "iopub.status.idle": "2025-07-21T16:13:49.717446Z",
     "shell.execute_reply": "2025-07-21T16:13:49.716733Z",
     "shell.execute_reply.started": "2025-07-21T16:13:49.693954Z"
    },
    "id": "LNCRWX5js2d7",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficientnet\n",
    "from tensorflow.keras.applications.xception import preprocess_input as preprocess_xception\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as preprocess_inception_v3\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as preprocess_efficientnetv2\n",
    "def preprocess_simple_cnn(image):\n",
    "    return image / 255.0\n",
    "\n",
    "\n",
    "augmentation_std = tf.keras.Sequential([\n",
    "    #layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.05),\n",
    "    #layers.RandomContrast(0.05),\n",
    "    #layers.RandomBrightness(0.05),\n",
    "], name=\"augmentation_std\")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def load_image(file_path, label, preprocess, augment):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "\n",
    "    if IMG_RESIZE:\n",
    "        image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    else:\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        \n",
    "    if preprocess:\n",
    "        image = preprocess(image)\n",
    "\n",
    "    if augment:\n",
    "        image = augment(image)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def get_data_for_model(df, class_col, files_col, batch_size, preprocess, augment):\n",
    "    image_paths = df[files_col].values\n",
    "    labels = df[class_col]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda path, label: load_image(path, label, preprocess, augment), num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:49.718648Z",
     "iopub.status.busy": "2025-07-21T16:13:49.718357Z",
     "iopub.status.idle": "2025-07-21T16:13:49.728209Z",
     "shell.execute_reply": "2025-07-21T16:13:49.727547Z",
     "shell.execute_reply.started": "2025-07-21T16:13:49.718627Z"
    },
    "id": "iYJbzSoMv9Vd",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "07789495-ad79-42f3-eacb-3cc9507b3425",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create folder if it doesn't exist\n",
    "if not os.path.exists(RES_PATH):\n",
    "    os.makedirs(RES_PATH)\n",
    "    print(f\"Created folder: {RES_PATH}\")\n",
    "elif REMOVE_RES_CONTENT:\n",
    "    # Remove all files inside the folder\n",
    "    for filename in os.listdir(RES_PATH):\n",
    "        file_path = os.path.join(RES_PATH, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)          # remove file or link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)      # remove folder and contents\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "    print(f\"Cleared contents of folder: {RES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:49.729389Z",
     "iopub.status.busy": "2025-07-21T16:13:49.729102Z",
     "iopub.status.idle": "2025-07-21T16:13:49.749902Z",
     "shell.execute_reply": "2025-07-21T16:13:49.749033Z",
     "shell.execute_reply.started": "2025-07-21T16:13:49.729368Z"
    },
    "id": "CQ_JQedB1623",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_done_reps(model_name, feature_name, amt_per_rep):\n",
    "  results_path = f'{RES_PATH}res_{feature_name}_{model_name.replace(\" \", \"_\")}.csv'\n",
    "  if not os.path.exists(results_path):\n",
    "    return [], None\n",
    "\n",
    "  tmp_df = pd.read_csv(results_path)\n",
    "  dones = []\n",
    "  for r in tmp_df[\"rep\"].unique():\n",
    "    amt = len(tmp_df[tmp_df[\"rep\"]==r])\n",
    "    if amt == amt_per_rep:\n",
    "      dones.append(r)\n",
    "\n",
    "  return dones, tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_fairness_metrics(y_true, y_pred, group_labels):\n",
    "    \"\"\"Compute per-group rates and their gaps.\"\"\"\n",
    "    groups = pd.unique(group_labels)\n",
    "    tpr = {}\n",
    "    fpr = {}\n",
    "    pos_rate = {}\n",
    "\n",
    "    for g in groups:\n",
    "        mask = (group_labels == g)\n",
    "        yt = y_true[mask]\n",
    "        yp = y_pred[mask]\n",
    "        tn, fp, fn, tp = confusion_matrix(yt, yp, labels=[0,1]).ravel()\n",
    "        tpr[g] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        fpr[g] = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "        pos_rate[g] = (yp == 1).mean()\n",
    "\n",
    "    tpr_gap = max(tpr.values()) - min(tpr.values())\n",
    "    fpr_gap = max(fpr.values()) - min(fpr.values())\n",
    "    pos_rate_gap = max(pos_rate.values()) - min(pos_rate.values())\n",
    "\n",
    "    return {\n",
    "        \"tpr_gap\": tpr_gap,\n",
    "        \"fpr_gap\": fpr_gap,\n",
    "        \"pos_rate_gap\": pos_rate_gap,\n",
    "        \"per_group\": {\n",
    "            \"tpr\": tpr,\n",
    "            \"fpr\": fpr,\n",
    "            \"pos_rate\": pos_rate\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:49.751077Z",
     "iopub.status.busy": "2025-07-21T16:13:49.750804Z",
     "iopub.status.idle": "2025-07-21T16:13:49.769382Z",
     "shell.execute_reply": "2025-07-21T16:13:49.768633Z",
     "shell.execute_reply.started": "2025-07-21T16:13:49.751056Z"
    },
    "id": "ik6Z-AUis6G7",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def perform_tests(df, train_metas, test_metas, validation_size, reps, feature_split_col, get_model, preprocess, class_col='type', exclude_column='path', \n",
    "                  files_col='path', data_augmentation=False, epochs_num=15, batch_size=64):\n",
    "    res = []\n",
    "\n",
    "    model_name = get_model.__name__.replace(\"create_\", \"\")\n",
    "\n",
    "    done_reps, prev_results = get_done_reps(model_name, feature_split_col, len(test_metas) * len(train_metas))\n",
    "\n",
    "    for r in range(reps):\n",
    "        if r in done_reps:\n",
    "          res.extend(prev_results[prev_results['rep']==r].values.tolist())\n",
    "          print(f\"Rep {r} already done for {model_name}. Skipping...\")\n",
    "          continue\n",
    "\n",
    "        np.random.seed(SEED + r)\n",
    "        tf.random.set_seed(SEED + r)\n",
    "\n",
    "        for train_meta in train_metas:\n",
    "            train_val = get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=train_meta['ratio'], size=train_meta['size'] + validation_size)\n",
    "            train_val = train_val.sample(frac=1, random_state=SEED+r).reset_index(drop=True)\n",
    "\n",
    "            stratify_key = train_val[class_col].astype(str) + \"_\" + train_val[feature_split_col].astype(str)\n",
    "\n",
    "            train, val = train_test_split(\n",
    "                train_val,\n",
    "                test_size=validation_size / (train_meta['size'] + validation_size),\n",
    "                stratify=stratify_key,\n",
    "                random_state=SEED + r\n",
    "            )\n",
    "\n",
    "            tests = [\n",
    "                get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=tm['ratio'], size=tm['size'], exclude_column=exclude_column, exclude_df=train) for tm in test_metas\n",
    "            ]\n",
    "\n",
    "            train_dataset = get_data_for_model(train, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=data_augmentation)\n",
    "            val_dataset = get_data_for_model(val, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=False)\n",
    "            test_datasets = [\n",
    "                get_data_for_model(test, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=False) for test in tests\n",
    "            ]\n",
    "\n",
    "            train_ratio = '/'.join([f\"{k}:{v}\" for k, v in train_meta['ratio'].items()])\n",
    "            train_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in train[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
    "            train_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', train_ratio)\n",
    "\n",
    "            model = get_model(input_shape=(IMG_SIZE,IMG_SIZE,3), \n",
    "                              train_dataset=train_dataset, val_dataset=val_dataset, \n",
    "                              epochs_num=epochs_num)\n",
    "\n",
    "            for test_dataset, test_meta, test_df in zip(test_datasets, test_metas, tests):\n",
    "                predictions = model.predict(test_dataset)\n",
    "                y_true = test_df[class_col]\n",
    "                y_pred = (predictions > 0.5).astype(int).flatten()\n",
    "                \n",
    "                tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "                accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "                fairness = compute_fairness_metrics(\n",
    "                    y_true=np.array(y_true),\n",
    "                    y_pred=np.array(y_pred),\n",
    "                    group_labels=test_df[feature_split_col].values\n",
    "                )\n",
    "                fairness_json = json.dumps(fairness[\"per_group\"])\n",
    "                \n",
    "                test_ratio = '/'.join([f\"{k}:{v}\" for k, v in test_meta['ratio'].items()])\n",
    "                test_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in test_df[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
    "                test_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', test_ratio)\n",
    "\n",
    "                roc_df = pd.DataFrame({\n",
    "                    'y_true': y_true,\n",
    "                    'y_pred_proba': predictions.flatten()\n",
    "                })\n",
    "                path_roc = f'{RES_PATH}roc_{feature_split_col}_{model_name.replace(\" \", \"_\")}_rep{r}_{time.time_ns()}.csv'\n",
    "                roc_df.to_csv(path_roc, index=False)\n",
    "                                \n",
    "                res.append([\n",
    "                    r,\n",
    "                    model_name,\n",
    "                    feature_split_col,\n",
    "                    train_meta['size'],\n",
    "                    train_ratio,\n",
    "                    test_meta['size'],\n",
    "                    test_ratio,\n",
    "                    train_ratio_rel,\n",
    "                    test_ratio_rel,\n",
    "                    train_ratio_sim,\n",
    "                    test_ratio_sim,\n",
    "                    accuracy,\n",
    "                    tn, fp, fn, tp,\n",
    "                    fairness[\"tpr_gap\"],\n",
    "                    fairness[\"fpr_gap\"],\n",
    "                    fairness[\"pos_rate_gap\"],\n",
    "                    fairness_json,\n",
    "                    path_roc\n",
    "                ])\n",
    "\n",
    "                print(f\"Rep: {r:2} | Model: {model_name} | Feature Split: {feature_split_col} | Ratio: {test_ratio} | Acc: {accuracy:.2f}\")\n",
    "\n",
    "                res_df = pd.DataFrame(res, columns=[\n",
    "                    'rep', 'model_name', 'feature_split_col',\n",
    "                    'train_size', 'train_ratio_detail', 'test_size', 'test_ratio_detail',\n",
    "                    'train_ratio_rel', 'test_ratio_rel', \"train_ratio\", \"test_ratio\",\n",
    "                    'accuracy', 'TN', 'FP', 'FN', 'TP',\n",
    "                    'fairness_tpr_gap', 'fairness_fpr_gap', 'fairness_pos_rate_gap',\n",
    "                    'fairness_per_group_json',\n",
    "                    'path_roc'\n",
    "                ])\n",
    "\n",
    "                res_df.to_csv(f'{RES_PATH}res_{feature_split_col}_{model_name.replace(\" \", \"_\")}.csv', index=False)\n",
    "    print(f\"Done for {model_name}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:49.770630Z",
     "iopub.status.busy": "2025-07-21T16:13:49.770300Z",
     "iopub.status.idle": "2025-07-21T16:13:49.789442Z",
     "shell.execute_reply": "2025-07-21T16:13:49.788693Z",
     "shell.execute_reply.started": "2025-07-21T16:13:49.770600Z"
    },
    "id": "_cfHfqsPvi3O",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_xception(input_shape, train_dataset, val_dataset, epochs_num):\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    #x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"FITTING FULL XCEPTION\")\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:49.792117Z",
     "iopub.status.busy": "2025-07-21T16:13:49.791851Z",
     "iopub.status.idle": "2025-07-21T16:13:49.809823Z",
     "shell.execute_reply": "2025-07-21T16:13:49.809080Z",
     "shell.execute_reply.started": "2025-07-21T16:13:49.792100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_efficientnetb4(input_shape, train_dataset, val_dataset, epochs_num):\n",
    "    base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    #x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
    "\n",
    "    print(\"TRAINING FULL EfficientNetB4\")\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:49.811003Z",
     "iopub.status.busy": "2025-07-21T16:13:49.810677Z",
     "iopub.status.idle": "2025-07-21T16:13:49.825836Z",
     "shell.execute_reply": "2025-07-21T16:13:49.825039Z",
     "shell.execute_reply.started": "2025-07-21T16:13:49.810980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_efficientnetv2(input_shape, train_dataset, val_dataset, epochs_num):\n",
    "    base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    #base_model = EfficientNetV2M(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    #x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
    "\n",
    "    print(\"TRAINING FULL EfficientNetV2S\")\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:49.826813Z",
     "iopub.status.busy": "2025-07-21T16:13:49.826551Z",
     "iopub.status.idle": "2025-07-21T16:13:49.844027Z",
     "shell.execute_reply": "2025-07-21T16:13:49.843468Z",
     "shell.execute_reply.started": "2025-07-21T16:13:49.826795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_inceptionv3(input_shape, train_dataset, val_dataset, epochs_num):\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"TRAINING FULL InceptionV3\")\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_cnn(input_shape, train_dataset, val_dataset, epochs_num):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"TRAINING Simple CNN\")\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', restore_best_weights=True, verbose=1)\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "perform_tests(df=df,\n",
    "             train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_xception,\n",
    "              preprocess=preprocess_xception,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "perform_tests(df=df,\n",
    "               train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_inceptionv3,\n",
    "              preprocess=preprocess_inception_v3,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "perform_tests(df=df,\n",
    "              train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_efficientnetv2,\n",
    "              preprocess=preprocess_efficientnetv2,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "perform_tests(df=df,\n",
    "              train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_efficientnetb4,\n",
    "              preprocess=preprocess_efficientnet,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:13:49.845183Z",
     "iopub.status.busy": "2025-07-21T16:13:49.844917Z",
     "iopub.status.idle": "2025-07-21T16:51:59.041962Z",
     "shell.execute_reply": "2025-07-21T16:51:59.041053Z",
     "shell.execute_reply.started": "2025-07-21T16:13:49.845162Z"
    },
    "id": "yM2K_awwwv2m",
    "outputId": "65cbb83b-b777-4386-d68f-0cd46a28dc09",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 0 already done for xception. Skipping...\n",
      "Rep 1 already done for xception. Skipping...\n",
      "Done for xception.\n",
      "Rep 0 already done for inceptionv3. Skipping...\n",
      "Rep 1 already done for inceptionv3. Skipping...\n",
      "Done for inceptionv3.\n",
      "TRAINING FULL EfficientNetV2S\n",
      "Epoch 1/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 2s/step - accuracy: 0.5317 - loss: 0.6997 - val_accuracy: 0.6620 - val_loss: 0.6008\n",
      "Epoch 2/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 940ms/step - accuracy: 0.7442 - loss: 0.5141 - val_accuracy: 0.8440 - val_loss: 0.3781\n",
      "Epoch 3/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 933ms/step - accuracy: 0.8610 - loss: 0.3027 - val_accuracy: 0.8940 - val_loss: 0.2486\n",
      "Epoch 4/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 926ms/step - accuracy: 0.9219 - loss: 0.1873 - val_accuracy: 0.9060 - val_loss: 0.2479\n",
      "Epoch 5/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 928ms/step - accuracy: 0.9303 - loss: 0.1656 - val_accuracy: 0.9320 - val_loss: 0.1745\n",
      "Epoch 6/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 931ms/step - accuracy: 0.9600 - loss: 0.1005 - val_accuracy: 0.9400 - val_loss: 0.1468\n",
      "Epoch 7/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 930ms/step - accuracy: 0.9634 - loss: 0.0929 - val_accuracy: 0.9320 - val_loss: 0.2031\n",
      "Epoch 8/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 931ms/step - accuracy: 0.9726 - loss: 0.0704 - val_accuracy: 0.9480 - val_loss: 0.1723\n",
      "Epoch 9/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 933ms/step - accuracy: 0.9800 - loss: 0.0555 - val_accuracy: 0.9560 - val_loss: 0.1591\n",
      "Epoch 10/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 927ms/step - accuracy: 0.9845 - loss: 0.0413 - val_accuracy: 0.9380 - val_loss: 0.1506\n",
      "Epoch 11/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 930ms/step - accuracy: 0.9873 - loss: 0.0337 - val_accuracy: 0.9340 - val_loss: 0.2122\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step \n",
      "Rep:  0 | Model: efficientnetv2 | Feature Split: sex | Ratio: male:0.5/female:0.5 | Acc: 0.96\n",
      "TRAINING FULL EfficientNetV2S\n",
      "Epoch 1/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 2s/step - accuracy: 0.5299 - loss: 0.6999 - val_accuracy: 0.6360 - val_loss: 0.6166\n",
      "Epoch 2/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 938ms/step - accuracy: 0.7195 - loss: 0.5477 - val_accuracy: 0.8040 - val_loss: 0.3981\n",
      "Epoch 3/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 929ms/step - accuracy: 0.8539 - loss: 0.3375 - val_accuracy: 0.8760 - val_loss: 0.2928\n",
      "Epoch 4/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 933ms/step - accuracy: 0.9065 - loss: 0.2132 - val_accuracy: 0.9140 - val_loss: 0.1981\n",
      "Epoch 5/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 932ms/step - accuracy: 0.9379 - loss: 0.1512 - val_accuracy: 0.9340 - val_loss: 0.1806\n",
      "Epoch 6/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 931ms/step - accuracy: 0.9564 - loss: 0.1201 - val_accuracy: 0.9380 - val_loss: 0.1764\n",
      "Epoch 7/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 929ms/step - accuracy: 0.9623 - loss: 0.0958 - val_accuracy: 0.9120 - val_loss: 0.2238\n",
      "Epoch 8/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 932ms/step - accuracy: 0.9713 - loss: 0.0788 - val_accuracy: 0.9400 - val_loss: 0.1493\n",
      "Epoch 9/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 930ms/step - accuracy: 0.9803 - loss: 0.0551 - val_accuracy: 0.9520 - val_loss: 0.1550\n",
      "Epoch 10/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 931ms/step - accuracy: 0.9802 - loss: 0.0486 - val_accuracy: 0.9540 - val_loss: 0.1648\n",
      "Epoch 11/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 929ms/step - accuracy: 0.9817 - loss: 0.0466 - val_accuracy: 0.9480 - val_loss: 0.1922\n",
      "Epoch 12/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 944ms/step - accuracy: 0.9841 - loss: 0.0435 - val_accuracy: 0.9320 - val_loss: 0.1735\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3s/step \n",
      "Rep:  1 | Model: efficientnetv2 | Feature Split: sex | Ratio: male:0.5/female:0.5 | Acc: 0.95\n",
      "Done for efficientnetv2.\n"
     ]
    }
   ],
   "source": [
    "perform_tests(df=df,\n",
    "             train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_xception,\n",
    "              preprocess=preprocess_xception,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "perform_tests(df=df,\n",
    "              train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_inceptionv3,\n",
    "              preprocess=preprocess_inception_v3,\n",
    "              data_augmentation=augmentation_std\n",
    "              )\n",
    "\n",
    "perform_tests(df=df,\n",
    "             train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
    "              ],\n",
    "              validation_size=500,\n",
    "              reps=10,\n",
    "              feature_split_col='sex',\n",
    "              get_model=create_efficientnetv2,\n",
    "              preprocess=preprocess_efficientnetv2,\n",
    "              data_augmentation=augmentation_std\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T16:51:59.044673Z",
     "iopub.status.busy": "2025-07-21T16:51:59.044450Z",
     "iopub.status.idle": "2025-07-21T16:51:59.088405Z",
     "shell.execute_reply": "2025-07-21T16:51:59.087711Z",
     "shell.execute_reply.started": "2025-07-21T16:51:59.044656Z"
    },
    "id": "ivm4W_DLs7jK",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /kaggle/working/res/res_sex_efficientnetb4.csv does not exists!\n",
      "MODEL: efficientnetv2\n",
      "+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n",
      "|    | TrainRatio   | TestRatio   |   Accuracy |   AccuracySTD |    F1 |   F1STD |   EODiff |   EODiffSTD |\n",
      "|----+--------------+-------------+------------+---------------+-------+---------+----------+-------------|\n",
      "|  0 | 5/5          | 5/5         |      0.955 |         0.001 | 0.955 |   0.001 |     0.02 |       0.017 |\n",
      "+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n",
      "\n",
      "\n",
      "MODEL: xception\n",
      "+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n",
      "|    | TrainRatio   | TestRatio   |   Accuracy |   AccuracySTD |    F1 |   F1STD |   EODiff |   EODiffSTD |\n",
      "|----+--------------+-------------+------------+---------------+-------+---------+----------+-------------|\n",
      "|  0 | 5/5          | 5/5         |      0.915 |         0.016 | 0.915 |   0.016 |     0.04 |       0.011 |\n",
      "+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n",
      "\n",
      "\n",
      "MODEL: inceptionv3\n",
      "+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n",
      "|    | TrainRatio   | TestRatio   |   Accuracy |   AccuracySTD |    F1 |   F1STD |   EODiff |   EODiffSTD |\n",
      "|----+--------------+-------------+------------+---------------+-------+---------+----------+-------------|\n",
      "|  0 | 5/5          | 5/5         |      0.931 |         0.018 | 0.931 |   0.018 |    0.044 |       0.028 |\n",
      "+----+--------------+-------------+------------+---------------+-------+---------+----------+-------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_summarise_res(model_name:str):\n",
    "  path = RES_PATH + f'res_sex_{model_name}.csv'\n",
    "\n",
    "  if not os.path.exists(path):\n",
    "    print(f\"File {path} does not exists!\")\n",
    "    return\n",
    "\n",
    "  res = pd.read_csv(path)\n",
    "  gr = res.groupby(['train_ratio', 'test_ratio']).agg(\n",
    "      # Model=('model_name', 'first'),\n",
    "      TrainRatio=('train_ratio', 'first'),\n",
    "      TestRatio=('test_ratio', 'first'),\n",
    "      Accuracy= ('accuracy', 'mean'),\n",
    "      AccuracySTD= ('accuracy', 'std'),\n",
    "      F1=('f1_score', 'mean'),\n",
    "      F1STD=('f1_score', 'std'),\n",
    "      EODiff=('eo_diff', 'mean'),\n",
    "      EODiffSTD=('eo_diff', 'std'),\n",
    "  ).reset_index(drop=True)\n",
    "\n",
    "  gr = gr.round(3).sort_values(by=['TrainRatio', 'TestRatio'], ascending=False)\n",
    "\n",
    "  print(\"MODEL: \" + model_name)\n",
    "  print(tb.tabulate(gr, headers='keys', tablefmt='psql'))\n",
    "  print()\n",
    "  print()\n",
    "\n",
    "print_summarise_res('efficientnetv2')\n",
    "print_summarise_res('xception')\n",
    "print_summarise_res('inceptionv3')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNmMsy80ByKTqTKo1AGnh2X",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7892222,
     "sourceId": 12521871,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
