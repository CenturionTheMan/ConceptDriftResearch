{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9118653d",
   "metadata": {},
   "source": [
    "### Overall Idea\n",
    "\n",
    "The overall approach ensures that the impact of demographic feature distributions (like sex) on model performance is explicitly studied. By carefully controlling the sampling ratios in training and testing datasets, it enables fair comparisons and insights into potential biases or generalization challenges in the classification of skin lesion images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84430f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tabulate as tb\n",
    "from typing import Dict\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358e713",
   "metadata": {},
   "source": [
    "### 1. Data Preparation and Filtering\n",
    "\n",
    "- Load metadata about skin lesion images, including image IDs, diagnoses, and demographic features.\n",
    "- Create file paths for each image based on the metadata.\n",
    "- Filter the dataset to focus on three specific lesion classes.\n",
    "- Remove unnecessary columns to keep the dataset clean and relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "926edb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-------------+--------------+----------+--------+--------------------------------+\n",
      "|    | type   | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                           |\n",
      "|----+--------+--------+-------------+--------------+----------+--------+--------------------------------|\n",
      "|  0 | real   | male   |             | yes          | no       | big    | data/original/805/frame271.jpg |\n",
      "|  1 | real   | female | white       | no           |          | big    | data/original/083/frame191.jpg |\n",
      "|  2 | real   | male   | white       | no           | no       | small  | data/original/878/frame111.jpg |\n",
      "|  3 | real   | female | white       | no           |          |        | data/original/158/frame201.jpg |\n",
      "|  4 | real   | female | white       | no           |          |        | data/original/606/frame71.jpg  |\n",
      "+----+--------+--------+-------------+--------------+----------+--------+--------------------------------+\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/metadata.csv'\n",
    "df_tmp = pd.read_csv(file_path, sep=',')\n",
    "df_tmp['path'] = 'data/' + df_tmp['path']\n",
    "\n",
    "df_tmp = df_tmp[df_tmp['deepfake'] != 0]\n",
    "# df = df[df['male'] != 0]\n",
    "# df = df[df['white'] != 0]\n",
    "# df = df[df['black'] != 0]\n",
    "# df = df[df['asian'] != 0]\n",
    "# df = df[df['eyeglasses'] != 0]\n",
    "# df = df[df['heavy_makeup'] != 0]\n",
    "# df = df[df['big_lips'] != 0]\n",
    "\n",
    "df_tmp['ethnicity'] = df_tmp.apply(\n",
    "    lambda row: 'white' if row['white'] == 1 else ('black' if row['black'] == 1 else (\n",
    "        'asian' if row['asian'] == 1 else None)), axis=1)\n",
    "\n",
    "df = df_tmp[['deepfake', 'male', 'ethnicity', 'eyeglasses', 'heavy_makeup', 'big_lips', 'path']]\n",
    "\n",
    "df = df.rename(columns={'deepfake': 'type', 'male': 'sex', 'heavy_makeup': 'makeup', 'big_lips': 'lips',})\n",
    "\n",
    "df['type'] = df['type'].replace({1: 'fake', -1: 'real'})\n",
    "df['sex'] = df['sex'].replace({-1: 'female', 0: None, 1: 'male'})\n",
    "df['makeup'] = df['makeup'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
    "df['lips'] = df['lips'].replace({-1: 'small', 0: None, 1: 'big'})\n",
    "df['eyeglasses'] = df['eyeglasses'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
    "\n",
    "\n",
    "print(tb.tabulate(df.head(), headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ead6f6",
   "metadata": {},
   "source": [
    "### 2. Balanced Subset Sampling\n",
    "\n",
    "- Define a function to extract balanced subsets of the data based on class labels and specific feature values (e.g., male or female).\n",
    "- Ensure each class has the same number of samples within the chosen feature group.\n",
    "- This helps in creating fair and balanced datasets for training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf0abeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+-------------+--------------+----------+--------+------------------------------------+\n",
      "|    | type   | sex   | ethnicity   | eyeglasses   | makeup   | lips   | path                               |\n",
      "|----+--------+-------+-------------+--------------+----------+--------+------------------------------------|\n",
      "|  0 | fake   | male  | white       | no           | no       | small  | data/deepfake/374_407/frame41.jpg  |\n",
      "|  1 | fake   | male  | white       | no           | no       | small  | data/deepfake/015_919/frame281.jpg |\n",
      "|  2 | real   | male  |             | no           | no       | big    | data/original/995/frame11.jpg      |\n",
      "|  3 | real   | male  | white       | no           | no       |        | data/original/579/frame201.jpg     |\n",
      "+----+--------+-------+-------------+--------------+----------+--------+------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "def get_balanced_subset(\n",
    "    df, class_col, feature_col, feature_value,\n",
    "    samples_per_class, randomize=True, reset_index=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a balanced subset of the data for a given feature value, with equal number of samples per class.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        class_col: column name of class labels\n",
    "        feature_col: column name of feature\n",
    "        feature_value: specific feature value to filter\n",
    "        samples_per_class: number of samples per class\n",
    "        randomize: whether to shuffle within class before selecting\n",
    "        reset_index: whether to reset index of returned DataFrame\n",
    "        seed: random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Balanced DataFrame subset\n",
    "    \"\"\"\n",
    "    tmp = df[df[feature_col] == feature_value]\n",
    "\n",
    "    counts = tmp[class_col].value_counts()\n",
    "    for cl, count in counts.items():\n",
    "        if count < samples_per_class:\n",
    "            raise ValueError(f\"Not enough samples for class '{cl}' in feature '{feature_value}'. \"\n",
    "                             f\"Required: {samples_per_class}, Available: {count}\")\n",
    "\n",
    "    tmp = pd.concat([\n",
    "        (g.sample(frac=1, random_state=SEED).head(samples_per_class) if randomize else g.head(samples_per_class))\n",
    "        for _, g in tmp.groupby(class_col)\n",
    "    ])\n",
    "\n",
    "    if reset_index:\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "tmp_test = get_balanced_subset(\n",
    "    df=df, class_col='type', feature_col='sex', feature_value='male', \n",
    "    samples_per_class=2, randomize=True, reset_index=True)\n",
    "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44caca5d",
   "metadata": {},
   "source": [
    "### 3. Experimental Data Sampling with Feature Ratios\n",
    "\n",
    "- Develop a more flexible sampling function to create subsets with specific ratios of feature values (e.g., 20% male, 80% female).\n",
    "- Balance the classes within each feature group according to the desired ratio and total sample size.\n",
    "- Optionally exclude samples already used in other datasets to avoid overlap.\n",
    "- Validate that the resulting subsets respect the requested feature distributions within a certain tolerance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f991920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 'None' not found in ratios. Skipping.\n",
      "[] Ratios for ethnicity: {'black': 0.6, 'white': 0.2, 'asian': 0.2}\n",
      "[] Ratios for type: {'fake': 5, 'real': 5}\n",
      "\n",
      "+----+--------+--------+-------------+--------------+----------+--------+------------------------------------+\n",
      "|    | type   | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                               |\n",
      "|----+--------+--------+-------------+--------------+----------+--------+------------------------------------|\n",
      "|  0 | fake   | male   | white       | no           | no       | small  | data/deepfake/594_530/frame121.jpg |\n",
      "|  1 | real   | female | white       | no           |          |        | data/original/240/frame41.jpg      |\n",
      "|  2 | fake   | female | asian       |              |          | big    | data/deepfake/249_280/frame261.jpg |\n",
      "|  3 | real   | female | asian       | no           |          | big    | data/original/758/frame161.jpg     |\n",
      "|  4 | fake   | female | black       | no           |          | big    | data/deepfake/986_994/frame271.jpg |\n",
      "|  5 | fake   | male   | black       | no           | no       | big    | data/deepfake/144_122/frame101.jpg |\n",
      "|  6 | fake   | male   | black       | yes          | no       | big    | data/deepfake/081_087/frame41.jpg  |\n",
      "|  7 | real   | male   | black       | no           | no       | big    | data/original/715/frame231.jpg     |\n",
      "|  8 | real   | female | black       | no           |          | big    | data/original/762/frame61.jpg      |\n",
      "|  9 | real   | female | black       | no           |          | big    | data/original/328/frame241.jpg     |\n",
      "+----+--------+--------+-------------+--------------+----------+--------+------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "def get_exp_data(df, class_col, feature_col, ratio : Dict, size, randomize=True, exclude_column=None, exclude_df=None, max_diff=0.05):\n",
    "    '''\n",
    "    Get a balanced subset of the data based on specified ratios for features.\n",
    "    Args:\n",
    "        df: DataFrame containing the data\n",
    "        class_col: column name for class labels\n",
    "        feature_col: column name for features\n",
    "        ratio: dictionary with feature values as keys and their ratios as values\n",
    "        size: total number of samples to return\n",
    "        randomize: whether to shuffle the DataFrame before processing\n",
    "        exclude_column: column name to exclude from the DataFrame\n",
    "        exclude_df: DataFrame containing values to exclude based on exclude_column\n",
    "    '''\n",
    "    if randomize:\n",
    "        df_rnd = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    else:\n",
    "        df_rnd = df.copy()\n",
    "        \n",
    "    if exclude_column is not None and exclude_df is not None:\n",
    "        if exclude_column not in df_rnd.columns:\n",
    "            raise ValueError(f\"Column '{exclude_column}' not found in DataFrame.\")\n",
    "        if exclude_column not in exclude_df.columns:\n",
    "            raise ValueError(f\"Column '{exclude_column}' not found in exclude DataFrame.\")\n",
    "        df_rnd = df_rnd[~df_rnd[exclude_column].isin(exclude_df[exclude_column])]\n",
    "        \n",
    "    uniq_classes = df_rnd[class_col].unique()\n",
    "    uniq_features = df_rnd[feature_col].unique()\n",
    "    \n",
    "    def get_exp_data_inner(tmp_df, size):\n",
    "        df_tmp = None\n",
    "        for uf in uniq_features:\n",
    "            if ratio.get(uf) is None:\n",
    "                print(f\"Feature '{uf}' not found in ratios. Skipping.\")\n",
    "                continue            \n",
    "            c_amt = int(size * ratio[uf] / len(uniq_classes))\n",
    "            # if c_amt <= 0:\n",
    "            #     raise ValueError(f\"Calculated samples per class ({c_amt}) is less than or equal to zero for feature '{uf}' with ratio {ratio}.\")\n",
    "            tmp = get_balanced_subset(df=tmp_df, class_col=class_col, feature_col=feature_col, feature_value=uf, \n",
    "                                        samples_per_class=c_amt, randomize=False)\n",
    "            if df_tmp is None:\n",
    "                df_tmp = tmp\n",
    "            else:\n",
    "                df_tmp = pd.concat([df_tmp, tmp])\n",
    "        return df_tmp\n",
    "            \n",
    "    df_res = get_exp_data_inner(df_rnd, size)\n",
    "    \n",
    "    if len(df_res) < size:\n",
    "        print(f\"Samples for ({len(df_res)}) are less than requested ({size}).\")\n",
    "    \n",
    "    ratios_fet = df_res[feature_col].value_counts(normalize=True).to_dict()\n",
    "    ratios_cls = df_res[class_col].value_counts(normalize=False).to_dict()\n",
    "    print(f\"[] Ratios for {feature_col}: {ratios_fet}\")\n",
    "    print(f\"[] Ratios for {class_col}: {ratios_cls}\")\n",
    "    \n",
    "    for k in ratio:\n",
    "        if ratios_fet.get(k) is None:\n",
    "            if ratio[k] > 0.0:\n",
    "                raise ValueError(f\"Feature '{k}' not found in DataFrame after sampling (try increase 'size' parameter).\")\n",
    "        elif abs(ratios_fet[k] - ratio[k]) > max_diff:\n",
    "            raise ValueError(f\"Feature '{k}' ratio {ratios_fet[k]} differs from requested {ratio[k]} by more than {max_diff}.\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    df_res = df_res.reset_index(drop=True)\n",
    "    \n",
    "    return df_res      \n",
    "\n",
    "tmp_test = get_exp_data(\n",
    "    df=df, class_col='type', feature_col='ethnicity', ratio={'white':0.2, 'black':0.6, 'asian': 0.2}, size=10, randomize=True)\n",
    "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee01fc",
   "metadata": {},
   "source": [
    "### 4. Data Loading and Model Preparation\n",
    "\n",
    "- Implement image loading and preprocessing to prepare images for model input.\n",
    "- Convert class labels from strings to integer codes for model compatibility.\n",
    "- Create TensorFlow datasets from file paths and labels for efficient batch processing.\n",
    "- Define two types of models:\n",
    "  - A simple convolutional neural network built from scratch.\n",
    "  - A transfer learning model based on MobileNetV2, using pretrained weights and a custom classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_path, target_size=(224, 224)):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, target_size)\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "def get_data_for_model(df, class_col, files_col):\n",
    "    image_paths = df[files_col].values\n",
    "    labels = df[class_col].values\n",
    "    labels = df[class_col].astype('category').cat.codes.values #classes strs to ints\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda path, label: (load_image(path), label))\n",
    "    dataset = dataset.batch(32)\n",
    "    return dataset\n",
    "    \n",
    "def create_simple_model(num_classes, input_shape=(224, 224, 3)):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model, \"CNV\"\n",
    "\n",
    "\n",
    "def create_mobile_net2(num_classes, input_shape=(224, 224, 3)):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model, \"mobile net 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0747b",
   "metadata": {},
   "source": [
    "### 5. Experiment Execution\n",
    "\n",
    "- Build a function to run repeated training and evaluation experiments.\n",
    "- For each repetition:\n",
    "  - Sample training data according to specified feature ratios.\n",
    "  - Sample multiple test sets with different feature distributions to analyze model performance under varying conditions.\n",
    "  - Train the model on the training set.\n",
    "  - Evaluate the model on each test set, calculating accuracy and F1 scores.\n",
    "- Save results after each repetition to keep track of performance metrics across experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8565ac95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 'None' not found in ratios. Skipping.\n",
      "[] Ratios for sex: {'female': 0.9, 'male': 0.1}\n",
      "[] Ratios for type: {'fake': 500, 'real': 500}\n",
      "\n",
      "Feature 'None' not found in ratios. Skipping.\n",
      "[] Ratios for sex: {'female': 0.9, 'male': 0.1}\n",
      "[] Ratios for type: {'fake': 50, 'real': 50}\n",
      "\n",
      "Feature 'None' not found in ratios. Skipping.\n",
      "[] Ratios for sex: {'female': 0.7, 'male': 0.3}\n",
      "[] Ratios for type: {'fake': 50, 'real': 50}\n",
      "\n",
      "Feature 'None' not found in ratios. Skipping.\n",
      "[] Ratios for sex: {'male': 0.5, 'female': 0.5}\n",
      "[] Ratios for type: {'fake': 50, 'real': 50}\n",
      "\n",
      "Feature 'None' not found in ratios. Skipping.\n",
      "[] Ratios for sex: {'male': 0.7, 'female': 0.3}\n",
      "[] Ratios for type: {'fake': 50, 'real': 50}\n",
      "\n",
      "Feature 'None' not found in ratios. Skipping.\n",
      "[] Ratios for sex: {'male': 0.9, 'female': 0.1}\n",
      "[] Ratios for type: {'fake': 50, 'real': 50}\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 500ms/step - accuracy: 0.6567 - loss: 1.4436\n",
      "Epoch 2/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 529ms/step - accuracy: 0.2735 - loss: 1.2516\n",
      "Epoch 3/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 525ms/step - accuracy: 0.1753 - loss: 0.7013\n",
      "Epoch 4/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 524ms/step - accuracy: 0.2067 - loss: 0.6914\n",
      "Epoch 5/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 565ms/step - accuracy: 0.2247 - loss: 0.9936\n",
      "Epoch 6/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 548ms/step - accuracy: 0.2241 - loss: 0.6813\n",
      "Epoch 7/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 564ms/step - accuracy: 0.1782 - loss: 1.1594\n",
      "Epoch 8/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 536ms/step - accuracy: 0.7180 - loss: 0.6855\n",
      "Epoch 9/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 520ms/step - accuracy: 0.6898 - loss: 0.6864\n",
      "Epoch 10/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 539ms/step - accuracy: 0.7314 - loss: 0.6773\n",
      "Epoch 11/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 553ms/step - accuracy: 0.4204 - loss: 1.2779\n",
      "Epoch 12/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 541ms/step - accuracy: 0.7538 - loss: 0.6810\n",
      "Epoch 13/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 526ms/step - accuracy: 0.7588 - loss: 0.6817\n",
      "Epoch 14/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 555ms/step - accuracy: 0.7039 - loss: 0.6814\n",
      "Epoch 15/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 555ms/step - accuracy: 0.4079 - loss: 1.1783\n",
      "Epoch 16/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 535ms/step - accuracy: 0.7318 - loss: 0.6859\n",
      "Epoch 17/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 541ms/step - accuracy: 0.7463 - loss: 0.6798\n",
      "Epoch 18/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 545ms/step - accuracy: 0.6773 - loss: 0.6831\n",
      "Epoch 19/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 603ms/step - accuracy: 0.2711 - loss: 0.8800\n",
      "Epoch 20/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 545ms/step - accuracy: 0.7388 - loss: 0.6738\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 828ms/step\n",
      "Rep:  0 | Model: mobile net 2 | Feature Split: sex | Ratio: male:0.1/female:0.9 | Acc: 0.51\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 366ms/step\n",
      "Rep:  0 | Model: mobile net 2 | Feature Split: sex | Ratio: male:0.3/female:0.7 | Acc: 0.51\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 364ms/step\n",
      "Rep:  0 | Model: mobile net 2 | Feature Split: sex | Ratio: male:0.5/female:0.5 | Acc: 0.53\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 372ms/step\n",
      "Rep:  0 | Model: mobile net 2 | Feature Split: sex | Ratio: male:0.7/female:0.3 | Acc: 0.53\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 357ms/step\n",
      "Rep:  0 | Model: mobile net 2 | Feature Split: sex | Ratio: male:0.9/female:0.1 | Acc: 0.54\n"
     ]
    }
   ],
   "source": [
    "def perform_tests(df, train_metas, test_metas, reps, class_col, feature_split_col, exclude_column, files_col, get_model, epochs_num):\n",
    "    res = []\n",
    "\n",
    "    for r in range(reps):\n",
    "        for train_meta in train_metas:\n",
    "            train = get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=train_meta['ratio'], size=train_meta['size'])\n",
    "            tests = [\n",
    "                get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=tm['ratio'], size=tm['size'], exclude_column=exclude_column, exclude_df=train) for tm in test_metas\n",
    "            ]\n",
    "\n",
    "            train_dataset = get_data_for_model(train, class_col=class_col, files_col=files_col)\n",
    "            test_datasets = [\n",
    "                get_data_for_model(test, class_col=class_col, files_col=files_col) for test in tests\n",
    "            ]\n",
    "\n",
    "            train_ratio = '/'.join([f\"{k}:{v}\" for k, v in train_meta['ratio'].items()])\n",
    "            train_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in train[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
    "            train_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', train_ratio)\n",
    "\n",
    "            \n",
    "            model, model_name = get_model(num_classes=len(df[class_col].unique()))\n",
    "            model.fit(train_dataset, epochs=epochs_num)\n",
    "            \n",
    "            for test_dataset, test_meta, test_df in zip(test_datasets, test_metas, tests):\n",
    "                predictions = model.predict(test_dataset)            \n",
    "                y_true = test_df[class_col].astype('category').cat.codes.values\n",
    "                y_pred = np.argmax(predictions, axis=1)\n",
    "                acc = accuracy_score(y_true, y_pred)\n",
    "                f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "                eo_diff = equalized_odds_difference(y_true, y_pred, sensitive_features=test_df[feature_split_col])\n",
    "                \n",
    "                test_ratio = '/'.join([f\"{k}:{v}\" for k, v in test_meta['ratio'].items()])\n",
    "                test_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in test_df[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
    "                test_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', test_ratio)\n",
    "                \n",
    "                res.append([\n",
    "                    r,\n",
    "                    model_name,\n",
    "                    feature_split_col,\n",
    "                    train_meta['size'], \n",
    "                    train_ratio,\n",
    "                    test_meta['size'],\n",
    "                    test_ratio,\n",
    "                    acc,\n",
    "                    f1,\n",
    "                    eo_diff,\n",
    "                    train_ratio_rel,\n",
    "                    test_ratio_rel,\n",
    "                    train_ratio_sim,\n",
    "                    test_ratio_sim\n",
    "                ])    \n",
    "\n",
    "                print(f\"Rep: {r:2} | Model: {model_name} | Feature Split: {feature_split_col} | Ratio: {test_ratio} | Acc: {acc:.2f}\")\n",
    "                \n",
    "                res_df = pd.DataFrame(res, columns=[\n",
    "                    'rep', 'model_name', 'feature_split_col', \n",
    "                    'train_size', 'train_ratio_detail', 'test_size', 'test_ratio_detail',\n",
    "                    'accuracy', 'f1_score', 'eo_diff', 'train_ratio_rel', 'test_ratio_rel', \"train_ratio\", \"test_ratio\"\n",
    "                ])\n",
    "                \n",
    "                res_df.to_csv(f'res/res_{model_name.replace(' ', '_')}.csv', index=False)         \n",
    "            \n",
    "        \n",
    "perform_tests(df=df, \n",
    "              train_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 1000},\n",
    "                  ],\n",
    "              test_metas=[\n",
    "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 100},\n",
    "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 100},\n",
    "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 100},\n",
    "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 100},\n",
    "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 100},\n",
    "              ],\n",
    "              reps=1,\n",
    "              class_col='type',\n",
    "              feature_split_col='sex',\n",
    "              exclude_column='path',\n",
    "              files_col='path',\n",
    "              get_model=create_mobile_net2,\n",
    "              epochs_num=20\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb915e0",
   "metadata": {},
   "source": [
    "### 6. Results Aggregation and Summary\n",
    "\n",
    "- Load the experiment results from saved files.\n",
    "- Group results by test feature ratios and calculate average performance metrics along with their variability.\n",
    "- Present the summarized results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79932277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-------------+------------+---------------+------+---------+----------+-------------+\n",
      "|    | TrainRatio   | TestRatio   |   Accuracy |   AccuracySTD |   F1 |   F1STD |   EODiff |   EODiffSTD |\n",
      "|----+--------------+-------------+------------+---------------+------+---------+----------+-------------|\n",
      "|  4 | 1/9          | 9/1         |        0.5 |           nan | 0.33 |     nan |        0 |         nan |\n",
      "|  3 | 1/9          | 7/3         |        0.5 |           nan | 0.33 |     nan |        0 |         nan |\n",
      "|  2 | 1/9          | 5/5         |        0.5 |           nan | 0.33 |     nan |        0 |         nan |\n",
      "|  1 | 1/9          | 3/7         |        0.5 |           nan | 0.33 |     nan |        0 |         nan |\n",
      "|  0 | 1/9          | 1/9         |        0.5 |           nan | 0.33 |     nan |        0 |         nan |\n",
      "+----+--------------+-------------+------------+---------------+------+---------+----------+-------------+\n"
     ]
    }
   ],
   "source": [
    "res = pd.read_csv('res/res_mobile_net_2.csv')\n",
    "\n",
    "gr = res.groupby(['train_ratio', 'test_ratio']).agg(\n",
    "    # Model=('model_name', 'first'),\n",
    "    TrainRatio=('train_ratio', 'first'),\n",
    "    TestRatio=('test_ratio', 'first'),\n",
    "    Accuracy= ('accuracy', 'mean'),\n",
    "    AccuracySTD= ('accuracy', 'std'),\n",
    "    F1=('f1_score', 'mean'),\n",
    "    F1STD=('f1_score', 'std'),\n",
    "    EODiff=('eo_diff', 'mean'),\n",
    "    EODiffSTD=('eo_diff', 'std'),\n",
    ").reset_index(drop=True)\n",
    "\n",
    "gr = gr.round(2).sort_values(by=['TrainRatio', 'TestRatio'], ascending=False)\n",
    "              \n",
    "print(tb.tabulate(gr, headers='keys', tablefmt='psql'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
