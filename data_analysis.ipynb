{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28fc076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import tabulate as tb\n",
    "from typing import Literal, List, Tuple\n",
    "import scipy\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "import glob\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f29fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_PATH = './res/'\n",
    "ROC_PREFIX='roc'\n",
    "SUMM_PREFIX='res'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bd99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_files = [os.path.join(RES_PATH, f) for f in os.listdir(RES_PATH) if f.startswith(SUMM_PREFIX) and f.endswith('.csv')]\n",
    "print(\"Summary files found:\", summary_files)\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "for file in summary_files:\n",
    "    try:\n",
    "        temp_df = pd.read_csv(file)\n",
    "        df_all = pd.concat([df_all, temp_df], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        \n",
    "df_all['precision'] = df_all['TP'] / (df_all['TP'] + df_all['FP'])\n",
    "df_all['recall'] = df_all['TP'] / (df_all['TP'] + df_all['FN'])\n",
    "df_all['f1_score'] = 2 * (df_all['precision'] * df_all['recall']) / (df_all['precision'] + df_all['recall'])\n",
    "df_all = df_all.rename(columns={'model_name': 'model'})\n",
    "df_all['model'] = df_all['model'].replace({\n",
    "    'efficientnetv2': 'EfficientNetV2S',\n",
    "    'mobilenetv2': 'MobileNetV2'\n",
    "    })\n",
    "\n",
    "print(tb.tabulate(df_all, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bdca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_base_metrics(df_org, feature, train_ratio):\n",
    "    df = df_org[(df_org['train_ratio'] == train_ratio) & (df_org['feature_split_col'] == feature)].copy()\n",
    "    df['test_ratio'] = pd.Categorical(\n",
    "            df['test_ratio'], \n",
    "            categories=sorted(df['test_ratio'].unique(), key=lambda x: eval(x.replace('/', ','))),\n",
    "            ordered=True\n",
    "        )\n",
    "    \n",
    "    def create_plt():\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Accuracy\n",
    "        acc_plot = sns.lineplot(\n",
    "            data=df,\n",
    "            x='test_ratio',\n",
    "            y='accuracy',\n",
    "            hue='model',\n",
    "            marker='o',\n",
    "            linestyle='--',\n",
    "            err_style='bars',\n",
    "            errorbar='sd',\n",
    "            ax=axes[0]\n",
    "        )\n",
    "        axes[0].set_title(f'Accuracy vs Test Ratio (feature={feature}, train ratio={train_ratio})')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].set_xlabel('Test Ratio')\n",
    "        #axes[0].set_ylim(0.85, 1.0)\n",
    "        axes[0].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "        # F1\n",
    "        f1_plot = sns.lineplot(\n",
    "            data=df,\n",
    "            x='test_ratio',\n",
    "            y='f1_score',\n",
    "            hue='model',\n",
    "            marker='s',\n",
    "            linestyle='--',\n",
    "            err_style='bars',\n",
    "            errorbar='sd',\n",
    "            ax=axes[1]\n",
    "        )\n",
    "        axes[1].set_title(f'F1 Score vs Test Ratio (feature={feature}, train ratio={train_ratio})')\n",
    "        axes[1].set_ylabel('F1 Score')\n",
    "        axes[1].set_xlabel('Test Ratio')\n",
    "        #axes[1].set_ylim(0.85, 1.0)\n",
    "        axes[1].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "        # ðŸ”· Add vertical line\n",
    "        if train_ratio in df['test_ratio'].unique():\n",
    "            axes[0].axvline(x=train_ratio, color='black', linestyle=':', alpha=0.7, label='Train ratio')\n",
    "            axes[1].axvline(x=train_ratio, color='black', linestyle=':', alpha=0.7, label='Train ratio')\n",
    "            axes[0].legend(title=None)\n",
    "            axes[1].legend(title=None)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def create_table():\n",
    "        tmp = df.groupby(['test_ratio','model'], observed=False).agg(\n",
    "            TestRatio=('test_ratio', 'first'),\n",
    "            Model=('model', 'first'),\n",
    "            Acc=('accuracy', 'mean'),\n",
    "            AccStd=('accuracy', 'std'),\n",
    "            FI=('f1_score', 'mean'),\n",
    "            FiStd=('f1_score', 'std')\n",
    "        ).reset_index(drop=True)\n",
    "        \n",
    "        tmp_acc = tmp.pivot(index='TestRatio', columns='Model', values=['Acc', 'AccStd'])\n",
    "        tmp_acc.columns = [f'{metric}{model}' for metric, model in tmp_acc.columns]\n",
    "        \n",
    "        tmp_f1 = tmp.pivot(index='TestRatio', columns='Model', values=['FI', 'FiStd'])\n",
    "        tmp_f1.columns = [f'{metric}{model}' for metric, model in tmp_f1.columns]\n",
    "\n",
    "        print(f\"Accuracy Table (feature={feature}, train ratio={train_ratio}):\")        \n",
    "        print(tb.tabulate(tmp_acc, headers='keys', tablefmt='psql'))\n",
    "        \n",
    "        print(f\"F1 Score Table (feature={feature}, train ratio={train_ratio}):\")\n",
    "        print(tb.tabulate(tmp_f1, headers='keys', tablefmt='psql'))\n",
    "        \n",
    "    create_table()\n",
    "    create_plt()\n",
    "    \n",
    "handle_base_metrics(df_all, 'sex', '2/8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_base_metrics(df_org, feature, train_ratio, value):\n",
    "    df = df_org[(df_org['train_ratio'] == train_ratio) & (df_org['feature_split_col'] == feature)].copy()\n",
    "    df['test_ratio'] = pd.Categorical(\n",
    "            df['test_ratio'], \n",
    "            categories=sorted(df['test_ratio'].unique(), key=lambda x: eval(x.replace('/', ','))),\n",
    "            ordered=True)\n",
    "    \n",
    "    res = []\n",
    "    for test_ratio in df['test_ratio'].unique():\n",
    "        columns = [\"Test ratio\"]\n",
    "        single = [test_ratio]\n",
    "        all_normal = True\n",
    "        for model in df['model'].unique():\n",
    "            df_tmp = df[(df['test_ratio'] == test_ratio) & (df['model'] == model)]\n",
    "            \n",
    "            shapiro_p = stats.shapiro(df_tmp[value])[1]\n",
    "            is_normal = shapiro_p > 0.05\n",
    "            if not is_normal:\n",
    "                all_normal = False\n",
    "            single.append(f'Shapiro: p={shapiro_p:.3}')\n",
    "            columns.append(f'{model} normality')\n",
    "        \n",
    "        levene_data = df_tmp[value].values\n",
    "        if all_normal:\n",
    "            stat, p_leve = stats.levene(*levene_data, center='mean')\n",
    "            single.append(f'Levene (mean): p={p_leve:.3}')\n",
    "        else:\n",
    "            stat, p_leve = stats.levene(*levene_data, center='median')\n",
    "            single.append(f'Levene (median): p={p_leve:.3}')\n",
    "        is_variance_equal = p_leve > 0.05\n",
    "        columns.append('Variance equality')\n",
    "        \n",
    "        if all_normal and is_variance_equal:\n",
    "            anova_results = pg.anova(data=df, dv=value, between='model')\n",
    "            name_anova = 'ANOVA'\n",
    "            poc_name = 'Tukey'\n",
    "            poc_results = pairwise_tukeyhsd(df[value], df['model'])\n",
    "        elif all_normal and not is_variance_equal:\n",
    "            anova_results = pg.welch_anova(data=df, dv=value, between='model')\n",
    "            name_anova = 'Welch ANOVA'\n",
    "            poc_name = 'Games-Howell'\n",
    "            poc_results = sp.posthoc_gameshowell(df, val_col=value, group_col='model')\n",
    "        else:\n",
    "            anova_results = pg.kruskal(data=df, dv=value, between='model')\n",
    "            name_anova = 'Kruskal-Wallis'\n",
    "            poc_name = 'Dunn'\n",
    "            poc_results = sp.posthoc_dunn(df, val_col=value, group_col='model', p_adjust='holm')\n",
    "        p_anova = anova_results['p-unc'].values[0]\n",
    "        rownosc = f'{name_anova}: {p_anova:.2E}'\n",
    "        h0_result =  'H0 rejected' if p_anova < 0.05 else 'H0 accepted'\n",
    "        \n",
    "        single.append(rownosc)\n",
    "        single.append(h0_result)\n",
    "        columns.append('ANOVA result')\n",
    "        columns.append('H0 result')\n",
    "        res.append(single)\n",
    "        \n",
    "        poc_results = pd.DataFrame(poc_results)\n",
    "        poc_results['Comparison'] = poc_results.index\n",
    "        poc_results['TestRatio'] = test_ratio\n",
    "        poc_results['Test'] = poc_name\n",
    "        poc_results.to_csv(f'analysis/posthoc_{feature}_{train_ratio.replace(\"/\",\"_\")}_{test_ratio.replace(\"/\",\"_\")}_{value}.csv', index=False)\n",
    "        \n",
    "    ana_df = pd.DataFrame(res, columns=columns)\n",
    "    ana_df.to_csv(f'analysis/stat_{feature}_{train_ratio}.csv', index=False)\n",
    "    print(tb.tabulate(ana_df, headers='keys', tablefmt='psql'))\n",
    "        \n",
    "handle_base_metrics(df_all, 'sex', '2/8', value='accuracy')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
