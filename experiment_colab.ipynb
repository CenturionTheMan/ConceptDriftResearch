{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CenturionTheMan/ConceptDriftResearch/blob/main/experiment_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REMOVE_RES_CONTENT = True\n",
        "IMG_SIZE=224\n",
        "#IMG_SIZE=64\n",
        "IMG_RESIZE=False\n",
        "SEED = 42\n",
        "DATA_ZIP_PATH=f'/content/drive/MyDrive/Studia/MAGISTER/PracaMagisterska/data224.zip'\n",
        "RES_PATH = '/content/drive/MyDrive/Studia/MAGISTER/PracaMagisterska/res/'\n",
        "SHOW_EXTRA_INFO=False"
      ],
      "metadata": {
        "id": "uO5Tf1r1Ts6P"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shghl8SNlxZ3"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFFQKC56qsb-",
        "outputId": "1f389cd5-929a-42f6-83ed-b1d3fd4de4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Pliki już są rozpakowane — pomijam kopiowanie i rozpakowywanie.\n",
            "total 890M\n",
            "drwxrwxr-x 4 root root 4.0K Jul 16 20:22 data\n",
            "-rw------- 1 root root 890M Jul 22 13:24 data.zip\n",
            "drwx------ 6 root root 4.0K Jul 22 13:24 drive\n",
            "drwxr-xr-x 1 root root 4.0K Jul 18 13:38 sample_data\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Montuj Google Drive (tylko raz)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ścieżki\n",
        "zip_path = '/content/data.zip'\n",
        "dataset_dir = '/content/'\n",
        "zip_on_drive = DATA_ZIP_PATH\n",
        "\n",
        "# Sprawdź, czy katalog z danymi już istnieje\n",
        "if not os.path.exists(dataset_dir+'data') or len(os.listdir(dataset_dir)) == 0:\n",
        "    print(\"Pliki jeszcze nie wypakowane — kopiuję i rozpakowuję...\")\n",
        "    # Skopiuj zip z Drive do RAM\n",
        "    if not os.path.exists(zip_path):\n",
        "        !cp \"$zip_on_drive\" \"$zip_path\"\n",
        "    # Rozpakuj zip\n",
        "    !unzip -q \"$zip_path\" -d \"$dataset_dir\"\n",
        "else:\n",
        "    print(\"Pliki już są rozpakowane — pomijam kopiowanie i rozpakowywanie.\")\n",
        "\n",
        "# Podejrzyj zawartość\n",
        "!ls -lh \"$dataset_dir\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPKtaDEKrVlM",
        "outputId": "fe2f30d3-9696-417c-f608-bd87226dac63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tabulate\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate as tb\n",
        "from typing import Dict\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import re\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Input, BatchNormalization, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tensorflow.keras.applications import Xception, EfficientNetB4, InceptionV3, EfficientNetV2M,EfficientNetV2S , MobileNetV2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, BatchNormalization, LeakyReLU,\n",
        "    MaxPooling2D, Flatten, Dense, Dropout, concatenate\n",
        ")\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score, log_loss, brier_score_loss\n",
        "import time\n",
        "\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name:\n",
        "    print(f\"GPU available: {device_name}\")\n",
        "else:\n",
        "    print(\"No GPU available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSoODNE0lxZ7"
      },
      "source": [
        "# Dataset splitting - handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQHjq4DXrY3h",
        "outputId": "4e1542e5-810b-47e7-8334-9a4841500ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+--------+-------------+--------------+----------+--------+-----------------------------------------+------------+\n",
            "|    |   type | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                                    | typeName   |\n",
            "|----+--------+--------+-------------+--------------+----------+--------+-----------------------------------------+------------|\n",
            "|  0 |      0 | male   |             | yes          | no       | big    | /content/data/original/805/frame271.jpg | real       |\n",
            "|  1 |      0 | female | white       | no           |          | big    | /content/data/original/083/frame191.jpg | real       |\n",
            "|  2 |      0 | male   | white       | no           | no       | small  | /content/data/original/878/frame111.jpg | real       |\n",
            "|  3 |      0 | female | white       | no           |          |        | /content/data/original/158/frame201.jpg | real       |\n",
            "|  4 |      0 | female | white       | no           |          |        | /content/data/original/606/frame71.jpg  | real       |\n",
            "+----+--------+--------+-------------+--------------+----------+--------+-----------------------------------------+------------+\n",
            "Dataset size: 59552\n"
          ]
        }
      ],
      "source": [
        "file_path = '/content/data/metadata.csv'\n",
        "df_tmp = pd.read_csv(file_path, sep=',')\n",
        "df_tmp['path'] = '/content/data/' + df_tmp['path']\n",
        "\n",
        "df_tmp = df_tmp[df_tmp['deepfake'] != 0]\n",
        "\n",
        "df_tmp['ethnicity'] = df_tmp.apply(\n",
        "    lambda row: 'white' if row['white'] == 1 else ('black' if row['black'] == 1 else (\n",
        "        'asian' if row['asian'] == 1 else None)), axis=1)\n",
        "\n",
        "df = df_tmp[['deepfake', 'male', 'ethnicity', 'eyeglasses', 'heavy_makeup', 'big_lips', 'path']]\n",
        "\n",
        "df = df.rename(columns={'deepfake': 'type', 'male': 'sex', 'heavy_makeup': 'makeup', 'big_lips': 'lips',})\n",
        "\n",
        "df['typeName'] = df['type'].replace({1: 'fake', -1: 'real'})\n",
        "df['type'] = df['typeName'].map({'fake': 1, 'real': 0}).values\n",
        "df['sex'] = df['sex'].replace({-1: 'female', 0: None, 1: 'male'})\n",
        "df['makeup'] = df['makeup'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
        "df['lips'] = df['lips'].replace({-1: 'small', 0: None, 1: 'big'})\n",
        "df['eyeglasses'] = df['eyeglasses'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
        "\n",
        "\n",
        "print(tb.tabulate(df.head(), headers='keys', tablefmt='psql'))\n",
        "print(f\"Dataset size: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqJXku-tsxK7",
        "outputId": "2806d74f-68d9-4b05-cd82-66cbed6b637f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+-------+-------------+--------------+----------+--------+---------------------------------------------+------------+\n",
            "|    |   type | sex   | ethnicity   | eyeglasses   | makeup   | lips   | path                                        | typeName   |\n",
            "|----+--------+-------+-------------+--------------+----------+--------+---------------------------------------------+------------|\n",
            "|  0 |      0 | male  |             | no           | no       | big    | /content/data/original/995/frame11.jpg      | real       |\n",
            "|  1 |      0 | male  | white       | no           | no       |        | /content/data/original/579/frame201.jpg     | real       |\n",
            "|  2 |      1 | male  | white       | no           | no       | small  | /content/data/deepfake/374_407/frame41.jpg  | fake       |\n",
            "|  3 |      1 | male  | white       | no           | no       | small  | /content/data/deepfake/015_919/frame281.jpg | fake       |\n",
            "+----+--------+-------+-------------+--------------+----------+--------+---------------------------------------------+------------+\n"
          ]
        }
      ],
      "source": [
        "def get_balanced_subset(\n",
        "    df, class_col, feature_col, feature_value,\n",
        "    samples_per_class, randomize=True, reset_index=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Select a balanced subset of the data for a given feature value, with equal number of samples per class.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame\n",
        "        class_col: column name of class labels\n",
        "        feature_col: column name of feature\n",
        "        feature_value: specific feature value to filter\n",
        "        samples_per_class: number of samples per class\n",
        "        randomize: whether to shuffle within class before selecting\n",
        "        reset_index: whether to reset index of returned DataFrame\n",
        "        seed: random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Balanced DataFrame subset\n",
        "    \"\"\"\n",
        "    tmp = df[df[feature_col] == feature_value]\n",
        "\n",
        "    counts = tmp[class_col].value_counts()\n",
        "    for cl, count in counts.items():\n",
        "        if count < samples_per_class:\n",
        "            raise ValueError(f\"Not enough samples for class '{cl}' in feature '{feature_value}'. \"\n",
        "                             f\"Required: {samples_per_class}, Available: {count}\")\n",
        "\n",
        "    tmp = pd.concat([\n",
        "        (g.sample(frac=1, random_state=SEED).head(samples_per_class) if randomize else g.head(samples_per_class))\n",
        "        for _, g in tmp.groupby(class_col)\n",
        "    ])\n",
        "\n",
        "    if reset_index:\n",
        "        tmp = tmp.reset_index(drop=True)\n",
        "\n",
        "    return tmp\n",
        "\n",
        "tmp_test = get_balanced_subset(\n",
        "    df=df, class_col='type', feature_col='sex', feature_value='male',\n",
        "    samples_per_class=2, randomize=True, reset_index=True)\n",
        "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJZmhWnis0yh",
        "outputId": "260b9b18-5d0e-408b-eaf7-6a24e4f69792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+--------+-------------+--------------+----------+--------+---------------------------------------------+------------+\n",
            "|    |   type | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                                        | typeName   |\n",
            "|----+--------+--------+-------------+--------------+----------+--------+---------------------------------------------+------------|\n",
            "|  0 |      0 | female | white       | no           |          |        | /content/data/original/240/frame41.jpg      | real       |\n",
            "|  1 |      1 | male   | white       | no           | no       | small  | /content/data/deepfake/594_530/frame121.jpg | fake       |\n",
            "|  2 |      0 | female | asian       | no           |          | big    | /content/data/original/758/frame161.jpg     | real       |\n",
            "|  3 |      1 | female | asian       |              |          | big    | /content/data/deepfake/249_280/frame261.jpg | fake       |\n",
            "|  4 |      0 | male   | black       | no           | no       | big    | /content/data/original/715/frame231.jpg     | real       |\n",
            "|  5 |      0 | female | black       | no           |          | big    | /content/data/original/762/frame61.jpg      | real       |\n",
            "|  6 |      0 | female | black       | no           |          | big    | /content/data/original/328/frame241.jpg     | real       |\n",
            "|  7 |      1 | female | black       | no           |          | big    | /content/data/deepfake/986_994/frame271.jpg | fake       |\n",
            "|  8 |      1 | male   | black       | no           | no       | big    | /content/data/deepfake/144_122/frame101.jpg | fake       |\n",
            "|  9 |      1 | male   | black       | yes          | no       | big    | /content/data/deepfake/081_087/frame41.jpg  | fake       |\n",
            "+----+--------+--------+-------------+--------------+----------+--------+---------------------------------------------+------------+\n"
          ]
        }
      ],
      "source": [
        "def get_exp_data(df, class_col, feature_col, ratio : Dict, size, randomize=True, exclude_column=None, exclude_df=None, max_diff=0.05):\n",
        "    '''\n",
        "    Get a balanced subset of the data based on specified ratios for features.\n",
        "    Args:\n",
        "        df: DataFrame containing the data\n",
        "        class_col: column name for class labels\n",
        "        feature_col: column name for features\n",
        "        ratio: dictionary with feature values as keys and their ratios as values\n",
        "        size: total number of samples to return\n",
        "        randomize: whether to shuffle the DataFrame before processing\n",
        "        exclude_column: column name to exclude from the DataFrame\n",
        "        exclude_df: DataFrame containing values to exclude based on exclude_column\n",
        "    '''\n",
        "    if randomize:\n",
        "        df_rnd = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "    else:\n",
        "        df_rnd = df.copy()\n",
        "\n",
        "    if exclude_column is not None and exclude_df is not None:\n",
        "        if exclude_column not in df_rnd.columns:\n",
        "            raise ValueError(f\"Column '{exclude_column}' not found in DataFrame.\")\n",
        "        if exclude_column not in exclude_df.columns:\n",
        "            raise ValueError(f\"Column '{exclude_column}' not found in exclude DataFrame.\")\n",
        "        df_rnd = df_rnd[~df_rnd[exclude_column].isin(exclude_df[exclude_column])]\n",
        "\n",
        "    uniq_classes = df_rnd[class_col].unique()\n",
        "    uniq_features = df_rnd[feature_col].unique()\n",
        "\n",
        "    def get_exp_data_inner(tmp_df, size):\n",
        "        df_tmp = None\n",
        "        for uf in uniq_features:\n",
        "            if ratio.get(uf) is None:\n",
        "                if SHOW_EXTRA_INFO:\n",
        "                    print(f\"Feature '{uf}' not found in ratios. Skipping.\")\n",
        "                continue\n",
        "            c_amt = int(size * ratio[uf] / len(uniq_classes))\n",
        "            # if c_amt <= 0:\n",
        "            #     raise ValueError(f\"Calculated samples per class ({c_amt}) is less than or equal to zero for feature '{uf}' with ratio {ratio}.\")\n",
        "            tmp = get_balanced_subset(df=tmp_df, class_col=class_col, feature_col=feature_col, feature_value=uf,\n",
        "                                        samples_per_class=c_amt, randomize=False)\n",
        "            if df_tmp is None:\n",
        "                df_tmp = tmp\n",
        "            else:\n",
        "                df_tmp = pd.concat([df_tmp, tmp])\n",
        "        return df_tmp\n",
        "\n",
        "    df_res = get_exp_data_inner(df_rnd, size)\n",
        "\n",
        "    if len(df_res) < size and SHOW_EXTRA_INFO:\n",
        "        print(f\"Samples for ({len(df_res)}) are less than requested ({size}).\")\n",
        "\n",
        "    ratios_fet = df_res[feature_col].value_counts(normalize=True).to_dict()\n",
        "    ratios_cls = df_res[class_col].value_counts(normalize=False).to_dict()\n",
        "\n",
        "    if SHOW_EXTRA_INFO:\n",
        "        print(f\"[] Ratios for {feature_col}: {ratios_fet}\")\n",
        "        print(f\"[] Ratios for {class_col}: {ratios_cls}\")\n",
        "        print()\n",
        "\n",
        "\n",
        "    for k in ratio:\n",
        "        if ratios_fet.get(k) is None:\n",
        "            if ratio[k] > 0.0:\n",
        "                raise ValueError(f\"Feature '{k}' not found in DataFrame after sampling (try increase 'size' parameter).\")\n",
        "        elif abs(ratios_fet[k] - ratio[k]) > max_diff:\n",
        "            raise ValueError(f\"Feature '{k}' ratio {ratios_fet[k]} differs from requested {ratio[k]} by more than {max_diff}.\")\n",
        "\n",
        "\n",
        "\n",
        "    df_res = df_res.reset_index(drop=True)\n",
        "\n",
        "    return df_res\n",
        "\n",
        "tmp_test = get_exp_data(\n",
        "    df=df, class_col='type', feature_col='ethnicity', ratio={'white':0.2, 'black':0.6, 'asian': 0.2}, size=10)\n",
        "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO4YLfxMlxZ8"
      },
      "source": [
        "# Methods for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LNCRWX5js2d7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficientnet\n",
        "from tensorflow.keras.applications.xception import preprocess_input as preprocess_xception\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input as preprocess_inception_v3\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as preprocess_efficientnetv2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_mobilenet_v2\n",
        "\n",
        "augmentation_std = tf.keras.Sequential([\n",
        "    #layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomZoom(0.05),\n",
        "    #layers.RandomContrast(0.05),\n",
        "    #layers.RandomBrightness(0.05),\n",
        "], name=\"augmentation_std\")\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def load_image(file_path, label, preprocess, augment):\n",
        "    image = tf.io.read_file(file_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "    if IMG_RESIZE:\n",
        "        image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    else:\n",
        "        image = tf.cast(image, tf.float32)\n",
        "\n",
        "    if preprocess:\n",
        "        image = preprocess(image)\n",
        "\n",
        "    if augment:\n",
        "        image = augment(image)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def get_data_for_model(df, class_col, files_col, batch_size, preprocess, augment):\n",
        "    image_paths = df[files_col].values\n",
        "    labels = df[class_col]\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "    dataset = dataset.map(lambda path, label: load_image(path, label, preprocess, augment), num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KR8_Qyf5lxZ8"
      },
      "outputs": [],
      "source": [
        "# Create folder if it doesn't exist\n",
        "if not os.path.exists(RES_PATH):\n",
        "    os.makedirs(RES_PATH)\n",
        "    print(f\"Created folder: {RES_PATH}\")\n",
        "elif REMOVE_RES_CONTENT:\n",
        "    # Remove all files inside the folder\n",
        "    for filename in os.listdir(RES_PATH):\n",
        "        file_path = os.path.join(RES_PATH, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.unlink(file_path)          # remove file or link\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)      # remove folder and contents\n",
        "        except Exception as e:\n",
        "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
        "    print(f\"Cleared contents of folder: {RES_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fYdm-HiMlxZ8"
      },
      "outputs": [],
      "source": [
        "def get_done_reps(model_name, feature_name, amt_per_rep):\n",
        "  results_path = f'{RES_PATH}res_{feature_name}_{model_name.replace(\" \", \"_\")}.csv'\n",
        "  if not os.path.exists(results_path):\n",
        "    return [], None\n",
        "\n",
        "  tmp_df = pd.read_csv(results_path)\n",
        "  dones = []\n",
        "  for r in tmp_df[\"rep\"].unique():\n",
        "    amt = len(tmp_df[tmp_df[\"rep\"]==r])\n",
        "    if amt == amt_per_rep:\n",
        "      dones.append(r)\n",
        "\n",
        "  return dones, tmp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hwAcU3dZlxZ8"
      },
      "outputs": [],
      "source": [
        "def compute_fairness_metrics(y_true, y_pred, group_labels):\n",
        "    \"\"\"Compute per-group rates and their gaps.\"\"\"\n",
        "    groups = pd.unique(group_labels)\n",
        "    tpr = {}\n",
        "    fpr = {}\n",
        "    pos_rate = {}\n",
        "\n",
        "    for g in groups:\n",
        "        mask = (group_labels == g)\n",
        "        yt = y_true[mask]\n",
        "        yp = y_pred[mask]\n",
        "        tn, fp, fn, tp = confusion_matrix(yt, yp, labels=[0,1]).ravel()\n",
        "        tpr[g] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        fpr[g] = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "        pos_rate[g] = (yp == 1).mean()\n",
        "\n",
        "    tpr_gap = max(tpr.values()) - min(tpr.values())\n",
        "    fpr_gap = max(fpr.values()) - min(fpr.values())\n",
        "    pos_rate_gap = max(pos_rate.values()) - min(pos_rate.values())\n",
        "\n",
        "    return {\n",
        "        \"tpr_gap\": tpr_gap,\n",
        "        \"fpr_gap\": fpr_gap,\n",
        "        \"pos_rate_gap\": pos_rate_gap,\n",
        "        \"per_group\": {\n",
        "            \"tpr\": tpr,\n",
        "            \"fpr\": fpr,\n",
        "            \"pos_rate\": pos_rate\n",
        "        }\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GA9sTR7elxZ8"
      },
      "outputs": [],
      "source": [
        "def perform_tests(df, train_metas, test_metas, validation_size, reps, feature_split_col, get_model, preprocess, class_col='type', exclude_column='path',\n",
        "                  files_col='path', data_augmentation=False, epochs_num=15, batch_size=64):\n",
        "    res = []\n",
        "\n",
        "    model_name = get_model.__name__.replace(\"create_\", \"\")\n",
        "\n",
        "    done_reps, prev_results = get_done_reps(model_name, feature_split_col, len(test_metas) * len(train_metas))\n",
        "\n",
        "    for r in range(reps):\n",
        "        if r in done_reps:\n",
        "          res.extend(prev_results[prev_results['rep']==r].values.tolist())\n",
        "          print(f\"Rep {r} already done for {model_name}. Skipping...\")\n",
        "          continue\n",
        "\n",
        "        np.random.seed(SEED + r)\n",
        "        tf.random.set_seed(SEED + r)\n",
        "\n",
        "        for train_meta in train_metas:\n",
        "            train_val = get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=train_meta['ratio'], size=train_meta['size'] + validation_size)\n",
        "            train_val = train_val.sample(frac=1, random_state=SEED+r).reset_index(drop=True)\n",
        "\n",
        "            stratify_key = train_val[class_col].astype(str) + \"_\" + train_val[feature_split_col].astype(str)\n",
        "\n",
        "            train, val = train_test_split(\n",
        "                train_val,\n",
        "                test_size=validation_size / (train_meta['size'] + validation_size),\n",
        "                stratify=stratify_key,\n",
        "                random_state=SEED + r\n",
        "            )\n",
        "\n",
        "            tests = [\n",
        "                get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=tm['ratio'], size=tm['size'], exclude_column=exclude_column, exclude_df=train) for tm in test_metas\n",
        "            ]\n",
        "\n",
        "            train_dataset = get_data_for_model(train, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=data_augmentation)\n",
        "            val_dataset = get_data_for_model(val, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=False)\n",
        "            test_datasets = [\n",
        "                get_data_for_model(test, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=False) for test in tests\n",
        "            ]\n",
        "\n",
        "            train_ratio = '/'.join([f\"{k}:{v}\" for k, v in train_meta['ratio'].items()])\n",
        "            train_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in train[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
        "            train_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', train_ratio)\n",
        "\n",
        "            model = get_model(input_shape=(IMG_SIZE,IMG_SIZE,3),\n",
        "                              train_dataset=train_dataset, val_dataset=val_dataset,\n",
        "                              epochs_num=epochs_num)\n",
        "\n",
        "            for test_dataset, test_meta, test_df in zip(test_datasets, test_metas, tests):\n",
        "                predictions = model.predict(test_dataset)\n",
        "                y_true = test_df[class_col]\n",
        "                y_pred = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "                tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "                accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "                fairness = compute_fairness_metrics(\n",
        "                    y_true=np.array(y_true),\n",
        "                    y_pred=np.array(y_pred),\n",
        "                    group_labels=test_df[feature_split_col].values\n",
        "                )\n",
        "                fairness_json = json.dumps(fairness[\"per_group\"])\n",
        "\n",
        "                test_ratio = '/'.join([f\"{k}:{v}\" for k, v in test_meta['ratio'].items()])\n",
        "                test_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in test_df[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
        "                test_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', test_ratio)\n",
        "\n",
        "                roc_df = pd.DataFrame({\n",
        "                    'y_true': y_true,\n",
        "                    'y_pred_proba': predictions.flatten()\n",
        "                })\n",
        "                path_roc = f'{RES_PATH}roc_{feature_split_col}_{model_name.replace(\" \", \"_\")}_rep{r}_{time.time_ns()}.csv'\n",
        "                roc_df.to_csv(path_roc, index=False)\n",
        "\n",
        "                res.append([\n",
        "                    r,\n",
        "                    model_name,\n",
        "                    feature_split_col,\n",
        "                    train_meta['size'],\n",
        "                    train_ratio,\n",
        "                    test_meta['size'],\n",
        "                    test_ratio,\n",
        "                    train_ratio_rel,\n",
        "                    test_ratio_rel,\n",
        "                    train_ratio_sim,\n",
        "                    test_ratio_sim,\n",
        "                    accuracy,\n",
        "                    tn, fp, fn, tp,\n",
        "                    fairness[\"tpr_gap\"],\n",
        "                    fairness[\"fpr_gap\"],\n",
        "                    fairness[\"pos_rate_gap\"],\n",
        "                    fairness_json,\n",
        "                    path_roc\n",
        "                ])\n",
        "\n",
        "                print(f\"Rep: {r:2} | Model: {model_name} | Feature Split: {feature_split_col} | Ratio: {test_ratio} | Acc: {accuracy:.2f}\")\n",
        "\n",
        "                res_df = pd.DataFrame(res, columns=[\n",
        "                    'rep', 'model_name', 'feature_split_col',\n",
        "                    'train_size', 'train_ratio_detail', 'test_size', 'test_ratio_detail',\n",
        "                    'train_ratio_rel', 'test_ratio_rel', \"train_ratio\", \"test_ratio\",\n",
        "                    'accuracy', 'TN', 'FP', 'FN', 'TP',\n",
        "                    'fairness_tpr_gap', 'fairness_fpr_gap', 'fairness_pos_rate_gap',\n",
        "                    'fairness_per_group_json',\n",
        "                    'path_roc'\n",
        "                ])\n",
        "\n",
        "                res_df.to_csv(f'{RES_PATH}res_{feature_split_col}_{model_name.replace(\" \", \"_\")}.csv', index=False)\n",
        "    print(f\"Done for {model_name}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeRfWCbzlxZ9"
      },
      "source": [
        "# Models implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cPE2ybiqlxZ9"
      },
      "outputs": [],
      "source": [
        "def create_xception(input_shape, train_dataset, val_dataset, epochs_num):\n",
        "    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    #x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"FITTING FULL XCEPTION\")\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
        "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "loq_AI6blxZ9"
      },
      "outputs": [],
      "source": [
        "def create_efficientnetb4(input_shape, train_dataset, val_dataset, epochs_num):\n",
        "    base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    #x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
        "\n",
        "    print(\"TRAINING FULL EfficientNetB4\")\n",
        "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "P-7kueQ4lxZ9"
      },
      "outputs": [],
      "source": [
        "def create_efficientnetv2(input_shape, train_dataset, val_dataset, epochs_num):\n",
        "    base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    #base_model = EfficientNetV2M(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    #x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
        "\n",
        "    print(\"TRAINING FULL EfficientNetV2S\")\n",
        "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "czXKH9trlxZ9"
      },
      "outputs": [],
      "source": [
        "def create_inceptionv3(input_shape, train_dataset, val_dataset, epochs_num):\n",
        "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"TRAINING FULL InceptionV3\")\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
        "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UqUe0lwMlxZ9"
      },
      "outputs": [],
      "source": [
        "def create_mobilenetv2(input_shape, train_dataset, val_dataset, epochs_num):\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"TRAINING FULL MobileNetV2\")\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
        "    model.fit(train_dataset,\n",
        "              validation_data=val_dataset,\n",
        "              epochs=epochs_num,\n",
        "              callbacks=[early_stopping])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Presets"
      ],
      "metadata": {
        "id": "SeAizc5zMaRr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX2IbWlylxZ9"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "perform_tests(df=df,\n",
        "             train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_xception,\n",
        "              preprocess=preprocess_xception,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n",
        "\n",
        "perform_tests(df=df,\n",
        "              train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_inceptionv3,\n",
        "              preprocess=preprocess_inception_v3,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n",
        "\n",
        "perform_tests(df=df,\n",
        "             train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_efficientnetv2,\n",
        "              preprocess=preprocess_efficientnetv2,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "perform_tests(df=df,\n",
        "              train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_efficientnetb4,\n",
        "              preprocess=preprocess_efficientnet,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n",
        "\n",
        "\n",
        "perform_tests(df=df,\n",
        "             train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_simple_cnn,\n",
        "              preprocess=preprocess_simple_cnn,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWZjB1d1lxZ9"
      },
      "source": [
        "# Tests starter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb_U5sSAlxZ9",
        "outputId": "7656bba0-27c2-442b-9989-5c1dcd744471"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rep 0 already done for efficientnetv2. Skipping...\n",
            "Rep 1 already done for efficientnetv2. Skipping...\n",
            "Done for efficientnetv2.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-33-1286027400.py:2: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING FULL MobileNetV2\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - accuracy: 0.4915 - loss: 0.9160 - val_accuracy: 0.5440 - val_loss: 0.8110\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 358ms/step\n",
            "Rep:  0 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.1/female:0.9 | Acc: 0.54\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 181ms/step\n",
            "Rep:  0 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.2/female:0.8 | Acc: 0.54\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 174ms/step\n",
            "Rep:  0 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.3/female:0.7 | Acc: 0.55\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 280ms/step\n",
            "Rep:  0 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.4/female:0.6 | Acc: 0.55\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 179ms/step\n",
            "Rep:  0 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.5/female:0.5 | Acc: 0.54\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 175ms/step\n",
            "Rep:  0 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.6/female:0.4 | Acc: 0.54\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step\n",
            "Rep:  0 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.7/female:0.3 | Acc: 0.53\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step\n",
            "Rep:  0 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.8/female:0.2 | Acc: 0.52\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 172ms/step\n",
            "Rep:  0 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.9/female:0.1 | Acc: 0.51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-33-1286027400.py:2: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING FULL MobileNetV2\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.4950 - loss: 0.9117 - val_accuracy: 0.4920 - val_loss: 0.9613\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 363ms/step\n",
            "Rep:  1 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.1/female:0.9 | Acc: 0.50\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 305ms/step\n",
            "Rep:  1 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.2/female:0.8 | Acc: 0.49\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 224ms/step\n",
            "Rep:  1 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.3/female:0.7 | Acc: 0.49\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 178ms/step\n",
            "Rep:  1 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.4/female:0.6 | Acc: 0.50\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
            "Rep:  1 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.5/female:0.5 | Acc: 0.49\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step\n",
            "Rep:  1 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.6/female:0.4 | Acc: 0.48\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
            "Rep:  1 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.7/female:0.3 | Acc: 0.49\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 281ms/step\n",
            "Rep:  1 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.8/female:0.2 | Acc: 0.48\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 222ms/step\n",
            "Rep:  1 | Model: mobilenetv2 | Feature Split: sex | Ratio: male:0.9/female:0.1 | Acc: 0.47\n",
            "Done for mobilenetv2.\n"
          ]
        }
      ],
      "source": [
        "perform_tests(df=df,\n",
        "             train_metas=[\n",
        "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.4, 'female':0.6}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.6, 'female':0.4}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.8, 'female':0.2}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_xception,\n",
        "              preprocess=preprocess_xception,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n",
        "\n",
        "perform_tests(df=df,\n",
        "              train_metas=[\n",
        "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.4, 'female':0.6}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.6, 'female':0.4}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.8, 'female':0.2}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_inceptionv3,\n",
        "              preprocess=preprocess_inception_v3,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n",
        "\n",
        "perform_tests(df=df,\n",
        "             train_metas=[\n",
        "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.4, 'female':0.6}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.6, 'female':0.4}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.8, 'female':0.2}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_efficientnetv2,\n",
        "              preprocess=preprocess_efficientnetv2,\n",
        "              data_augmentation=augmentation_std\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M0_AIpZlxZ9",
        "outputId": "337e9e5a-7e1b-421c-96ce-73a5e6b67f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL: efficientnetv2\n",
            "+----+--------------+-------------+------------+---------------+\n",
            "|    | TrainRatio   | TestRatio   |   Accuracy |   AccuracySTD |\n",
            "|----+--------------+-------------+------------+---------------|\n",
            "|  8 | 2/8          | 9/1         |      0.893 |         0.018 |\n",
            "|  7 | 2/8          | 8/2         |      0.899 |         0.016 |\n",
            "|  6 | 2/8          | 7/3         |      0.908 |         0.014 |\n",
            "|  5 | 2/8          | 6/4         |      0.916 |         0.014 |\n",
            "|  4 | 2/8          | 5/5         |      0.931 |         0.013 |\n",
            "|  3 | 2/8          | 4/6         |      0.931 |         0.01  |\n",
            "|  2 | 2/8          | 3/7         |      0.939 |         0.01  |\n",
            "|  1 | 2/8          | 2/8         |      0.95  |         0.008 |\n",
            "|  0 | 2/8          | 1/9         |      0.953 |         0.001 |\n",
            "+----+--------------+-------------+------------+---------------+\n",
            "\n",
            "\n",
            "File /content/drive/MyDrive/Studia/MAGISTER/PracaMagisterska/res/res_sex_xception.csv does not exists!\n",
            "File /content/drive/MyDrive/Studia/MAGISTER/PracaMagisterska/res/res_sex_inceptionv3.csv does not exists!\n"
          ]
        }
      ],
      "source": [
        "def print_summarise_res(model_name:str):\n",
        "  path = RES_PATH + f'res_sex_{model_name}.csv'\n",
        "\n",
        "  if not os.path.exists(path):\n",
        "    print(f\"File {path} does not exists!\")\n",
        "    return\n",
        "\n",
        "  res = pd.read_csv(path)\n",
        "  gr = res.groupby(['train_ratio', 'test_ratio']).agg(\n",
        "      # Model=('model_name', 'first'),\n",
        "      TrainRatio=('train_ratio', 'first'),\n",
        "      TestRatio=('test_ratio', 'first'),\n",
        "      Accuracy= ('accuracy', 'mean'),\n",
        "      AccuracySTD= ('accuracy', 'std'),\n",
        "  ).reset_index(drop=True)\n",
        "\n",
        "  gr = gr.round(3).sort_values(by=['TrainRatio', 'TestRatio'], ascending=False)\n",
        "\n",
        "  print(\"MODEL: \" + model_name)\n",
        "  print(tb.tabulate(gr, headers='keys', tablefmt='psql'))\n",
        "  print()\n",
        "  print()\n",
        "\n",
        "print_summarise_res('efficientnetv2')\n",
        "print_summarise_res('xception')\n",
        "print_summarise_res('inceptionv3')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}