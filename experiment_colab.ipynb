{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CenturionTheMan/ConceptDriftResearch/blob/main/concept_drift_exp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnW2ICuewM0-"
      },
      "outputs": [],
      "source": [
        "REMOVE_RES_CONTENT = True\n",
        "# IMG_SIZE=299\n",
        "IMG_SIZE=224\n",
        "DATA_ZIP_PATH=f'/content/drive/MyDrive/Studia/MAGISTER/PracaMagisterska/data{IMG_SIZE}.zip'\n",
        "IMG_RESIZE = False\n",
        "SEED = 42\n",
        "RES_PATH = '/content/drive/MyDrive/Studia/MAGISTER/PracaMagisterska/res/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFFQKC56qsb-",
        "outputId": "8b4215a9-b1ef-4961-88c3-064bf0822d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Pliki już są rozpakowane — pomijam kopiowanie i rozpakowywanie.\n",
            "total 890M\n",
            "drwxrwxr-x 4 root root 4.0K Jul 16 20:22 data\n",
            "-rw------- 1 root root 890M Jul 17 15:42 data.zip\n",
            "drwx------ 6 root root 4.0K Jul 17 15:26 drive\n",
            "drwxr-xr-x 1 root root 4.0K Jul 15 13:41 sample_data\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Montuj Google Drive (tylko raz)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ścieżki\n",
        "zip_path = '/content/data.zip'\n",
        "dataset_dir = '/content/'\n",
        "zip_on_drive = DATA_ZIP_PATH\n",
        "\n",
        "# Sprawdź, czy katalog z danymi już istnieje\n",
        "if not os.path.exists(dataset_dir+'data') or len(os.listdir(dataset_dir)) == 0:\n",
        "    print(\"Pliki jeszcze nie wypakowane — kopiuję i rozpakowuję...\")\n",
        "    # Skopiuj zip z Drive do RAM\n",
        "    if not os.path.exists(zip_path):\n",
        "        !cp \"$zip_on_drive\" \"$zip_path\"\n",
        "    # Rozpakuj zip\n",
        "    !unzip -q \"$zip_path\" -d \"$dataset_dir\"\n",
        "else:\n",
        "    print(\"Pliki już są rozpakowane — pomijam kopiowanie i rozpakowywanie.\")\n",
        "\n",
        "# Podejrzyj zawartość\n",
        "!ls -lh \"$dataset_dir\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPKtaDEKrVlM",
        "outputId": "f35d774d-1a32-46a0-fbb9-1b5dd41d79ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU NIE jest dostępne, sprawdź ustawienia środowiska\n"
          ]
        }
      ],
      "source": [
        "!pip install -q fairlearn tabulate\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate as tb\n",
        "from typing import Dict\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
        "from tensorflow.keras.applications import Xception, EfficientNetB0, ResNet50\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Informacja o dostępności GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name:\n",
        "    print(f\"GPU jest dostępne: {device_name}\")\n",
        "else:\n",
        "    print(\"GPU NIE jest dostępne, sprawdź ustawienia środowiska\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQHjq4DXrY3h",
        "outputId": "ee466649-50e5-43a4-c318-b6aedc03dc0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+--------+--------+-------------+--------------+----------+--------+-----------------------------------------+\n",
            "|    | type   | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                                    |\n",
            "|----+--------+--------+-------------+--------------+----------+--------+-----------------------------------------|\n",
            "|  0 | real   | male   |             | yes          | no       | big    | /content/data/original/805/frame271.jpg |\n",
            "|  1 | real   | female | white       | no           |          | big    | /content/data/original/083/frame191.jpg |\n",
            "|  2 | real   | male   | white       | no           | no       | small  | /content/data/original/878/frame111.jpg |\n",
            "|  3 | real   | female | white       | no           |          |        | /content/data/original/158/frame201.jpg |\n",
            "|  4 | real   | female | white       | no           |          |        | /content/data/original/606/frame71.jpg  |\n",
            "+----+--------+--------+-------------+--------------+----------+--------+-----------------------------------------+\n",
            "Dataset size: 59552\n"
          ]
        }
      ],
      "source": [
        "file_path = '/content/data/metadata.csv'\n",
        "df_tmp = pd.read_csv(file_path, sep=',')\n",
        "df_tmp['path'] = '/content/data/' + df_tmp['path']\n",
        "\n",
        "df_tmp = df_tmp[df_tmp['deepfake'] != 0]\n",
        "\n",
        "df_tmp['ethnicity'] = df_tmp.apply(\n",
        "    lambda row: 'white' if row['white'] == 1 else ('black' if row['black'] == 1 else (\n",
        "        'asian' if row['asian'] == 1 else None)), axis=1)\n",
        "\n",
        "df = df_tmp[['deepfake', 'male', 'ethnicity', 'eyeglasses', 'heavy_makeup', 'big_lips', 'path']]\n",
        "\n",
        "df = df.rename(columns={'deepfake': 'type', 'male': 'sex', 'heavy_makeup': 'makeup', 'big_lips': 'lips',})\n",
        "\n",
        "df['type'] = df['type'].replace({1: 'fake', -1: 'real'})\n",
        "df['sex'] = df['sex'].replace({-1: 'female', 0: None, 1: 'male'})\n",
        "df['makeup'] = df['makeup'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
        "df['lips'] = df['lips'].replace({-1: 'small', 0: None, 1: 'big'})\n",
        "df['eyeglasses'] = df['eyeglasses'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
        "\n",
        "\n",
        "print(tb.tabulate(df.head(), headers='keys', tablefmt='psql'))\n",
        "print(f\"Dataset size: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqJXku-tsxK7",
        "outputId": "03c62d12-4bff-4e41-cd9b-7d694cc52732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+--------+-------+-------------+--------------+----------+--------+---------------------------------------------+\n",
            "|    | type   | sex   | ethnicity   | eyeglasses   | makeup   | lips   | path                                        |\n",
            "|----+--------+-------+-------------+--------------+----------+--------+---------------------------------------------|\n",
            "|  0 | fake   | male  | white       | no           | no       | small  | /content/data/deepfake/374_407/frame41.jpg  |\n",
            "|  1 | fake   | male  | white       | no           | no       | small  | /content/data/deepfake/015_919/frame281.jpg |\n",
            "|  2 | real   | male  |             | no           | no       | big    | /content/data/original/995/frame11.jpg      |\n",
            "|  3 | real   | male  | white       | no           | no       |        | /content/data/original/579/frame201.jpg     |\n",
            "+----+--------+-------+-------------+--------------+----------+--------+---------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "def get_balanced_subset(\n",
        "    df, class_col, feature_col, feature_value,\n",
        "    samples_per_class, randomize=True, reset_index=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Select a balanced subset of the data for a given feature value, with equal number of samples per class.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame\n",
        "        class_col: column name of class labels\n",
        "        feature_col: column name of feature\n",
        "        feature_value: specific feature value to filter\n",
        "        samples_per_class: number of samples per class\n",
        "        randomize: whether to shuffle within class before selecting\n",
        "        reset_index: whether to reset index of returned DataFrame\n",
        "        seed: random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Balanced DataFrame subset\n",
        "    \"\"\"\n",
        "    tmp = df[df[feature_col] == feature_value]\n",
        "\n",
        "    counts = tmp[class_col].value_counts()\n",
        "    for cl, count in counts.items():\n",
        "        if count < samples_per_class:\n",
        "            raise ValueError(f\"Not enough samples for class '{cl}' in feature '{feature_value}'. \"\n",
        "                             f\"Required: {samples_per_class}, Available: {count}\")\n",
        "\n",
        "    tmp = pd.concat([\n",
        "        (g.sample(frac=1, random_state=SEED).head(samples_per_class) if randomize else g.head(samples_per_class))\n",
        "        for _, g in tmp.groupby(class_col)\n",
        "    ])\n",
        "\n",
        "    if reset_index:\n",
        "        tmp = tmp.reset_index(drop=True)\n",
        "\n",
        "    return tmp\n",
        "\n",
        "tmp_test = get_balanced_subset(\n",
        "    df=df, class_col='type', feature_col='sex', feature_value='male',\n",
        "    samples_per_class=2, randomize=True, reset_index=True)\n",
        "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJZmhWnis0yh",
        "outputId": "555bdf23-ce59-42e2-da9c-dffb8e307822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature 'None' not found in ratios. Skipping.\n",
            "[] Ratios for ethnicity: {'black': 0.6, 'white': 0.2, 'asian': 0.2}\n",
            "[] Ratios for type: {'fake': 5, 'real': 5}\n",
            "\n",
            "+----+--------+--------+-------------+--------------+----------+--------+---------------------------------------------+\n",
            "|    | type   | sex    | ethnicity   | eyeglasses   | makeup   | lips   | path                                        |\n",
            "|----+--------+--------+-------------+--------------+----------+--------+---------------------------------------------|\n",
            "|  0 | fake   | male   | white       | no           | no       | small  | /content/data/deepfake/594_530/frame121.jpg |\n",
            "|  1 | real   | female | white       | no           |          |        | /content/data/original/240/frame41.jpg      |\n",
            "|  2 | fake   | female | asian       |              |          | big    | /content/data/deepfake/249_280/frame261.jpg |\n",
            "|  3 | real   | female | asian       | no           |          | big    | /content/data/original/758/frame161.jpg     |\n",
            "|  4 | fake   | female | black       | no           |          | big    | /content/data/deepfake/986_994/frame271.jpg |\n",
            "|  5 | fake   | male   | black       | no           | no       | big    | /content/data/deepfake/144_122/frame101.jpg |\n",
            "|  6 | fake   | male   | black       | yes          | no       | big    | /content/data/deepfake/081_087/frame41.jpg  |\n",
            "|  7 | real   | male   | black       | no           | no       | big    | /content/data/original/715/frame231.jpg     |\n",
            "|  8 | real   | female | black       | no           |          | big    | /content/data/original/762/frame61.jpg      |\n",
            "|  9 | real   | female | black       | no           |          | big    | /content/data/original/328/frame241.jpg     |\n",
            "+----+--------+--------+-------------+--------------+----------+--------+---------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "def get_exp_data(df, class_col, feature_col, ratio : Dict, size, randomize=True, exclude_column=None, exclude_df=None, max_diff=0.05):\n",
        "    '''\n",
        "    Get a balanced subset of the data based on specified ratios for features.\n",
        "    Args:\n",
        "        df: DataFrame containing the data\n",
        "        class_col: column name for class labels\n",
        "        feature_col: column name for features\n",
        "        ratio: dictionary with feature values as keys and their ratios as values\n",
        "        size: total number of samples to return\n",
        "        randomize: whether to shuffle the DataFrame before processing\n",
        "        exclude_column: column name to exclude from the DataFrame\n",
        "        exclude_df: DataFrame containing values to exclude based on exclude_column\n",
        "    '''\n",
        "    if randomize:\n",
        "        df_rnd = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "    else:\n",
        "        df_rnd = df.copy()\n",
        "\n",
        "    if exclude_column is not None and exclude_df is not None:\n",
        "        if exclude_column not in df_rnd.columns:\n",
        "            raise ValueError(f\"Column '{exclude_column}' not found in DataFrame.\")\n",
        "        if exclude_column not in exclude_df.columns:\n",
        "            raise ValueError(f\"Column '{exclude_column}' not found in exclude DataFrame.\")\n",
        "        df_rnd = df_rnd[~df_rnd[exclude_column].isin(exclude_df[exclude_column])]\n",
        "\n",
        "    uniq_classes = df_rnd[class_col].unique()\n",
        "    uniq_features = df_rnd[feature_col].unique()\n",
        "\n",
        "    def get_exp_data_inner(tmp_df, size):\n",
        "        df_tmp = None\n",
        "        for uf in uniq_features:\n",
        "            if ratio.get(uf) is None:\n",
        "                print(f\"Feature '{uf}' not found in ratios. Skipping.\")\n",
        "                continue\n",
        "            c_amt = int(size * ratio[uf] / len(uniq_classes))\n",
        "            # if c_amt <= 0:\n",
        "            #     raise ValueError(f\"Calculated samples per class ({c_amt}) is less than or equal to zero for feature '{uf}' with ratio {ratio}.\")\n",
        "            tmp = get_balanced_subset(df=tmp_df, class_col=class_col, feature_col=feature_col, feature_value=uf,\n",
        "                                        samples_per_class=c_amt, randomize=False)\n",
        "            if df_tmp is None:\n",
        "                df_tmp = tmp\n",
        "            else:\n",
        "                df_tmp = pd.concat([df_tmp, tmp])\n",
        "        return df_tmp\n",
        "\n",
        "    df_res = get_exp_data_inner(df_rnd, size)\n",
        "\n",
        "    if len(df_res) < size:\n",
        "        print(f\"Samples for ({len(df_res)}) are less than requested ({size}).\")\n",
        "\n",
        "    ratios_fet = df_res[feature_col].value_counts(normalize=True).to_dict()\n",
        "    ratios_cls = df_res[class_col].value_counts(normalize=False).to_dict()\n",
        "    print(f\"[] Ratios for {feature_col}: {ratios_fet}\")\n",
        "    print(f\"[] Ratios for {class_col}: {ratios_cls}\")\n",
        "\n",
        "    for k in ratio:\n",
        "        if ratios_fet.get(k) is None:\n",
        "            if ratio[k] > 0.0:\n",
        "                raise ValueError(f\"Feature '{k}' not found in DataFrame after sampling (try increase 'size' parameter).\")\n",
        "        elif abs(ratios_fet[k] - ratio[k]) > max_diff:\n",
        "            raise ValueError(f\"Feature '{k}' ratio {ratios_fet[k]} differs from requested {ratio[k]} by more than {max_diff}.\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    df_res = df_res.reset_index(drop=True)\n",
        "\n",
        "    return df_res\n",
        "\n",
        "tmp_test = get_exp_data(\n",
        "    df=df, class_col='type', feature_col='ethnicity', ratio={'white':0.2, 'black':0.6, 'asian': 0.2}, size=10, randomize=True)\n",
        "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNCRWX5js2d7"
      },
      "outputs": [],
      "source": [
        "def load_image(file_path):\n",
        "    image = tf.io.read_file(file_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    if IMG_RESIZE:\n",
        "      image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "def get_data_for_model(df, class_col, files_col, batch_size):\n",
        "    image_paths = df[files_col].values\n",
        "    labels = df[class_col].values\n",
        "    labels = df[class_col].astype('category').cat.codes.values #classes strs to ints\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "    dataset = dataset.map(lambda path, label: (load_image(path), label))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cfHfqsPvi3O"
      },
      "outputs": [],
      "source": [
        "def create_mobile_net2(num_classes, input_shape, model=None):\n",
        "  if model is None:\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "  else:\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = True\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-5),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "  return model, \"mobile net 2\"\n",
        "\n",
        "def create_resnet50(num_classes, input_shape, model=None):\n",
        "  if model is None:\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "  else:\n",
        "    for layer in model.layers:\n",
        "      layer.trainable = True\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-5),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "  return model, \"resnet50\"\n",
        "\n",
        "def create_efficientnet_b0(num_classes, input_shape, model=None):\n",
        "  if model is None:\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "  else:\n",
        "    for layer in model.layers:\n",
        "      layer.trainable = True\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-5),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "  return model, \"efficientnet b0\"\n",
        "\n",
        "\n",
        "def create_xception(num_classes, input_shape, model=None):\n",
        "  if model is None:\n",
        "    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "  else:\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = True\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-5),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "  return model, \"xception\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYJbzSoMv9Vd",
        "outputId": "07789495-ad79-42f3-eacb-3cc9507b3425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleared contents of folder: /content/drive/MyDrive/Studia/MAGISTER/PracaMagisterska/res/\n"
          ]
        }
      ],
      "source": [
        "# Create folder if it doesn't exist\n",
        "if not os.path.exists(RES_PATH):\n",
        "    os.makedirs(RES_PATH)\n",
        "    print(f\"Created folder: {RES_PATH}\")\n",
        "elif REMOVE_RES_CONTENT:\n",
        "    # Remove all files inside the folder\n",
        "    for filename in os.listdir(RES_PATH):\n",
        "        file_path = os.path.join(RES_PATH, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.unlink(file_path)          # remove file or link\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)      # remove folder and contents\n",
        "        except Exception as e:\n",
        "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
        "    print(f\"Cleared contents of folder: {RES_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ_JQedB1623"
      },
      "outputs": [],
      "source": [
        "def get_done_reps(model_name, feature_name, amt_per_rep):\n",
        "  results_path = f'{RES_PATH}res_{feature_name}_{model_name.replace(\" \", \"_\")}.csv'\n",
        "  if not os.path.exists(results_path):\n",
        "    return [], None\n",
        "\n",
        "  tmp_df = pd.read_csv(results_path)\n",
        "  dones = []\n",
        "  for r in tmp_df[\"rep\"].unique():\n",
        "    amt = len(tmp_df[tmp_df[\"rep\"]==r])\n",
        "    if amt == amt_per_rep:\n",
        "      dones.append(r)\n",
        "\n",
        "  return dones, tmp_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ik6Z-AUis6G7"
      },
      "outputs": [],
      "source": [
        "def perform_tests(df, train_metas, test_metas, validation_size, reps, class_col, feature_split_col, exclude_column, files_col, get_model, epochs_num_head, epochs_num_whole, batch_size):\n",
        "    res = []\n",
        "\n",
        "    _, model_name = get_model(num_classes=len(df[class_col].unique()), input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
        "\n",
        "    done_reps, prev_results = get_done_reps(model_name, feature_split_col, len(test_metas) * len(train_metas))\n",
        "\n",
        "    for r in range(reps):\n",
        "        if r in done_reps:\n",
        "          res.extend(prev_results[prev_results['rep']==r].values.tolist())\n",
        "          print(f\"Rep {r} already done for {model_name}. Skipping...\")\n",
        "          continue\n",
        "\n",
        "        np.random.seed(SEED + r)\n",
        "        tf.random.set_seed(SEED + r)\n",
        "\n",
        "        for train_meta in train_metas:\n",
        "            train_val = get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=train_meta['ratio'], size=train_meta['size'] + validation_size)\n",
        "            train_val = train_val.sample(frac=1, random_state=SEED+r).reset_index(drop=True)\n",
        "\n",
        "            stratify_key = train_val[class_col].astype(str) + \"_\" + train_val[feature_split_col].astype(str)\n",
        "\n",
        "            train, val = train_test_split(\n",
        "                train_val,\n",
        "                test_size=validation_size / (train_meta['size'] + validation_size),\n",
        "                stratify=stratify_key,\n",
        "                random_state=SEED + r\n",
        "            )\n",
        "\n",
        "            train = train.reset_index(drop=True)\n",
        "            val = val.reset_index(drop=True)\n",
        "\n",
        "            tests = [\n",
        "                get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=tm['ratio'], size=tm['size'], exclude_column=exclude_column, exclude_df=train) for tm in test_metas\n",
        "            ]\n",
        "\n",
        "            train_dataset = get_data_for_model(train, class_col=class_col, files_col=files_col, batch_size=batch_size)\n",
        "            val_dataset = get_data_for_model(val, class_col=class_col, files_col=files_col, batch_size=batch_size)\n",
        "            test_datasets = [\n",
        "                get_data_for_model(test, class_col=class_col, files_col=files_col, batch_size=batch_size) for test in tests\n",
        "            ]\n",
        "\n",
        "            train_ratio = '/'.join([f\"{k}:{v}\" for k, v in train_meta['ratio'].items()])\n",
        "            train_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in train[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
        "            train_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', train_ratio)\n",
        "\n",
        "            print(\"FITTING HEAD\")\n",
        "            model, model_name = get_model(num_classes=len(df[class_col].unique()), input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
        "            early_stopping_head = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
        "            model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num_head, callbacks=[early_stopping_head])\n",
        "\n",
        "            print(\"FITTING WHOLE\")\n",
        "            model, model_name = get_model(num_classes=len(df[class_col].unique()), input_shape=(IMG_SIZE,IMG_SIZE,3), model=model)\n",
        "            early_stopping_whole = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
        "            model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num_whole, callbacks=[early_stopping_whole])\n",
        "\n",
        "            for test_dataset, test_meta, test_df in zip(test_datasets, test_metas, tests):\n",
        "                predictions = model.predict(test_dataset)\n",
        "                y_true = test_df[class_col].astype('category').cat.codes.values\n",
        "                y_pred = np.argmax(predictions, axis=1)\n",
        "                acc = accuracy_score(y_true, y_pred)\n",
        "                f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "                eo_diff = equalized_odds_difference(y_true, y_pred, sensitive_features=test_df[feature_split_col])\n",
        "\n",
        "                test_ratio = '/'.join([f\"{k}:{v}\" for k, v in test_meta['ratio'].items()])\n",
        "                test_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in test_df[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
        "                test_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', test_ratio)\n",
        "\n",
        "                res.append([\n",
        "                    r,\n",
        "                    model_name,\n",
        "                    feature_split_col,\n",
        "                    train_meta['size'],\n",
        "                    train_ratio,\n",
        "                    test_meta['size'],\n",
        "                    test_ratio,\n",
        "                    acc,\n",
        "                    f1,\n",
        "                    eo_diff,\n",
        "                    train_ratio_rel,\n",
        "                    test_ratio_rel,\n",
        "                    train_ratio_sim,\n",
        "                    test_ratio_sim\n",
        "                ])\n",
        "\n",
        "                print(f\"Rep: {r:2} | Model: {model_name} | Feature Split: {feature_split_col} | Ratio: {test_ratio} | Acc: {acc:.2f}\")\n",
        "\n",
        "                res_df = pd.DataFrame(res, columns=[\n",
        "                    'rep', 'model_name', 'feature_split_col',\n",
        "                    'train_size', 'train_ratio_detail', 'test_size', 'test_ratio_detail',\n",
        "                    'accuracy', 'f1_score', 'eo_diff', 'train_ratio_rel', 'test_ratio_rel', \"train_ratio\", \"test_ratio\"\n",
        "                ])\n",
        "\n",
        "                res_df.to_csv(f'{RES_PATH}res_{feature_split_col}_{model_name.replace(\" \", \"_\")}.csv', index=False)\n",
        "    print(f\"Done for {model_name}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM2K_awwwv2m",
        "outputId": "65cbb83b-b777-4386-d68f-0cd46a28dc09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Feature 'None' not found in ratios. Skipping.\n",
            "[] Ratios for sex: {'female': 0.9, 'male': 0.1}\n",
            "[] Ratios for type: {'fake': 2500, 'real': 2500}\n",
            "\n",
            "Feature 'None' not found in ratios. Skipping.\n",
            "[] Ratios for sex: {'female': 0.9, 'male': 0.1}\n",
            "[] Ratios for type: {'fake': 250, 'real': 250}\n",
            "\n",
            "Feature 'None' not found in ratios. Skipping.\n",
            "[] Ratios for sex: {'female': 0.7, 'male': 0.3}\n",
            "[] Ratios for type: {'fake': 250, 'real': 250}\n",
            "\n",
            "Feature 'None' not found in ratios. Skipping.\n",
            "[] Ratios for sex: {'male': 0.5, 'female': 0.5}\n",
            "[] Ratios for type: {'fake': 250, 'real': 250}\n",
            "\n",
            "Feature 'None' not found in ratios. Skipping.\n",
            "[] Ratios for sex: {'male': 0.7, 'female': 0.3}\n",
            "[] Ratios for type: {'fake': 250, 'real': 250}\n",
            "\n",
            "Feature 'None' not found in ratios. Skipping.\n",
            "[] Ratios for sex: {'male': 0.9, 'female': 0.1}\n",
            "[] Ratios for type: {'fake': 250, 'real': 250}\n",
            "\n",
            "FITTING HEAD\n",
            "Epoch 1/8\n",
            "\u001b[1m 1/79\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m58:41\u001b[0m 45s/step - accuracy: 0.4375 - loss: 1.4117"
          ]
        }
      ],
      "source": [
        "perform_tests(df=df,\n",
        "              train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              class_col='type',\n",
        "              feature_split_col='sex',\n",
        "              exclude_column='path',\n",
        "              files_col='path',\n",
        "              get_model=create_xception,\n",
        "              epochs_num_head=15,\n",
        "              epochs_num_whole=10,\n",
        "              batch_size=64\n",
        "              )\n",
        "\n",
        "\n",
        "perform_tests(df=df,\n",
        "              train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              class_col='type',\n",
        "              feature_split_col='sex',\n",
        "              exclude_column='path',\n",
        "              files_col='path',\n",
        "              get_model=create_efficientnet_b0,\n",
        "              epochs_num_head=15,\n",
        "              epochs_num_whole=10,\n",
        "              batch_size=64\n",
        "              )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivm4W_DLs7jK"
      },
      "outputs": [],
      "source": [
        "def print_summarise_res(model_name:str):\n",
        "  path = RES_PATH + f'res_sex_{model_name}.csv'\n",
        "\n",
        "  if not os.path.exists(path):\n",
        "    print(f\"File {path} does not exists!\")\n",
        "    return\n",
        "\n",
        "  res = pd.read_csv(path)\n",
        "  gr = res.groupby(['train_ratio', 'test_ratio']).agg(\n",
        "      # Model=('model_name', 'first'),\n",
        "      TrainRatio=('train_ratio', 'first'),\n",
        "      TestRatio=('test_ratio', 'first'),\n",
        "      Accuracy= ('accuracy', 'mean'),\n",
        "      AccuracySTD= ('accuracy', 'std'),\n",
        "      F1=('f1_score', 'mean'),\n",
        "      F1STD=('f1_score', 'std'),\n",
        "      EODiff=('eo_diff', 'mean'),\n",
        "      EODiffSTD=('eo_diff', 'std'),\n",
        "  ).reset_index(drop=True)\n",
        "\n",
        "  gr = gr.round(3).sort_values(by=['TrainRatio', 'TestRatio'], ascending=False)\n",
        "\n",
        "  print(\"MODEL: \" + model_name)\n",
        "  print(tb.tabulate(gr, headers='keys', tablefmt='psql'))\n",
        "  print()\n",
        "  print()\n",
        "\n",
        "print_summarise_res('resnet50')\n",
        "print_summarise_res('efficientnet_b0')\n",
        "print_summarise_res('xception')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNmMsy80ByKTqTKo1AGnh2X",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
