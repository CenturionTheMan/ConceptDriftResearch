{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CenturionTheMan/ConceptDriftResearch/blob/main/experiment_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shghl8SNlxZ3"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnW2ICuewM0-"
      },
      "outputs": [],
      "source": [
        "REMOVE_RES_CONTENT = True\n",
        "IMG_SIZE=224\n",
        "#IMG_SIZE=64\n",
        "IMG_RESIZE=False\n",
        "SEED = 42\n",
        "DATA_ZIP_PATH=f'/content/drive/MyDrive/Studia/MAGISTER/PracaMagisterska/data224.zip'\n",
        "RES_PATH = '/content/drive/MyDrive/Studia/MAGISTER/PracaMagisterska/res/'\n",
        "SHOW_EXTRA_INFO=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFFQKC56qsb-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Montuj Google Drive (tylko raz)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ścieżki\n",
        "zip_path = '/content/data.zip'\n",
        "dataset_dir = '/content/'\n",
        "zip_on_drive = DATA_ZIP_PATH\n",
        "\n",
        "# Sprawdź, czy katalog z danymi już istnieje\n",
        "if not os.path.exists(dataset_dir+'data') or len(os.listdir(dataset_dir)) == 0:\n",
        "    print(\"Pliki jeszcze nie wypakowane — kopiuję i rozpakowuję...\")\n",
        "    # Skopiuj zip z Drive do RAM\n",
        "    if not os.path.exists(zip_path):\n",
        "        !cp \"$zip_on_drive\" \"$zip_path\"\n",
        "    # Rozpakuj zip\n",
        "    !unzip -q \"$zip_path\" -d \"$dataset_dir\"\n",
        "else:\n",
        "    print(\"Pliki już są rozpakowane — pomijam kopiowanie i rozpakowywanie.\")\n",
        "\n",
        "# Podejrzyj zawartość\n",
        "!ls -lh \"$dataset_dir\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPKtaDEKrVlM"
      },
      "outputs": [],
      "source": [
        "!pip install -q tabulate\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tabulate as tb\n",
        "from typing import Dict\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import re\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Input, BatchNormalization, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tensorflow.keras.applications import Xception, EfficientNetB4, InceptionV3, EfficientNetV2M,EfficientNetV2S\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, BatchNormalization, LeakyReLU,\n",
        "    MaxPooling2D, Flatten, Dense, Dropout, concatenate\n",
        ")\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score, log_loss, brier_score_loss\n",
        "import time\n",
        "\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name:\n",
        "    print(f\"GPU available: {device_name}\")\n",
        "else:\n",
        "    print(\"No GPU available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSoODNE0lxZ7"
      },
      "source": [
        "# Dataset splitting - handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQHjq4DXrY3h"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/data/metadata.csv'\n",
        "df_tmp = pd.read_csv(file_path, sep=',')\n",
        "df_tmp['path'] = '/content/data/' + df_tmp['path']\n",
        "\n",
        "df_tmp = df_tmp[df_tmp['deepfake'] != 0]\n",
        "\n",
        "df_tmp['ethnicity'] = df_tmp.apply(\n",
        "    lambda row: 'white' if row['white'] == 1 else ('black' if row['black'] == 1 else (\n",
        "        'asian' if row['asian'] == 1 else None)), axis=1)\n",
        "\n",
        "df = df_tmp[['deepfake', 'male', 'ethnicity', 'eyeglasses', 'heavy_makeup', 'big_lips', 'path']]\n",
        "\n",
        "df = df.rename(columns={'deepfake': 'type', 'male': 'sex', 'heavy_makeup': 'makeup', 'big_lips': 'lips',})\n",
        "\n",
        "df['typeName'] = df['type'].replace({1: 'fake', -1: 'real'})\n",
        "df['type'] = df['typeName'].map({'fake': 1, 'real': 0}).values\n",
        "df['sex'] = df['sex'].replace({-1: 'female', 0: None, 1: 'male'})\n",
        "df['makeup'] = df['makeup'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
        "df['lips'] = df['lips'].replace({-1: 'small', 0: None, 1: 'big'})\n",
        "df['eyeglasses'] = df['eyeglasses'].replace({-1: 'no', 0: None, 1: 'yes'})\n",
        "\n",
        "\n",
        "print(tb.tabulate(df.head(), headers='keys', tablefmt='psql'))\n",
        "print(f\"Dataset size: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqJXku-tsxK7"
      },
      "outputs": [],
      "source": [
        "def get_balanced_subset(\n",
        "    df, class_col, feature_col, feature_value,\n",
        "    samples_per_class, randomize=True, reset_index=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Select a balanced subset of the data for a given feature value, with equal number of samples per class.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame\n",
        "        class_col: column name of class labels\n",
        "        feature_col: column name of feature\n",
        "        feature_value: specific feature value to filter\n",
        "        samples_per_class: number of samples per class\n",
        "        randomize: whether to shuffle within class before selecting\n",
        "        reset_index: whether to reset index of returned DataFrame\n",
        "        seed: random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Balanced DataFrame subset\n",
        "    \"\"\"\n",
        "    tmp = df[df[feature_col] == feature_value]\n",
        "\n",
        "    counts = tmp[class_col].value_counts()\n",
        "    for cl, count in counts.items():\n",
        "        if count < samples_per_class:\n",
        "            raise ValueError(f\"Not enough samples for class '{cl}' in feature '{feature_value}'. \"\n",
        "                             f\"Required: {samples_per_class}, Available: {count}\")\n",
        "\n",
        "    tmp = pd.concat([\n",
        "        (g.sample(frac=1, random_state=SEED).head(samples_per_class) if randomize else g.head(samples_per_class))\n",
        "        for _, g in tmp.groupby(class_col)\n",
        "    ])\n",
        "\n",
        "    if reset_index:\n",
        "        tmp = tmp.reset_index(drop=True)\n",
        "\n",
        "    return tmp\n",
        "\n",
        "tmp_test = get_balanced_subset(\n",
        "    df=df, class_col='type', feature_col='sex', feature_value='male',\n",
        "    samples_per_class=2, randomize=True, reset_index=True)\n",
        "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJZmhWnis0yh"
      },
      "outputs": [],
      "source": [
        "def get_exp_data(df, class_col, feature_col, ratio : Dict, size, randomize=True, exclude_column=None, exclude_df=None, max_diff=0.05):\n",
        "    '''\n",
        "    Get a balanced subset of the data based on specified ratios for features.\n",
        "    Args:\n",
        "        df: DataFrame containing the data\n",
        "        class_col: column name for class labels\n",
        "        feature_col: column name for features\n",
        "        ratio: dictionary with feature values as keys and their ratios as values\n",
        "        size: total number of samples to return\n",
        "        randomize: whether to shuffle the DataFrame before processing\n",
        "        exclude_column: column name to exclude from the DataFrame\n",
        "        exclude_df: DataFrame containing values to exclude based on exclude_column\n",
        "    '''\n",
        "    if randomize:\n",
        "        df_rnd = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "    else:\n",
        "        df_rnd = df.copy()\n",
        "\n",
        "    if exclude_column is not None and exclude_df is not None:\n",
        "        if exclude_column not in df_rnd.columns:\n",
        "            raise ValueError(f\"Column '{exclude_column}' not found in DataFrame.\")\n",
        "        if exclude_column not in exclude_df.columns:\n",
        "            raise ValueError(f\"Column '{exclude_column}' not found in exclude DataFrame.\")\n",
        "        df_rnd = df_rnd[~df_rnd[exclude_column].isin(exclude_df[exclude_column])]\n",
        "\n",
        "    uniq_classes = df_rnd[class_col].unique()\n",
        "    uniq_features = df_rnd[feature_col].unique()\n",
        "\n",
        "    def get_exp_data_inner(tmp_df, size):\n",
        "        df_tmp = None\n",
        "        for uf in uniq_features:\n",
        "            if ratio.get(uf) is None:\n",
        "                if SHOW_EXTRA_INFO:\n",
        "                    print(f\"Feature '{uf}' not found in ratios. Skipping.\")\n",
        "                continue\n",
        "            c_amt = int(size * ratio[uf] / len(uniq_classes))\n",
        "            # if c_amt <= 0:\n",
        "            #     raise ValueError(f\"Calculated samples per class ({c_amt}) is less than or equal to zero for feature '{uf}' with ratio {ratio}.\")\n",
        "            tmp = get_balanced_subset(df=tmp_df, class_col=class_col, feature_col=feature_col, feature_value=uf,\n",
        "                                        samples_per_class=c_amt, randomize=False)\n",
        "            if df_tmp is None:\n",
        "                df_tmp = tmp\n",
        "            else:\n",
        "                df_tmp = pd.concat([df_tmp, tmp])\n",
        "        return df_tmp\n",
        "\n",
        "    df_res = get_exp_data_inner(df_rnd, size)\n",
        "\n",
        "    if len(df_res) < size and SHOW_EXTRA_INFO:\n",
        "        print(f\"Samples for ({len(df_res)}) are less than requested ({size}).\")\n",
        "\n",
        "    ratios_fet = df_res[feature_col].value_counts(normalize=True).to_dict()\n",
        "    ratios_cls = df_res[class_col].value_counts(normalize=False).to_dict()\n",
        "\n",
        "    if SHOW_EXTRA_INFO:\n",
        "        print(f\"[] Ratios for {feature_col}: {ratios_fet}\")\n",
        "        print(f\"[] Ratios for {class_col}: {ratios_cls}\")\n",
        "        print()\n",
        "\n",
        "\n",
        "    for k in ratio:\n",
        "        if ratios_fet.get(k) is None:\n",
        "            if ratio[k] > 0.0:\n",
        "                raise ValueError(f\"Feature '{k}' not found in DataFrame after sampling (try increase 'size' parameter).\")\n",
        "        elif abs(ratios_fet[k] - ratio[k]) > max_diff:\n",
        "            raise ValueError(f\"Feature '{k}' ratio {ratios_fet[k]} differs from requested {ratio[k]} by more than {max_diff}.\")\n",
        "\n",
        "\n",
        "\n",
        "    df_res = df_res.reset_index(drop=True)\n",
        "\n",
        "    return df_res\n",
        "\n",
        "tmp_test = get_exp_data(\n",
        "    df=df, class_col='type', feature_col='ethnicity', ratio={'white':0.2, 'black':0.6, 'asian': 0.2}, size=10)\n",
        "print(tb.tabulate(tmp_test, headers='keys', tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO4YLfxMlxZ8"
      },
      "source": [
        "# Methods for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNCRWX5js2d7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficientnet\n",
        "from tensorflow.keras.applications.xception import preprocess_input as preprocess_xception\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input as preprocess_inception_v3\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as preprocess_efficientnetv2\n",
        "def preprocess_simple_cnn(image):\n",
        "    return image / 255.0\n",
        "\n",
        "\n",
        "augmentation_std = tf.keras.Sequential([\n",
        "    #layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomZoom(0.05),\n",
        "    #layers.RandomContrast(0.05),\n",
        "    #layers.RandomBrightness(0.05),\n",
        "], name=\"augmentation_std\")\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def load_image(file_path, label, preprocess, augment):\n",
        "    image = tf.io.read_file(file_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "    if IMG_RESIZE:\n",
        "        image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    else:\n",
        "        image = tf.cast(image, tf.float32)\n",
        "\n",
        "    if preprocess:\n",
        "        image = preprocess(image)\n",
        "\n",
        "    if augment:\n",
        "        image = augment(image)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def get_data_for_model(df, class_col, files_col, batch_size, preprocess, augment):\n",
        "    image_paths = df[files_col].values\n",
        "    labels = df[class_col]\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "    dataset = dataset.map(lambda path, label: load_image(path, label, preprocess, augment), num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR8_Qyf5lxZ8"
      },
      "outputs": [],
      "source": [
        "# Create folder if it doesn't exist\n",
        "if not os.path.exists(RES_PATH):\n",
        "    os.makedirs(RES_PATH)\n",
        "    print(f\"Created folder: {RES_PATH}\")\n",
        "elif REMOVE_RES_CONTENT:\n",
        "    # Remove all files inside the folder\n",
        "    for filename in os.listdir(RES_PATH):\n",
        "        file_path = os.path.join(RES_PATH, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.unlink(file_path)          # remove file or link\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)      # remove folder and contents\n",
        "        except Exception as e:\n",
        "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
        "    print(f\"Cleared contents of folder: {RES_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYdm-HiMlxZ8"
      },
      "outputs": [],
      "source": [
        "def get_done_reps(model_name, feature_name, amt_per_rep):\n",
        "  results_path = f'{RES_PATH}res_{feature_name}_{model_name.replace(\" \", \"_\")}.csv'\n",
        "  if not os.path.exists(results_path):\n",
        "    return [], None\n",
        "\n",
        "  tmp_df = pd.read_csv(results_path)\n",
        "  dones = []\n",
        "  for r in tmp_df[\"rep\"].unique():\n",
        "    amt = len(tmp_df[tmp_df[\"rep\"]==r])\n",
        "    if amt == amt_per_rep:\n",
        "      dones.append(r)\n",
        "\n",
        "  return dones, tmp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwAcU3dZlxZ8"
      },
      "outputs": [],
      "source": [
        "def compute_fairness_metrics(y_true, y_pred, group_labels):\n",
        "    \"\"\"Compute per-group rates and their gaps.\"\"\"\n",
        "    groups = pd.unique(group_labels)\n",
        "    tpr = {}\n",
        "    fpr = {}\n",
        "    pos_rate = {}\n",
        "\n",
        "    for g in groups:\n",
        "        mask = (group_labels == g)\n",
        "        yt = y_true[mask]\n",
        "        yp = y_pred[mask]\n",
        "        tn, fp, fn, tp = confusion_matrix(yt, yp, labels=[0,1]).ravel()\n",
        "        tpr[g] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        fpr[g] = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "        pos_rate[g] = (yp == 1).mean()\n",
        "\n",
        "    tpr_gap = max(tpr.values()) - min(tpr.values())\n",
        "    fpr_gap = max(fpr.values()) - min(fpr.values())\n",
        "    pos_rate_gap = max(pos_rate.values()) - min(pos_rate.values())\n",
        "\n",
        "    return {\n",
        "        \"tpr_gap\": tpr_gap,\n",
        "        \"fpr_gap\": fpr_gap,\n",
        "        \"pos_rate_gap\": pos_rate_gap,\n",
        "        \"per_group\": {\n",
        "            \"tpr\": tpr,\n",
        "            \"fpr\": fpr,\n",
        "            \"pos_rate\": pos_rate\n",
        "        }\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA9sTR7elxZ8"
      },
      "outputs": [],
      "source": [
        "def perform_tests(df, train_metas, test_metas, validation_size, reps, feature_split_col, get_model, preprocess, class_col='type', exclude_column='path',\n",
        "                  files_col='path', data_augmentation=False, epochs_num=15, batch_size=64):\n",
        "    res = []\n",
        "\n",
        "    model_name = get_model.__name__.replace(\"create_\", \"\")\n",
        "\n",
        "    done_reps, prev_results = get_done_reps(model_name, feature_split_col, len(test_metas) * len(train_metas))\n",
        "\n",
        "    for r in range(reps):\n",
        "        if r in done_reps:\n",
        "          res.extend(prev_results[prev_results['rep']==r].values.tolist())\n",
        "          print(f\"Rep {r} already done for {model_name}. Skipping...\")\n",
        "          continue\n",
        "\n",
        "        np.random.seed(SEED + r)\n",
        "        tf.random.set_seed(SEED + r)\n",
        "\n",
        "        for train_meta in train_metas:\n",
        "            train_val = get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=train_meta['ratio'], size=train_meta['size'] + validation_size)\n",
        "            train_val = train_val.sample(frac=1, random_state=SEED+r).reset_index(drop=True)\n",
        "\n",
        "            stratify_key = train_val[class_col].astype(str) + \"_\" + train_val[feature_split_col].astype(str)\n",
        "\n",
        "            train, val = train_test_split(\n",
        "                train_val,\n",
        "                test_size=validation_size / (train_meta['size'] + validation_size),\n",
        "                stratify=stratify_key,\n",
        "                random_state=SEED + r\n",
        "            )\n",
        "\n",
        "            tests = [\n",
        "                get_exp_data(df, class_col=class_col, feature_col=feature_split_col, ratio=tm['ratio'], size=tm['size'], exclude_column=exclude_column, exclude_df=train) for tm in test_metas\n",
        "            ]\n",
        "\n",
        "            train_dataset = get_data_for_model(train, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=data_augmentation)\n",
        "            val_dataset = get_data_for_model(val, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=False)\n",
        "            test_datasets = [\n",
        "                get_data_for_model(test, class_col=class_col, files_col=files_col, batch_size=batch_size, preprocess=preprocess, augment=False) for test in tests\n",
        "            ]\n",
        "\n",
        "            train_ratio = '/'.join([f\"{k}:{v}\" for k, v in train_meta['ratio'].items()])\n",
        "            train_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in train[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
        "            train_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', train_ratio)\n",
        "\n",
        "            model = get_model(input_shape=(IMG_SIZE,IMG_SIZE,3),\n",
        "                              train_dataset=train_dataset, val_dataset=val_dataset,\n",
        "                              epochs_num=epochs_num)\n",
        "\n",
        "            for test_dataset, test_meta, test_df in zip(test_datasets, test_metas, tests):\n",
        "                predictions = model.predict(test_dataset)\n",
        "                y_true = test_df[class_col]\n",
        "                y_pred = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "                tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "                accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "                fairness = compute_fairness_metrics(\n",
        "                    y_true=np.array(y_true),\n",
        "                    y_pred=np.array(y_pred),\n",
        "                    group_labels=test_df[feature_split_col].values\n",
        "                )\n",
        "                fairness_json = json.dumps(fairness[\"per_group\"])\n",
        "\n",
        "                test_ratio = '/'.join([f\"{k}:{v}\" for k, v in test_meta['ratio'].items()])\n",
        "                test_ratio_rel = '/'.join([f\"{k}:{v:.4f}\" for k, v in test_df[feature_split_col].value_counts(normalize=True).to_dict().items()])\n",
        "                test_ratio_sim = re.sub(r'[a-zA-Z0.:]', '', test_ratio)\n",
        "\n",
        "                roc_df = pd.DataFrame({\n",
        "                    'y_true': y_true,\n",
        "                    'y_pred_proba': predictions.flatten()\n",
        "                })\n",
        "                path_roc = f'{RES_PATH}roc_{feature_split_col}_{model_name.replace(\" \", \"_\")}_rep{r}_{time.time_ns()}.csv'\n",
        "                roc_df.to_csv(path_roc, index=False)\n",
        "\n",
        "                res.append([\n",
        "                    r,\n",
        "                    model_name,\n",
        "                    feature_split_col,\n",
        "                    train_meta['size'],\n",
        "                    train_ratio,\n",
        "                    test_meta['size'],\n",
        "                    test_ratio,\n",
        "                    train_ratio_rel,\n",
        "                    test_ratio_rel,\n",
        "                    train_ratio_sim,\n",
        "                    test_ratio_sim,\n",
        "                    accuracy,\n",
        "                    tn, fp, fn, tp,\n",
        "                    fairness[\"tpr_gap\"],\n",
        "                    fairness[\"fpr_gap\"],\n",
        "                    fairness[\"pos_rate_gap\"],\n",
        "                    fairness_json,\n",
        "                    path_roc\n",
        "                ])\n",
        "\n",
        "                print(f\"Rep: {r:2} | Model: {model_name} | Feature Split: {feature_split_col} | Ratio: {test_ratio} | Acc: {accuracy:.2f}\")\n",
        "\n",
        "                res_df = pd.DataFrame(res, columns=[\n",
        "                    'rep', 'model_name', 'feature_split_col',\n",
        "                    'train_size', 'train_ratio_detail', 'test_size', 'test_ratio_detail',\n",
        "                    'train_ratio_rel', 'test_ratio_rel', \"train_ratio\", \"test_ratio\",\n",
        "                    'accuracy', 'TN', 'FP', 'FN', 'TP',\n",
        "                    'fairness_tpr_gap', 'fairness_fpr_gap', 'fairness_pos_rate_gap',\n",
        "                    'fairness_per_group_json',\n",
        "                    'path_roc'\n",
        "                ])\n",
        "\n",
        "                res_df.to_csv(f'{RES_PATH}res_{feature_split_col}_{model_name.replace(\" \", \"_\")}.csv', index=False)\n",
        "    print(f\"Done for {model_name}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeRfWCbzlxZ9"
      },
      "source": [
        "# Models implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPE2ybiqlxZ9"
      },
      "outputs": [],
      "source": [
        "def create_xception(input_shape, train_dataset, val_dataset, epochs_num):\n",
        "    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    #x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"FITTING FULL XCEPTION\")\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
        "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loq_AI6blxZ9"
      },
      "outputs": [],
      "source": [
        "def create_efficientnetb4(input_shape, train_dataset, val_dataset, epochs_num):\n",
        "    base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    #x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
        "\n",
        "    print(\"TRAINING FULL EfficientNetB4\")\n",
        "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-7kueQ4lxZ9"
      },
      "outputs": [],
      "source": [
        "def create_efficientnetv2(input_shape, train_dataset, val_dataset, epochs_num):\n",
        "    base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    #base_model = EfficientNetV2M(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    #x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
        "\n",
        "    print(\"TRAINING FULL EfficientNetV2S\")\n",
        "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czXKH9trlxZ9"
      },
      "outputs": [],
      "source": [
        "def create_inceptionv3(input_shape, train_dataset, val_dataset, epochs_num):\n",
        "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = True\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"TRAINING FULL InceptionV3\")\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, mode='max', restore_best_weights=True, verbose=1)\n",
        "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqUe0lwMlxZ9"
      },
      "outputs": [],
      "source": [
        "def create_simple_cnn(input_shape, train_dataset, val_dataset, epochs_num):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"TRAINING Simple CNN\")\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', restore_best_weights=True, verbose=1)\n",
        "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs_num, callbacks=[early_stopping])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWZjB1d1lxZ9"
      },
      "source": [
        "# Tests starter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX2IbWlylxZ9"
      },
      "source": [
        "perform_tests(df=df,\n",
        "             train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_xception,\n",
        "              preprocess=preprocess_xception,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n",
        "\n",
        "perform_tests(df=df,\n",
        "              train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_inceptionv3,\n",
        "              preprocess=preprocess_inception_v3,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n",
        "\n",
        "perform_tests(df=df,\n",
        "             train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_efficientnetv2,\n",
        "              preprocess=preprocess_efficientnetv2,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "perform_tests(df=df,\n",
        "              train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_efficientnetb4,\n",
        "              preprocess=preprocess_efficientnet,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n",
        "\n",
        "\n",
        "perform_tests(df=df,\n",
        "             train_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_simple_cnn,\n",
        "              preprocess=preprocess_simple_cnn,\n",
        "              data_augmentation=augmentation_std\n",
        "              )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb_U5sSAlxZ9",
        "outputId": "f56902dc-37a4-472e-9e83-b1e7eef65b6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING FULL EfficientNetV2S\n",
            "Epoch 1/15\n"
          ]
        }
      ],
      "source": [
        "perform_tests(df=df,\n",
        "             train_metas=[\n",
        "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 5000},\n",
        "                  #{'ratio': {'male':0.5, 'female':0.5}, 'size': 5000},\n",
        "                  ],\n",
        "              test_metas=[\n",
        "                  {'ratio': {'male':0.1, 'female':0.9}, 'size': 500},\n",
        "                  {'ratio': {'male':0.2, 'female':0.8}, 'size': 500},\n",
        "                  {'ratio': {'male':0.3, 'female':0.7}, 'size': 500},\n",
        "                  {'ratio': {'male':0.4, 'female':0.6}, 'size': 500},\n",
        "                  {'ratio': {'male':0.5, 'female':0.5}, 'size': 500},\n",
        "                  {'ratio': {'male':0.6, 'female':0.4}, 'size': 500},\n",
        "                  {'ratio': {'male':0.7, 'female':0.3}, 'size': 500},\n",
        "                  {'ratio': {'male':0.8, 'female':0.2}, 'size': 500},\n",
        "                  {'ratio': {'male':0.9, 'female':0.1}, 'size': 500},\n",
        "              ],\n",
        "              validation_size=500,\n",
        "              reps=10,\n",
        "              feature_split_col='sex',\n",
        "              get_model=create_efficientnetv2,\n",
        "              preprocess=preprocess_efficientnetv2,\n",
        "              data_augmentation=augmentation_std\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M0_AIpZlxZ9"
      },
      "outputs": [],
      "source": [
        "def print_summarise_res(model_name:str):\n",
        "  path = RES_PATH + f'res_sex_{model_name}.csv'\n",
        "\n",
        "  if not os.path.exists(path):\n",
        "    print(f\"File {path} does not exists!\")\n",
        "    return\n",
        "\n",
        "  res = pd.read_csv(path)\n",
        "  gr = res.groupby(['train_ratio', 'test_ratio']).agg(\n",
        "      # Model=('model_name', 'first'),\n",
        "      TrainRatio=('train_ratio', 'first'),\n",
        "      TestRatio=('test_ratio', 'first'),\n",
        "      Accuracy= ('accuracy', 'mean'),\n",
        "      AccuracySTD= ('accuracy', 'std'),\n",
        "      F1=('f1_score', 'mean'),\n",
        "      F1STD=('f1_score', 'std'),\n",
        "      EODiff=('eo_diff', 'mean'),\n",
        "      EODiffSTD=('eo_diff', 'std'),\n",
        "  ).reset_index(drop=True)\n",
        "\n",
        "  gr = gr.round(3).sort_values(by=['TrainRatio', 'TestRatio'], ascending=False)\n",
        "\n",
        "  print(\"MODEL: \" + model_name)\n",
        "  print(tb.tabulate(gr, headers='keys', tablefmt='psql'))\n",
        "  print()\n",
        "  print()\n",
        "\n",
        "print_summarise_res('efficientnetv2')\n",
        "print_summarise_res('xception')\n",
        "print_summarise_res('inceptionv3')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}